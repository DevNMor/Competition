{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d427e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67b6223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle('Preprocessing_Train.pkl')\n",
    "stop_words = stopwords.words('english')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = True, # Whether the model returns all hidden-states.       \n",
    "    )\n",
    "\n",
    "def bert_tokenize(doc, tokenizer=tokenizer):\n",
    "    \n",
    "    tokens = re.split(r'\\s', doc.lower())\n",
    "    tokens = [i for i in tokens if i not in stop_words]\n",
    "    tokens = [i for i in tokens if len(re.findall(r'\\w', i)) >= 2]\n",
    "    tokens = [re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)[0] for i in tokens if re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)]\n",
    "    result = []\n",
    "    for i in tokens:\n",
    "        result += tokenizer.convert_tokens_to_ids(tokenizer.tokenize(i))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59137bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.document_text.apply(bert_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef8cc7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12508/3565975597.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "class dataset_(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, max_len=1200):\n",
    "        super(dataset_, self).__init__()\n",
    "        pad = [i[:max_len] + [0]*(max_len-len(i[:max_len])) for i in X]\n",
    "        self.X = torch.tensor(pad, dtype=torch.int32)\n",
    "        self.y = torch.Tensor(np.array(list(y)))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "train_ = dataset_(X, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ec183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_layer = bert_model.get_input_embeddings()\n",
    "\n",
    "for param in pretrained_layer.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "# class PE(nn.Module): \n",
    "\n",
    "#     def __init__(self, dim_emb=256, dropout=0.5, max_len=1024): \n",
    "#         super(PE, self).__init__() \n",
    "#         self.dropout = nn.Dropout(dropout) \n",
    "#         pe = torch.zeros(max_len, dim_emb) \n",
    "#         position = torch.arange(0, max_len).unsqueeze(1) \n",
    "#         div_term = torch.exp(-math.log(10000) * torch.arange(0, dim_emb, 2)/dim_emb)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term) \n",
    "#         pe = pe.unsqueeze(0) \n",
    "#         self.register_buffer('pe', pe) \n",
    "        \n",
    "#     def forward(self, x): \n",
    "#         x = x + Variable(self.pe[:, :], requires_grad=False) \n",
    "#         return self.dropout(x) \n",
    "\n",
    "    \n",
    "class Att(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=512):\n",
    "        super(Att, self).__init__()\n",
    "        self.Wq = nn.Linear(dim_emb, 50, bias=False)\n",
    "        self.Wk = nn.Linear(768, dim_emb, bias=False)\n",
    "        self.Wv = nn.Linear(768, dim_emb, bias=False)\n",
    "\n",
    "    def forward(self, x): ## (1200, 768)\n",
    "        K = self.Wk(x) ## (1200, dim_emb)\n",
    "        V = self.Wv(x)\n",
    "        temp = self.Wq(K) ## (1200, 50)\n",
    "        score = temp.transpose(-2, -1) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "        return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "    \n",
    "class LN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=512):\n",
    "        super(LN, self).__init__()\n",
    "        self.feed = nn.LayerNorm(dim_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.feed(x)\n",
    "\n",
    "\n",
    "class final_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=512, dropout=0.3):\n",
    "        super(final_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            pretrained_layer,\n",
    "            # PE(dim_emb, dropout, max_len),  ## (1200, 768)\n",
    "            # nn.Dropout(dropout),\n",
    "            Att(dim_emb),  ## (50, dim_emb)\n",
    "            LN(dim_emb),\n",
    "            nn.Linear(dim_emb, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d847dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, device, norm=0.5,\n",
    "                lr=0.0005, epochs=50, batch_size=256):\n",
    "    \n",
    "    history = {'train_loss': []}\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = model.to(device)\n",
    "    if norm:\n",
    "        Loss = nn.BCELoss(weight=train_.y.sum(axis=0)**-norm).to(device)\n",
    "    else:\n",
    "        Loss = nn.BCELoss().to(device)\n",
    "    op = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        eval_loss = 0\n",
    "        pred = []\n",
    "        real = []\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            \n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "            \n",
    "                \n",
    "        train_loss = (train_loss/len(train_loader)).item()\n",
    "\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "            \n",
    "        \n",
    "    print('Training complete!')\n",
    "    print(f'\\nSpent time: {time.time()-start} seconds')\n",
    "    \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "077c5043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "mod = final_model(dim_emb=384)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "norm = None\n",
    "epochs = 150\n",
    "lr = 0.00027\n",
    "batch_size = 128\n",
    "# weight_decay = 0.0003\n",
    "hist = train_model(mod, train_, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc136f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = pd.read_csv('datasolve-us/test.csv')\n",
    "X_test = tst.document_text.apply(bert_tokenize)\n",
    "max_len = 1200\n",
    "pad = [i[:max_len] + [0]*(max_len-len(i[:max_len])) for i in X_test]\n",
    "X_final = torch.tensor(pad, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373204e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = torch.load('bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ded4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mod = mod.to(device)\n",
    "X = X_final.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab2b537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.cpu()\n",
    "X_final.cpu()\n",
    "res = []\n",
    "with torch.no_grad():\n",
    "    for i in DataLoader(X_final, batch_size=100, shuffle=False):\n",
    "        res.append(mod(i).squeeze(-1).view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d3a05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.round(torch.concat(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "618eaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mod, 'bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1311137",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('try_1.csv')\n",
    "t2 = pd.read_csv('try_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9246ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx = torch.Tensor(np.array(t1.predictions))\n",
    "lj = torch.Tensor(np.array(t2.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a7fbd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9681)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(yx==res).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b882ecec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9549)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lj==res).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40d5c42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9474)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lj==yx).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "808114da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res, columns=['predictions']).to_csv('try_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877918a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
