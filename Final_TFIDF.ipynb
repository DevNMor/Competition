{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e143c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa00d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize needed variables and set up pipelines for tokenizations\n",
    "\n",
    "train = pd.read_pickle('Preprocessing_Train.pkl')\n",
    "tst = pd.read_csv('datasolve-us/test.csv')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train.document_text, train.label, test_size=0.25)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_neg_stop = [i for i in stop_words if \"n't\" not in i and \"no\" not in i]\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos(x):\n",
    "    if x.startswith('J'):\n",
    "        return 'a'\n",
    "    elif x.startswith('V'):\n",
    "        return 'v'\n",
    "    elif x.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "    \n",
    "def tok(doc, non_neg=False, lemmatized=False):\n",
    "    \n",
    "    if non_neg:\n",
    "        stop = non_neg_stop\n",
    "    else:\n",
    "        stop = stop_words\n",
    "       \n",
    "    tokens = re.split(r'\\s', doc.lower())\n",
    "    \n",
    "    if lemmatized:\n",
    "        pos_list = nltk.pos_tag(tokens)\n",
    "        tokens = list(map(lambda x: lemma.lemmatize(x[0], pos(x[1])), pos_list))    \n",
    "\n",
    "    tokens = [i for i in tokens if i not in stop]\n",
    "    tokens = [i for i in tokens if len(re.findall(r'\\w', i)) >= 2]\n",
    "    tokens = [re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)[0] for i in tokens if re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_voc_new(X, non_neg=True, lemmatized=True):\n",
    "    \n",
    "    voc = []\n",
    "    token_list = []\n",
    "    for i in X:\n",
    "        tokens = tok(i, non_neg, lemmatized)\n",
    "        token_list.append(tokens)\n",
    "        voc += tokens\n",
    "        \n",
    "    voc = list(set(voc))\n",
    "    \n",
    "    return voc, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7fc7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_final, _ = get_voc_new(train.document_text, False, True)\n",
    "voc_tst, _ = get_voc_new(tst.document_text, False, True)\n",
    "tfidf = TfidfVectorizer(min_df=10, vocabulary=voc_final)\n",
    "vec_train = tfidf.fit_transform(train.document_text)\n",
    "vec_test = tfidf.transform(tst.document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b9d4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "ok_idx = torch.Tensor(vec_train.toarray()).max(0).values>threshold\n",
    "advoc_size = ok_idx.int().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0f636af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        super(_dataset, self).__init__()\n",
    "        self.X = torch.Tensor(X.toarray())[:, ok_idx]\n",
    "        self.y = torch.Tensor(y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "_train = _dataset(vec_train, np.array(list(train.label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74eeb07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(MLP_model, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(advoc_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 50),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93856f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gg(model, train_dataset, device, norm=0.5,\n",
    "                lr=0.0005, epochs=50, batch_size=256):\n",
    "    \n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if norm:\n",
    "        Loss = nn.BCELoss(weight=train_.y.sum(axis=0)**-norm).to(device)\n",
    "\n",
    "    else:\n",
    "        Loss = nn.BCELoss().to(device)\n",
    "        \n",
    "    op = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(X)\n",
    "            loss = Loss(out, y)\n",
    "            \n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "    print('Training complete!')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "376e5e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model_A = MLP_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 100\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "hist = gg(model_A, _train, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc1e82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = torch.Tensor(vec_test.toarray())[:, ok_idx].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model_A(xxx)\n",
    "\n",
    "rr = np.round(res.view(-1).cpu())\n",
    "pd.DataFrame(rr, columns=['predictions']).to_csv('tfidf_my.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37815f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
