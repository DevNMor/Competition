{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784f5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887fc16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85691e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('Preprocessing_Train.pkl')\n",
    "test = pd.read_csv('datasolve-us/test.csv')\n",
    "new = pd.read_csv('new.csv')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_neg_stop = [i for i in stop_words if \"n't\" not in i and \"no\" not in i]\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos(x):\n",
    "    if x.startswith('J'):\n",
    "        return 'a'\n",
    "    elif x.startswith('V'):\n",
    "        return 'v'\n",
    "    elif x.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "    \n",
    "def tok(doc, non_neg=False, lemmatized=False):\n",
    "    \n",
    "    if non_neg:\n",
    "        stop = non_neg_stop\n",
    "    else:\n",
    "        stop = stop_words\n",
    "       \n",
    "    tokens = re.split(r'\\s', doc.lower())\n",
    "    \n",
    "    if lemmatized:\n",
    "        pos_list = nltk.pos_tag(tokens)\n",
    "        tokens = list(map(lambda x: lemma.lemmatize(x[0], pos(x[1])), pos_list))    \n",
    "\n",
    "    tokens = [i for i in tokens if i not in stop]\n",
    "    tokens = [i for i in tokens if len(re.findall(r'\\w', i)) >= 2]\n",
    "    tokens = [re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)[0] for i in tokens if re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_voc_new(X, non_neg=True, lemmatized=True):\n",
    "    \n",
    "    voc = []\n",
    "    token_list = []\n",
    "    for i in X:\n",
    "        tokens = tok(i, non_neg, lemmatized)\n",
    "        token_list.append(tokens)\n",
    "        voc += tokens\n",
    "        \n",
    "    voc = list(set(voc))\n",
    "    \n",
    "    return voc, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be66a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, tk_list_train = get_voc_new(train.document_text, False, True)\n",
    "voc_test, tk_list_test = get_voc_new(test.document_text, False, True)\n",
    "voc_dic = {item: idx+1 for idx, item in enumerate(voc)}\n",
    "\n",
    "def word2idx(token_list):\n",
    "    \n",
    "    token_idx = []\n",
    "    for sent in token_list:\n",
    "        sent_list = []\n",
    "        for token in sent:\n",
    "            if token in voc_dic:\n",
    "                idx = voc_dic[token]\n",
    "            else:\n",
    "                idx = 0\n",
    "            sent_list.append(idx)\n",
    "        token_idx.append(sent_list)\n",
    "\n",
    "    return token_idx\n",
    "\n",
    "\n",
    "tk_idx_train = word2idx(tk_list_train)\n",
    "tk_idx_test = word2idx(tk_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1217963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, target, max_len=1024):\n",
    "        super(dataset_, self).__init__()\n",
    "        pad = [i[:max_len] + [0]*(max_len-len(i[:max_len])) for i in X]\n",
    "        self.X = torch.tensor(pad, dtype=torch.int32)\n",
    "        self.y = torch.Tensor(np.array(list(y))[:, target])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "hard = [8, 19, 35, 41]   \n",
    "ez = [i for i in range(50) if i not in hard]\n",
    "train_ez = dataset_(tk_idx_train, train.label, ez)\n",
    "train_hard = dataset_(tk_idx_train, train.label, hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30cb17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emb_1(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, dim_emb):\n",
    "        super(Emb_1, self).__init__()\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Embedding(voc_size, dim_emb, padding_idx=0),\n",
    "            nn.Linear(dim_emb, 256),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "    \n",
    "class Emb_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, dim_emb):\n",
    "        super(Emb_2, self).__init__()\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Embedding(voc_size, dim_emb, padding_idx=0),\n",
    "            nn.Linear(dim_emb, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "    \n",
    "class Conv1(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out):\n",
    "        super(Conv1, self).__init__()\n",
    "        self.cv1 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 1),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2))\n",
    "        self.cv2 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 2),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2)) \n",
    "        self.cv3 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 3),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        C1 = self.cv1(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C2 = self.cv2(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C3 = self.cv3(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        return torch.cat([C1,C2,C3], dim=-1)\n",
    "    \n",
    "            \n",
    "class Att(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256, dim_enc=128):\n",
    "        super(Att, self).__init__()\n",
    "        self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "        self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        temp = self.Wq(K) \n",
    "        score = temp.transpose(-2, -1) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "        return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "\n",
    "\n",
    "class ez_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size=len(voc)+1, dim_emb=256, dropout=0.5, max_len=1024):\n",
    "        super(ez_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Emb_1(voc_size, dim_emb),  ## (1024, dim_emb)\n",
    "            Conv1(256, 256),\n",
    "            nn.Dropout(dropout),\n",
    "            Att(768, 46),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class comp_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size=len(voc)+1, dim_emb=256, dropout=0.5, max_len=1024):\n",
    "        super(comp_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Emb_2(voc_size, dim_emb),  ## (1024, dim_emb)\n",
    "            Conv1(256, 256),\n",
    "            nn.Dropout(dropout),\n",
    "            Att(768, 4),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b46dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_A, model_B, train_dataset_A, train_dataset_B, device,\n",
    "                lr=0.0005, epochs_A=50, epochs_B=50, batch_size=256):\n",
    "\n",
    "    \n",
    "    train_loader_A = DataLoader(train_dataset_A, batch_size=batch_size, shuffle=True)\n",
    "    train_loader_B = DataLoader(train_dataset_B, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model_A = model_A.to(device)\n",
    "    model_B = model_B.to(device)\n",
    "    Loss = nn.BCELoss().to(device)\n",
    "    op_A = torch.optim.Adam(model_A.parameters(), lr=lr)\n",
    "    op_B = torch.optim.Adam(model_B.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs_A):\n",
    "        \n",
    "        model_A.train()\n",
    "\n",
    "        for X, y in train_loader_A:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model_A(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            \n",
    "            op_A.zero_grad()\n",
    "            loss.backward()\n",
    "            op_A.step()\n",
    "    \n",
    "    print('model A training complete!')\n",
    "    print(f'Spent time: {time.time()-start} seconds')\n",
    "      \n",
    "    for epoch in range(epochs_B):\n",
    "        \n",
    "        model_B.train()\n",
    "\n",
    "        for X, y in train_loader_B:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model_B(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            \n",
    "            op_B.zero_grad()\n",
    "            loss.backward()\n",
    "            op_B.step()\n",
    "            \n",
    "    print('model B training complete!')\n",
    "    print(f'Spent time: {time.time()-start} seconds')\n",
    "            \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8b846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "model A training complete!\n",
      "Spent time: 2262.119045495987 seconds\n"
     ]
    }
   ],
   "source": [
    "model_A = ez_model()\n",
    "model_B = comp_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs_A = 200\n",
    "epochs_B = 200\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "\n",
    "train_model(model_A, model_B, train_ez, train_hard, device, lr, epochs_A, epochs_B, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_test = [i[:1024] + [0]*(1024-len(i[:1024])) for i in tk_idx_test]\n",
    "test_ = torch.tensor(pad_test, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = test_.cpu()\n",
    "model_A = model_A.cpu()\n",
    "model_B = model_B.cpu()\n",
    "res_A = []\n",
    "res_B = []\n",
    "with torch.no_grad():\n",
    "    for i in DataLoader(test_, batch_size=256, shuffle=False):\n",
    "        res_A.append(model_A(i).cpu())\n",
    "        res_B.append(model_B(i).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4724123",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_A = torch.cat(res_A, dim=0).squeeze(-1)\n",
    "fin_B = torch.cat(res_B, dim=0).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b61345",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = torch.zeros(4993,50)\n",
    "final[:, ez] = fin_A\n",
    "final[:, hard] = fin_B\n",
    "fin = np.round(final.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faad558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fin, columns=['predictions']).to_csv('bailanle.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
