{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21978a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46105ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('Preprocessing_Train.pkl')\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.document_text, train.label, test_size=0.25)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_neg_stop = [i for i in stop_words if \"n't\" not in i and \"no\" not in i]\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos(x):\n",
    "    if x.startswith('J'):\n",
    "        return 'a'\n",
    "    elif x.startswith('V'):\n",
    "        return 'v'\n",
    "    elif x.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "    \n",
    "def tok(doc, non_neg=False, lemmatized=False):\n",
    "    \n",
    "    if non_neg:\n",
    "        stop = non_neg_stop\n",
    "    else:\n",
    "        stop = stop_words\n",
    "       \n",
    "    tokens = re.split(r'\\s', doc.lower())\n",
    "    \n",
    "    if lemmatized:\n",
    "        pos_list = nltk.pos_tag(tokens)\n",
    "        tokens = list(map(lambda x: lemma.lemmatize(x[0], pos(x[1])), pos_list))    \n",
    "\n",
    "    tokens = [i for i in tokens if i not in stop]\n",
    "    tokens = [i for i in tokens if len(re.findall(r'\\w', i)) >= 2]\n",
    "    tokens = [re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)[0] for i in tokens if re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_voc_new(X, non_neg=True, lemmatized=True):\n",
    "    \n",
    "    voc = []\n",
    "    token_list = []\n",
    "    for i in X:\n",
    "        tokens = tok(i, non_neg, lemmatized)\n",
    "        token_list.append(tokens)\n",
    "        voc += tokens\n",
    "        \n",
    "    voc = list(set(voc))\n",
    "    \n",
    "    return voc, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, tk_list_train = get_voc_new(X_train, False, True)\n",
    "# voc, tk_list_train = get_voc_new(train.document_text, False, True)\n",
    "voc_test, tk_list_test = get_voc_new(X_test, False, True)\n",
    "voc_dic = {item: idx+1 for idx, item in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231df435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(token_list):\n",
    "    \n",
    "    token_idx = []\n",
    "    for sent in token_list:\n",
    "        sent_list = []\n",
    "        for token in sent:\n",
    "            if token in voc_dic:\n",
    "                idx = voc_dic[token]\n",
    "            else:\n",
    "                idx = 0\n",
    "            sent_list.append(idx)\n",
    "        token_idx.append(sent_list)\n",
    "\n",
    "    return token_idx\n",
    "\n",
    "\n",
    "tk_idx_train = word2idx(tk_list_train)\n",
    "tk_idx_test = word2idx(tk_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, max_len=1024):\n",
    "        super(dataset_, self).__init__()\n",
    "        pad = [i[:max_len] + [0]*(max_len-len(i[:max_len])) for i in X]\n",
    "        self.X = torch.tensor(pad, dtype=torch.int32)\n",
    "        self.y = torch.Tensor(np.array(list(y)))\n",
    "    #[:, [8, 22, 35]]   \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "train_ = dataset_(tk_idx_train, y_train)\n",
    "# train_ = dataset_(tk_idx_train, train.label)\n",
    "test_ = dataset_(tk_idx_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab34761",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 调一下Emb这里的Linear层，确保最后总是映射到256\n",
    "## 试试多几层看看效果，中间的神经元个数可大可小\n",
    "class Emb(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, dim_emb):\n",
    "        super(Emb, self).__init__()\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Embedding(voc_size, dim_emb, padding_idx=0),\n",
    "            nn.Linear(dim_emb, 256),\n",
    "            nn.ReLU())\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "    \n",
    "class Conv1(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out):\n",
    "        super(Conv1, self).__init__()\n",
    "        self.cv1 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 1),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2))\n",
    "        self.cv2 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 2),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2)) \n",
    "        self.cv3 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 3),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        C1 = self.cv1(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C2 = self.cv2(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C3 = self.cv3(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        return torch.cat([C1,C2,C3], dim=-1)\n",
    "    \n",
    "class Conv2(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out):\n",
    "        super(Conv2, self).__init__()\n",
    "        self.cv1 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 1),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4, 2))\n",
    "        self.cv2 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 2),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2)) \n",
    "        self.cv3 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 3),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        C1 = self.cv1(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C2 = self.cv2(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C3 = self.cv3(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        return torch.cat([C1,C2,C3], dim=-1)\n",
    "    \n",
    "    \n",
    "class EndConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out, conv_size):\n",
    "        super(EndConv, self).__init__()\n",
    "        self.ecv = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, conv_size),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z = self.ecv(x.transpose(-2, -1))\n",
    "        return nn.MaxPool1d(Z.size(-1))(Z).squeeze(-1)\n",
    "\n",
    "    \n",
    "# class SelfAtt(nn.Module):\n",
    "    \n",
    "#     def __init__(self, dim_emb=256, dim_enc=256):\n",
    "#         super(SelfAtt, self).__init__()\n",
    "#         self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "#         self.Wk = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "#         self.Wv = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         Q = self.Wq(x)\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "#         score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "#         return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "            \n",
    "class Att(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256, dim_enc=128):\n",
    "        super(Att, self).__init__()\n",
    "        self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "        self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        temp = self.Wq(K) \n",
    "        score = temp.transpose(-2, -1) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "        return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "    \n",
    "class LN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256):\n",
    "        super(LN, self).__init__()\n",
    "        self.feed = nn.LayerNorm(dim_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.feed(x)\n",
    "\n",
    "\n",
    "class final_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size=len(voc)+1, dim_emb=256, dropout=0.5, max_len=1024):\n",
    "        super(final_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Emb(voc_size, dim_emb),  ## (1024, dim_emb)\n",
    "            Conv1(dim_emb, 256),\n",
    "            # Conv2(768, 256),\n",
    "            nn.Dropout(dropout),\n",
    "            Att(768, 50),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, eval_dataset, device, norm=0.5,\n",
    "                lr=0.0005, epochs=50, batch_size=256):\n",
    "    \n",
    "    history = {'train_loss': [], 'eval_loss': [], 'detail_train': [], 'detail_eval': []}\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "    save_loss = nn.BCELoss(reduction='none').to(device)\n",
    "    \n",
    "    if norm:\n",
    "        Loss = nn.BCELoss(weight=train_.y.sum(axis=0)**-norm).to(device)\n",
    "\n",
    "    else:\n",
    "        Loss = nn.BCELoss().to(device)\n",
    "        \n",
    "    op = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        eval_loss = 0\n",
    "        detail_train = torch.zeros(50).to(device)\n",
    "        detail_eval = torch.zeros(50).to(device)\n",
    "        pred = []\n",
    "        real = []\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            save_train = save_loss(out, y).sum(0)\n",
    "            \n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "            detail_train += save_train\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in eval_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(X).squeeze(-1)\n",
    "                loss = Loss(out, y)\n",
    "                save_eval = save_loss(out, y).sum(0)\n",
    "                detail_eval += save_eval\n",
    "                eval_loss += loss\n",
    "                pred.append(out.cpu())\n",
    "                real.append(y.cpu())\n",
    "                \n",
    "        train_loss = (train_loss/len(train_loader)).item()\n",
    "        eval_loss = (eval_loss/len(eval_loader)).item() \n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['eval_loss'].append(eval_loss)  \n",
    "        history['detail_train'].append(detail_train.cpu().detach())\n",
    "        history['detail_eval'].append(detail_eval.cpu().detach())  \n",
    "        \n",
    "        if not (epoch+1)%10:\n",
    "            print(f\"epoch {epoch+1}\\ntrain loss: {train_loss}\\t\\teval loss: {eval_loss}\")\n",
    "        \n",
    "        if not (epoch+1)%50:\n",
    "            res = torch.cat(pred)\n",
    "            tru = torch.cat(real)\n",
    "            print(f'\\nepoch {epoch+1}:\\n')\n",
    "            print(f\"f1_score for 50 classes: {f1_score(tru, np.round(res), average='macro')}\")\n",
    "            print(classification_report(tru, np.round(res)))\n",
    "#             print(classification_report(tru, np.round(res), target_names=['8','22','35']))\n",
    "            print(f'\\nSpent time: {time.time()-start} seconds')\n",
    "            \n",
    "        \n",
    "    print('Training complete!')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409df179",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = final_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "\n",
    "hist = train_model(mod, train_, test_, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['train_loss'], label='Train')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['eval_loss'], label='Eval')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
