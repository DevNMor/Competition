{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f10714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42f7657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = pd.read_csv('datasolve-us/train.csv')\n",
    "train = pd.read_pickle('Preprocessing_Train.pkl')\n",
    "\n",
    "t1, t2, _, _ = train_test_split(ori.id, ori.cat_name, test_size=0.5, stratify=ori.cat_name)\n",
    "train_idx = [i for i in range(len(train)) if train['id'][i] in t1.unique()]\n",
    "# test_idx = [i for i in range(len(train)) if i not in train_idx]\n",
    "test_idx = [i for i in range(len(train)) if train['id'][i] in t2.unique()]\n",
    "\n",
    "X_train = train['document_text'][train_idx]\n",
    "y_train = train['label'][train_idx]\n",
    "X_test = train['document_text'][test_idx]\n",
    "y_test = train['label'][test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb4bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "non_neg_stop = [i for i in stop_words if \"n't\" not in i and \"no\" not in i]\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos(x):\n",
    "    if x.startswith('J'):\n",
    "        return 'a'\n",
    "    elif x.startswith('V'):\n",
    "        return 'v'\n",
    "    elif x.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "    \n",
    "def tok(doc, non_neg=False, lemmatized=False):\n",
    "    \n",
    "    if non_neg:\n",
    "        stop = non_neg_stop\n",
    "    else:\n",
    "        stop = stop_words\n",
    "       \n",
    "    tokens = re.split(r'\\s', doc.lower())\n",
    "    \n",
    "    if lemmatized:\n",
    "        pos_list = nltk.pos_tag(tokens)\n",
    "        tokens = list(map(lambda x: lemma.lemmatize(x[0], pos(x[1])), pos_list))    \n",
    "\n",
    "    tokens = [i for i in tokens if i not in stop]\n",
    "    tokens = [i for i in tokens if len(re.findall(r'\\w', i)) >= 2]\n",
    "    tokens = [re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)[0] for i in tokens if re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_voc_new(X, non_neg=True, lemmatized=True):\n",
    "    \n",
    "    voc = []\n",
    "    token_list = []\n",
    "    for i in X:\n",
    "        tokens = tok(i, non_neg, lemmatized)\n",
    "        token_list.append(tokens)\n",
    "        voc += tokens\n",
    "        \n",
    "    voc = list(set(voc))\n",
    "    \n",
    "    return voc, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b21cfcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, tk_list_train = get_voc_new(X_train, False, True)\n",
    "# voc, tk_list_train = get_voc_new(train.document_text, False, True)\n",
    "voc_test, tk_list_test = get_voc_new(X_test, False, True)\n",
    "voc_dic = {item: idx+1 for idx, item in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb87c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(token_list):\n",
    "    \n",
    "    token_idx = []\n",
    "    for sent in token_list:\n",
    "        sent_list = []\n",
    "        for token in sent:\n",
    "            if token in voc_dic:\n",
    "                idx = voc_dic[token]\n",
    "            else:\n",
    "                idx = 0\n",
    "            sent_list.append(idx)\n",
    "        token_idx.append(sent_list)\n",
    "\n",
    "    return token_idx\n",
    "\n",
    "\n",
    "tk_idx_train = word2idx(tk_list_train)\n",
    "tk_idx_test = word2idx(tk_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91d36530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, max_len=1024):\n",
    "        super(dataset_, self).__init__()\n",
    "        pad = [i[:max_len] + [0]*(max_len-len(i[:max_len])) for i in X]\n",
    "        self.X = torch.tensor(pad, dtype=torch.int32)\n",
    "        self.y = torch.Tensor(np.array(list(y)))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "train_ = dataset_(tk_idx_train, y_train)\n",
    "# train_ = dataset_(tk_idx_train, train.label)\n",
    "test_ = dataset_(tk_idx_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "072ca70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PE(nn.Module): \n",
    "\n",
    "#     def __init__(self, dim_emb=256, dropout=0.5, max_len=1024): \n",
    "#         super(PE, self).__init__() \n",
    "#         self.dropout = nn.Dropout(dropout) \n",
    "#         pe = torch.zeros(max_len, dim_emb) \n",
    "#         position = torch.arange(0, max_len).unsqueeze(1) \n",
    "#         div_term = torch.exp(-math.log(10000) * torch.arange(0, dim_emb, 2)/dim_emb)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term) \n",
    "#         pe = pe.unsqueeze(0) \n",
    "#         self.register_buffer('pe', pe) \n",
    "        \n",
    "#     def forward(self, x): \n",
    "#         x = x + Variable(self.pe[:, :], requires_grad=False) \n",
    "#         return self.dropout(x) \n",
    "\n",
    "    \n",
    "# class MultAtt(nn.Module):\n",
    "    \n",
    "#     def __init__(self, dim_emb=256):\n",
    "#         super(Att, self).__init__()\n",
    "#         self.Wq = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "#         self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "#         self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "#         self.Wo = nn.Linear(dim_emb, 50, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         Q = self.Wq(x)\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "#         interval = int(K.size(-1)/4)\n",
    "#         Z = torch.cat([torch.matmul(nn.Softmax(dim=-1)(torch.matmul(\n",
    "#                 Q[:,:, i:i+interval], K[:,:, i:i+interval].transpose(-2, -1))\n",
    "#                 /math.sqrt(interval)), V[:,:, i:i+interval]) \n",
    "#                 for i in range(0, K.size(-1), interval)], dim=-1)\n",
    "#         return self.Wo(Z).transpose(-2, -1)\n",
    "\n",
    "\n",
    "class Emb(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, dim_emb):\n",
    "        super(Emb, self).__init__()\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Embedding(voc_size, dim_emb, padding_idx=0),\n",
    "            nn.Linear(dim_emb, 256),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "    \n",
    "class Conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out):\n",
    "        super(Conv, self).__init__()\n",
    "        self.cv1 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 1),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2))\n",
    "        self.cv2 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 2),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2)) \n",
    "        self.cv3 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 3),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        C1 = self.cv1(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C2 = self.cv2(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C3 = self.cv3(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        return torch.cat([C1,C2,C3], dim=-1)\n",
    "    \n",
    "    \n",
    "class EndConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out, conv_size):\n",
    "        super(EndConv, self).__init__()\n",
    "        self.ecv = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, conv_size),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z = self.ecv(x.transpose(-2, -1))\n",
    "        return nn.MaxPool1d(Z.size(-1))(Z).squeeze(-1)\n",
    "\n",
    "    \n",
    "# class SelfAtt(nn.Module):\n",
    "    \n",
    "#     def __init__(self, dim_emb=256, dim_enc=256):\n",
    "#         super(SelfAtt, self).__init__()\n",
    "#         self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "#         self.Wk = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "#         self.Wv = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         Q = self.Wq(x)\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "#         score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "#         return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "            \n",
    "class Att(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256, dim_enc=128):\n",
    "        super(Att, self).__init__()\n",
    "        self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "        self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        temp = self.Wq(K) \n",
    "        score = temp.transpose(-2, -1) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "        return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "    \n",
    "class LN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256):\n",
    "        super(LN, self).__init__()\n",
    "        self.feed = nn.LayerNorm(dim_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.feed(x)\n",
    "\n",
    "\n",
    "class final_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size=len(voc)+1, dim_emb=256, dropout=0.5, max_len=1024):\n",
    "        super(final_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Emb(voc_size, dim_emb),  ## (1024, dim_emb)\n",
    "            Conv(dim_emb, 256),\n",
    "            nn.Dropout(dropout),\n",
    "            Att(768, 50),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7ca57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, eval_dataset, device, norm=0.5,\n",
    "                lr=0.0005, epochs=50, batch_size=256):\n",
    "    \n",
    "    history = {'train_loss': [], 'eval_loss': [], 'detail_train': [], 'detail_eval': []}\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "    save_loss = nn.BCELoss(reduction='none').to(device)\n",
    "    \n",
    "    if norm:\n",
    "        Loss = nn.BCELoss(weight=train_.y.sum(axis=0)**-norm).to(device)\n",
    "\n",
    "    else:\n",
    "        Loss = nn.BCELoss().to(device)\n",
    "        \n",
    "    op = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        eval_loss = 0\n",
    "        detail_train = torch.zeros(50).to(device)\n",
    "        detail_eval = torch.zeros(50).to(device)\n",
    "        pred = []\n",
    "        real = []\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            save_train = save_loss(out, y).sum(0)\n",
    "            \n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "            detail_train += save_train\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in eval_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(X).squeeze(-1)\n",
    "                loss = Loss(out, y)\n",
    "                save_eval = save_loss(out, y).sum(0)\n",
    "                detail_eval += save_eval\n",
    "                eval_loss += loss\n",
    "                pred.append(out.cpu())\n",
    "                real.append(y.cpu())\n",
    "                \n",
    "        train_loss = (train_loss/len(train_loader)).item()\n",
    "        eval_loss = (eval_loss/len(eval_loader)).item() \n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['eval_loss'].append(eval_loss)  \n",
    "        history['detail_train'].append(detail_train.cpu().detach())\n",
    "        history['detail_eval'].append(detail_eval.cpu().detach())  \n",
    "        \n",
    "        if not (epoch+1)%10:\n",
    "            print(f\"epoch {epoch+1}\\ntrain loss: {train_loss}\\t\\teval loss: {eval_loss}\")\n",
    "        \n",
    "        if not (epoch+1)%50:\n",
    "            res = torch.cat(pred)\n",
    "            tru = torch.cat(real)\n",
    "            print(f'\\nepoch {epoch+1}:\\n')\n",
    "            print(f\"f1_score for 50 classes: {f1_score(tru, np.round(res), average='macro')}\")\n",
    "            print(classification_report(tru, np.round(res)))\n",
    "            print(f'\\nSpent time: {time.time()-start} seconds')\n",
    "            \n",
    "        \n",
    "    print('Training complete!')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fe8f7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3420d9b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "epoch 10\n",
      "train loss: 0.18921305239200592\t\teval loss: 0.17735131084918976\n",
      "epoch 20\n",
      "train loss: 0.11131879687309265\t\teval loss: 0.0994197428226471\n",
      "epoch 30\n",
      "train loss: 0.07532984763383865\t\teval loss: 0.06357717514038086\n",
      "epoch 40\n",
      "train loss: 0.052062809467315674\t\teval loss: 0.0419219546020031\n",
      "epoch 50\n",
      "train loss: 0.038454361259937286\t\teval loss: 0.03218347206711769\n",
      "\n",
      "epoch 50:\n",
      "\n",
      "f1_score for 50 classes: 0.947996635489219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       890\n",
      "           1       0.97      0.98      0.98       862\n",
      "           2       0.99      0.96      0.97       984\n",
      "           3       0.98      0.97      0.98       640\n",
      "           4       0.96      0.97      0.96       639\n",
      "           5       0.93      0.98      0.96      1333\n",
      "           6       0.99      0.96      0.97       925\n",
      "           7       0.94      0.95      0.94      1081\n",
      "           8       0.96      0.87      0.91       477\n",
      "           9       0.95      0.86      0.90       911\n",
      "          10       0.95      0.92      0.94       539\n",
      "          11       0.96      0.99      0.98       789\n",
      "          12       0.97      0.96      0.96      1692\n",
      "          13       0.98      0.94      0.96      1117\n",
      "          14       0.99      0.97      0.98      1209\n",
      "          15       0.98      0.97      0.98       515\n",
      "          16       0.95      0.98      0.97      1144\n",
      "          17       0.95      0.97      0.96       482\n",
      "          18       0.99      1.00      0.99       880\n",
      "          19       0.97      0.78      0.86       414\n",
      "          20       0.92      0.93      0.93      1314\n",
      "          21       0.90      0.97      0.93       683\n",
      "          22       0.94      0.94      0.94       869\n",
      "          23       0.96      0.97      0.96      1296\n",
      "          24       0.97      0.96      0.97       953\n",
      "          25       0.96      0.97      0.96       938\n",
      "          26       0.98      0.91      0.95       511\n",
      "          27       0.97      0.98      0.97      1065\n",
      "          28       0.99      0.97      0.98       703\n",
      "          29       0.96      0.94      0.95      1550\n",
      "          30       0.98      0.94      0.96       712\n",
      "          31       0.87      0.96      0.92       817\n",
      "          32       0.99      0.94      0.96       482\n",
      "          33       0.99      0.96      0.98       952\n",
      "          34       0.98      0.94      0.96      1040\n",
      "          35       0.96      0.75      0.84       752\n",
      "          36       0.98      0.88      0.93       581\n",
      "          37       0.98      0.86      0.92       602\n",
      "          38       0.97      0.97      0.97      1543\n",
      "          39       0.96      0.92      0.94      1003\n",
      "          40       0.95      0.81      0.88       602\n",
      "          41       0.89      0.81      0.85       505\n",
      "          42       0.97      0.95      0.96       933\n",
      "          43       0.98      0.98      0.98      1072\n",
      "          44       0.97      0.98      0.97      1057\n",
      "          45       0.98      0.83      0.90       645\n",
      "          46       0.98      0.96      0.97      1670\n",
      "          47       0.97      0.97      0.97       823\n",
      "          48       0.99      0.97      0.98       824\n",
      "          49       0.96      0.88      0.92       703\n",
      "\n",
      "   micro avg       0.96      0.94      0.95     44723\n",
      "   macro avg       0.96      0.93      0.95     44723\n",
      "weighted avg       0.96      0.94      0.95     44723\n",
      " samples avg       0.94      0.92      0.92     44723\n",
      "\n",
      "\n",
      "Spent time: 659.3570377826691 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60\n",
      "train loss: 0.028784293681383133\t\teval loss: 0.026090368628501892\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19808/3143880373.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m hist = train_model(mod, train_, test_, device, norm,\n\u001b[0m\u001b[0;32m     10\u001b[0m                 lr=lr, epochs=epochs, batch_size=batch_size)\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19808/3513326977.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, eval_dataset, device, norm, lr, epochs, batch_size)\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mdetail_eval\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msave_eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0meval_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                 \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                 \u001b[0mreal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 256*3\n",
    "mod = final_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "\n",
    "hist = train_model(mod, train_, test_, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['train_loss'], label='Train')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['eval_loss'], label='Eval')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26554f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
