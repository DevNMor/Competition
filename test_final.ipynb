{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ccab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe71654",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('Preprocessing_Train.pkl')\n",
    "test = pd.read_csv('datasolve-us/test.csv')\n",
    "new = pd.read_csv('new.csv')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_neg_stop = [i for i in stop_words if \"n't\" not in i and \"no\" not in i]\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos(x):\n",
    "    if x.startswith('J'):\n",
    "        return 'a'\n",
    "    elif x.startswith('V'):\n",
    "        return 'v'\n",
    "    elif x.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "    \n",
    "def tok(doc, non_neg=False, lemmatized=False):\n",
    "    \n",
    "    if non_neg:\n",
    "        stop = non_neg_stop\n",
    "    else:\n",
    "        stop = stop_words\n",
    "       \n",
    "    tokens = re.split(r'\\s', doc.lower())\n",
    "    \n",
    "    if lemmatized:\n",
    "        pos_list = nltk.pos_tag(tokens)\n",
    "        tokens = list(map(lambda x: lemma.lemmatize(x[0], pos(x[1])), pos_list))    \n",
    "\n",
    "    tokens = [i for i in tokens if i not in stop]\n",
    "    tokens = [i for i in tokens if len(re.findall(r'\\w', i)) >= 2]\n",
    "    tokens = [re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)[0] for i in tokens if re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_voc_new(X, non_neg=True, lemmatized=True):\n",
    "    \n",
    "    voc = []\n",
    "    token_list = []\n",
    "    for i in X:\n",
    "        tokens = tok(i, non_neg, lemmatized)\n",
    "        token_list.append(tokens)\n",
    "        voc += tokens\n",
    "        \n",
    "    voc = list(set(voc))\n",
    "    \n",
    "    return voc, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e33c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, tk_list_train = get_voc_new(train.document_text, False, True)\n",
    "voc_test, tk_list_test = get_voc_new(test.document_text, False, True)\n",
    "voc_dic = {item: idx+1 for idx, item in enumerate(voc)}\n",
    "\n",
    "def word2idx(token_list):\n",
    "    \n",
    "    token_idx = []\n",
    "    for sent in token_list:\n",
    "        sent_list = []\n",
    "        for token in sent:\n",
    "            if token in voc_dic:\n",
    "                idx = voc_dic[token]\n",
    "            else:\n",
    "                idx = 0\n",
    "            sent_list.append(idx)\n",
    "        token_idx.append(sent_list)\n",
    "\n",
    "    return token_idx\n",
    "\n",
    "\n",
    "tk_idx_train = word2idx(tk_list_train)\n",
    "tk_idx_test = word2idx(tk_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "942b174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(vocabulary=voc)\n",
    "vec_train = tfidf.fit_transform(train.document_text)\n",
    "\n",
    "class _dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        super(_dataset, self).__init__()\n",
    "        self.X = torch.Tensor(X)\n",
    "        self.y = torch.Tensor(y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "class dataset_(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, max_len=1024):\n",
    "        super(dataset_, self).__init__()\n",
    "        pad = [i[:max_len] + [0]*(max_len-len(i[:max_len])) for i in X]\n",
    "        self.X = torch.tensor(pad, dtype=torch.int32)\n",
    "        self.y = torch.Tensor(np.array(list(y)))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "train_ = dataset_(tk_idx_train, train.label)\n",
    "_train = _dataset(vec_train.toarray(), np.array(list(train.label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "056f189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_target=50):\n",
    "\n",
    "        super(MLP_model, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(vec_train.shape[1], 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, dim_target),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Att(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256, dim_target=50):\n",
    "        super(Att, self).__init__()\n",
    "        self.Wq = nn.Linear(dim_emb, dim_target, bias=False)\n",
    "        self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = self.Wk(x) \n",
    "        V = self.Wv(x)\n",
    "        temp = self.Wq(K) \n",
    "        score = temp.transpose(-2, -1) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "        return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "\n",
    "\n",
    "class final_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size=len(voc)+1, dim_emb=256, dim_target=50):\n",
    "        super(final_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(voc_size, dim_emb, padding_idx=0),\n",
    "            nn.Dropout(0.5),\n",
    "            Att(dim_emb, dim_target),  ## (50, dim_emb)\n",
    "            nn.Linear(dim_emb, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c63bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_A, model_B, train_dataset_A, train_dataset_B, device, A_idx, B_idx,\n",
    "                lr=0.0005, epochs_A=50, epochs_B=50, batch_size=256):\n",
    "\n",
    "    \n",
    "    train_loader_A = DataLoader(train_dataset_A, batch_size=batch_size, shuffle=True)\n",
    "    train_loader_B = DataLoader(train_dataset_B, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model_A = model_A.to(device)\n",
    "    model_B = model_B.to(device)\n",
    "    Loss = nn.BCELoss().to(device)\n",
    "    op_A = torch.optim.Adam(model_A.parameters(), lr=lr)\n",
    "    op_B = torch.optim.Adam(model_B.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs_A):\n",
    "        \n",
    "        model_A.train()\n",
    "\n",
    "        for X, y in train_loader_A:\n",
    "            X = X.to(device)\n",
    "            y = y[:, A_idx].to(device)\n",
    "            out = model_A(X)\n",
    "            loss = Loss(out, y)\n",
    "            \n",
    "            op_A.zero_grad()\n",
    "            loss.backward()\n",
    "            op_A.step()\n",
    "      \n",
    "    for epoch in range(epochs_B):\n",
    "        \n",
    "        model_B.train()\n",
    "\n",
    "        for X, y in train_loader_B:\n",
    "            X = X.to(device)\n",
    "            y = y[:, B_idx].to(device)\n",
    "            out = model_B(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            \n",
    "            op_B.zero_grad()\n",
    "            loss.backward()\n",
    "            op_B.step()\n",
    "            \n",
    "            \n",
    "    print('Training complete!')\n",
    "    print(f'Spent time: {time.time()-start} seconds')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "676ef837",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = new.sort_values('word_emb_loss', ascending=False).reset_index(drop=True).label_idx[:10]\n",
    "target = torch.tensor(np.array(list(target)), dtype=torch.long)\n",
    "normal = [i for i in range(50) if i not in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7c6d001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "Training complete!\n",
      "Spent time: 272.9622218608856 seconds\n"
     ]
    }
   ],
   "source": [
    "model_A = MLP_model(dim_target=40)\n",
    "model_B = final_model(dim_target=10)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs_A = 100\n",
    "epochs_B = 100\n",
    "A_idx = normal\n",
    "B_idx = target\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "\n",
    "train_model(model_A, model_B, _train, train_, device, A_idx, B_idx,\n",
    "                lr, epochs_A, epochs_B, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8cddb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_test = [i[:1024] + [0]*(1024-len(i[:1024])) for i in tk_idx_test]\n",
    "test_ = torch.tensor(pad_test, dtype=torch.int32)\n",
    "vec_test = tfidf.transform(test.document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d87be759",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = test_.cpu()\n",
    "# model_A = model_A.cpu()\n",
    "model_B = model_B.cpu()\n",
    "# res_A = []\n",
    "res_B = []\n",
    "with torch.no_grad():\n",
    "    for i in DataLoader(test_, batch_size=256, shuffle=False):\n",
    "#         res_A.append(model_A(i).cpu())\n",
    "        res_B.append(model_B(i).cpu())\n",
    "    \n",
    "xxx = torch.Tensor(vec_test.toarray()).to(device)\n",
    "with torch.no_grad():\n",
    "    res_A = model_A(xxx)\n",
    "\n",
    "rr = res_A.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6549e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin_A = torch.cat(res_A, dim=0).squeeze(-1)\n",
    "fin_B = torch.cat(res_B, dim=0).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6aced9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = torch.zeros(4993,50)\n",
    "final[:, normal] = rr\n",
    "final[:, target] = fin_B\n",
    "\n",
    "fin = np.round(final.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea75131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.read_csv('tfidf.csv').predictions\n",
    "att = pd.read_csv('att_3.csv').predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1da90111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9863328660124174"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(tfidf) == np.array(fin)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4561ee97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9662247146004406"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(att) == np.array(fin)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfa63ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fin, columns=['predictions']).to_csv('class_2_re.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13728d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
