{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "088b93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c2794ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize needed variables and set up pipelines for tokenizations\n",
    "\n",
    "train = pd.read_pickle('D:/Stevens/Semester_academic/Semester 3/667/Competition_DataSolver/Result/Preprocessing_Train.pkl')\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.document_text, train.label, test_size=0.25)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_neg_stop = [i for i in stop_words if \"n't\" not in i and \"no\" not in i]\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos(x):\n",
    "    if x.startswith('J'):\n",
    "        return 'a'\n",
    "    elif x.startswith('V'):\n",
    "        return 'v'\n",
    "    elif x.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "    \n",
    "def tok(doc, non_neg=False, lemmatized=False):\n",
    "    \n",
    "    if non_neg:\n",
    "        stop = non_neg_stop\n",
    "    else:\n",
    "        stop = stop_words\n",
    "       \n",
    "    tokens = re.split(r'\\s', doc.lower())\n",
    "    \n",
    "    if lemmatized:\n",
    "        pos_list = nltk.pos_tag(tokens)\n",
    "        tokens = list(map(lambda x: lemma.lemmatize(x[0], pos(x[1])), pos_list))    \n",
    "\n",
    "    tokens = [i for i in tokens if i not in stop]\n",
    "    tokens = [i for i in tokens if len(re.findall(r'\\w', i)) >= 2]\n",
    "    tokens = [re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)[0] for i in tokens if re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_voc_new(X, non_neg=True, lemmatized=True):\n",
    "    \n",
    "    voc = []\n",
    "    token_list = []\n",
    "    for i in X:\n",
    "        tokens = tok(i, non_neg, lemmatized)\n",
    "        token_list.append(tokens)\n",
    "        voc += tokens\n",
    "        \n",
    "    voc = list(set(voc))\n",
    "    \n",
    "    return voc, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "525aca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize train set and test set, create a vocabulary-index dictionary only based on train set\n",
    "\n",
    "voc, tk_list_train = get_voc_new(X_train, False, True)\n",
    "# voc, tk_list_train = get_voc_new(train.document_text, False, True)\n",
    "voc_test, tk_list_test = get_voc_new(X_test, False, True)\n",
    "voc_dic = {item: idx+1 for idx, item in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bad4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert tokens to index according to the dictionary we create in the last step \n",
    "## The unfound tokens would be padded with 0\n",
    "\n",
    "def word2idx(token_list):\n",
    "    \n",
    "    token_idx = []\n",
    "    for sent in token_list:\n",
    "        sent_list = []\n",
    "        for token in sent:\n",
    "            if token in voc_dic:\n",
    "                idx = voc_dic[token]\n",
    "            else:\n",
    "                idx = 0\n",
    "            sent_list.append(idx)\n",
    "        token_idx.append(sent_list)\n",
    "\n",
    "    return token_idx\n",
    "\n",
    "\n",
    "tk_idx_train = word2idx(tk_list_train)\n",
    "tk_idx_test = word2idx(tk_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ba50949",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Pytorch Dataset class to convert data to tensors\n",
    "## Each token index list is set to the same length, 1024 by default\n",
    "## The shorter one would be padded with 0, and the longer one would be truncated\n",
    "\n",
    "class dataset_(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, max_len=1024):\n",
    "        super(dataset_, self).__init__()\n",
    "        pad = [i[:max_len] + [0]*(max_len-len(i[:max_len])) for i in X]\n",
    "        self.X = torch.tensor(pad, dtype=torch.int32)\n",
    "        self.y = torch.Tensor(np.array(list(y)))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "train_ = dataset_(tk_idx_train, y_train)\n",
    "# train_ = dataset_(tk_idx_train, train.label)\n",
    "test_ = dataset_(tk_idx_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75129f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PE(nn.Module): \n",
    "\n",
    "#     def __init__(self, dim_emb=256, dropout=0.5, max_len=1024): \n",
    "#         super(PE, self).__init__() \n",
    "#         self.dropout = nn.Dropout(dropout) \n",
    "#         pe = torch.zeros(max_len, dim_emb) \n",
    "#         position = torch.arange(0, max_len).unsqueeze(1) \n",
    "#         div_term = torch.exp(-math.log(10000) * torch.arange(0, dim_emb, 2)/dim_emb)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term) \n",
    "#         pe = pe.unsqueeze(0) \n",
    "#         self.register_buffer('pe', pe) \n",
    "        \n",
    "#     def forward(self, x): \n",
    "#         x = x + Variable(self.pe[:, :], requires_grad=False) \n",
    "#         return self.dropout(x) \n",
    "\n",
    "    \n",
    "# class MultAtt(nn.Module):\n",
    "    \n",
    "#     def __init__(self, dim_emb=256):\n",
    "#         super(Att, self).__init__()\n",
    "#         self.Wq = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "#         self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "#         self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "#         self.Wo = nn.Linear(dim_emb, 50, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         Q = self.Wq(x)\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "#         interval = int(K.size(-1)/4)\n",
    "#         Z = torch.cat([torch.matmul(nn.Softmax(dim=-1)(torch.matmul(\n",
    "#                 Q[:,:, i:i+interval], K[:,:, i:i+interval].transpose(-2, -1))\n",
    "#                 /math.sqrt(interval)), V[:,:, i:i+interval]) \n",
    "#                 for i in range(0, K.size(-1), interval)], dim=-1)\n",
    "#         return self.Wo(Z).transpose(-2, -1)\n",
    "\n",
    "\n",
    "class Emb(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, dim_emb):\n",
    "        super(Emb, self).__init__()\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Embedding(voc_size, dim_emb, padding_idx=0),\n",
    "            nn.Linear(dim_emb, 256),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "    \n",
    "class Conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out):\n",
    "        super(Conv, self).__init__()\n",
    "        self.cv1 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 1),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2))\n",
    "        self.cv2 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 2),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2)) \n",
    "        self.cv3 = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, 3),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        C1 = self.cv1(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C2 = self.cv2(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        C3 = self.cv3(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        return torch.cat([C1,C2,C3], dim=-1)\n",
    "    \n",
    "    \n",
    "class EndConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out, conv_size):\n",
    "        super(EndConv, self).__init__()\n",
    "        self.ecv = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, conv_size),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z = self.ecv(x.transpose(-2, -1))\n",
    "        return nn.MaxPool1d(Z.size(-1))(Z).squeeze(-1)\n",
    "\n",
    "    \n",
    "# class SelfAtt(nn.Module):\n",
    "    \n",
    "#     def __init__(self, dim_emb=256, dim_enc=256):\n",
    "#         super(SelfAtt, self).__init__()\n",
    "#         self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "#         self.Wk = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "#         self.Wv = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         Q = self.Wq(x)\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "#         score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "#         return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "            \n",
    "class Att(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256, dim_enc=128):\n",
    "        super(Att, self).__init__()\n",
    "        self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "        self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        temp = self.Wq(K) \n",
    "        score = temp.transpose(-2, -1) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "        return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "    \n",
    "class LN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256):\n",
    "        super(LN, self).__init__()\n",
    "        self.feed = nn.LayerNorm(dim_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.feed(x)\n",
    "\n",
    "\n",
    "class final_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size=len(voc)+1, dim_emb=256, dropout=0.5, max_len=1024):\n",
    "        super(final_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Emb(voc_size, dim_emb),  ## (1024, dim_emb)\n",
    "            Conv(dim_emb, 256),\n",
    "            nn.Dropout(dropout),\n",
    "            Att(768, 50),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ceb4104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, eval_dataset, device, norm=0.5,\n",
    "                lr=0.0005, epochs=50, batch_size=256):\n",
    "    \n",
    "    history = {'train_loss': [], 'eval_loss': [], 'detail_train': [], 'detail_eval': []}\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "    save_loss = nn.BCELoss(reduction='none').to(device)\n",
    "    \n",
    "    if norm:\n",
    "        Loss = nn.BCELoss(weight=train_.y.sum(axis=0)**-norm).to(device)\n",
    "\n",
    "    else:\n",
    "        Loss = nn.BCELoss().to(device)\n",
    "        \n",
    "    op = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        eval_loss = 0\n",
    "        detail_train = torch.zeros(50).to(device)\n",
    "        detail_eval = torch.zeros(50).to(device)\n",
    "        pred = []\n",
    "        real = []\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            save_train = save_loss(out, y).sum(0)\n",
    "            \n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "            detail_train += save_train\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in eval_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(X).squeeze(-1)\n",
    "                loss = Loss(out, y)\n",
    "                save_eval = save_loss(out, y).sum(0)\n",
    "                detail_eval += save_eval\n",
    "                eval_loss += loss\n",
    "                pred.append(out.cpu())\n",
    "                real.append(y.cpu())\n",
    "                \n",
    "        train_loss = (train_loss/len(train_loader)).item()\n",
    "        eval_loss = (eval_loss/len(eval_loader)).item() \n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['eval_loss'].append(eval_loss)  \n",
    "        history['detail_train'].append(detail_train.cpu().detach())\n",
    "        history['detail_eval'].append(detail_eval.cpu().detach())  \n",
    "        \n",
    "        if not (epoch+1)%10:\n",
    "            print(f\"epoch {epoch+1}\\ntrain loss: {train_loss}\\t\\teval loss: {eval_loss}\")\n",
    "        \n",
    "        if not (epoch+1)%50:\n",
    "            res = torch.cat(pred)\n",
    "            tru = torch.cat(real)\n",
    "            print(f'\\nepoch {epoch+1}:\\n')\n",
    "            print(f\"f1_score for 50 classes: {f1_score(tru, np.round(res), average='macro')}\")\n",
    "            print(classification_report(tru, np.round(res)))\n",
    "            print(f'\\nSpent time: {time.time()-start} seconds')\n",
    "            \n",
    "        \n",
    "    print('Training complete!')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "136ece5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3806"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8da9342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "epoch 10\n",
      "train loss: 0.2387036830186844\t\teval loss: 0.23205509781837463\n",
      "epoch 20\n",
      "train loss: 0.15070392191410065\t\teval loss: 0.15272749960422516\n",
      "epoch 30\n",
      "train loss: 0.10736442357301712\t\teval loss: 0.12714196741580963\n",
      "epoch 40\n",
      "train loss: 0.08185892552137375\t\teval loss: 0.1246989518404007\n",
      "epoch 50\n",
      "train loss: 0.06472375988960266\t\teval loss: 0.12580625712871552\n",
      "\n",
      "epoch 50:\n",
      "\n",
      "f1_score for 50 classes: 0.7798970055958538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77       215\n",
      "           1       0.81      0.80      0.80       208\n",
      "           2       0.87      0.86      0.86       255\n",
      "           3       0.90      0.77      0.83       200\n",
      "           4       0.86      0.94      0.90       171\n",
      "           5       0.88      0.94      0.91       360\n",
      "           6       0.89      0.86      0.87       248\n",
      "           7       0.78      0.82      0.80       285\n",
      "           8       0.77      0.46      0.58       128\n",
      "           9       0.60      0.57      0.58       249\n",
      "          10       0.77      0.83      0.80       146\n",
      "          11       0.91      0.86      0.89       185\n",
      "          12       0.69      0.74      0.72       418\n",
      "          13       0.95      0.82      0.88       319\n",
      "          14       0.95      0.93      0.94       325\n",
      "          15       0.91      0.79      0.84       127\n",
      "          16       0.85      0.81      0.83       302\n",
      "          17       0.66      0.75      0.70       122\n",
      "          18       0.96      0.87      0.91       218\n",
      "          19       0.66      0.48      0.56       106\n",
      "          20       0.77      0.70      0.74       366\n",
      "          21       0.83      0.81      0.82       176\n",
      "          22       0.52      0.70      0.60       194\n",
      "          23       0.75      0.77      0.76       331\n",
      "          24       0.79      0.68      0.73       250\n",
      "          25       0.78      0.79      0.79       239\n",
      "          26       0.76      0.72      0.74       132\n",
      "          27       0.88      0.89      0.89       267\n",
      "          28       0.89      0.89      0.89       178\n",
      "          29       0.77      0.77      0.77       438\n",
      "          30       0.81      0.59      0.68       186\n",
      "          31       0.64      0.73      0.69       233\n",
      "          32       0.93      0.76      0.84       148\n",
      "          33       0.99      0.89      0.93       267\n",
      "          34       0.85      0.73      0.79       261\n",
      "          35       0.46      0.50      0.48       203\n",
      "          36       0.98      0.86      0.91       150\n",
      "          37       0.76      0.63      0.69       171\n",
      "          38       0.75      0.80      0.78       410\n",
      "          39       0.74      0.71      0.72       273\n",
      "          40       0.61      0.58      0.59       151\n",
      "          41       0.71      0.59      0.65       125\n",
      "          42       0.80      0.86      0.83       255\n",
      "          43       0.94      0.88      0.91       273\n",
      "          44       0.79      0.75      0.77       271\n",
      "          45       0.85      0.71      0.77       171\n",
      "          46       0.80      0.78      0.79       414\n",
      "          47       0.84      0.88      0.86       177\n",
      "          48       0.88      0.78      0.83       207\n",
      "          49       0.82      0.79      0.80       150\n",
      "\n",
      "   micro avg       0.80      0.78      0.79     11654\n",
      "   macro avg       0.80      0.76      0.78     11654\n",
      "weighted avg       0.81      0.78      0.79     11654\n",
      " samples avg       0.78      0.75      0.74     11654\n",
      "\n",
      "\n",
      "Spent time: 275.85157561302185 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60\n",
      "train loss: 0.053545087575912476\t\teval loss: 0.13724371790885925\n",
      "epoch 70\n",
      "train loss: 0.043245829641819\t\teval loss: 0.15043507516384125\n",
      "epoch 80\n",
      "train loss: 0.03669610992074013\t\teval loss: 0.1499050408601761\n",
      "epoch 90\n",
      "train loss: 0.030698789283633232\t\teval loss: 0.15864543616771698\n",
      "epoch 100\n",
      "train loss: 0.026505613699555397\t\teval loss: 0.16760171949863434\n",
      "\n",
      "epoch 100:\n",
      "\n",
      "f1_score for 50 classes: 0.7974522554128746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.73      0.78       215\n",
      "           1       0.80      0.83      0.82       208\n",
      "           2       0.91      0.86      0.88       255\n",
      "           3       0.89      0.83      0.86       200\n",
      "           4       0.85      0.91      0.88       171\n",
      "           5       0.88      0.94      0.91       360\n",
      "           6       0.94      0.87      0.90       248\n",
      "           7       0.83      0.80      0.81       285\n",
      "           8       0.68      0.58      0.62       128\n",
      "           9       0.67      0.59      0.63       249\n",
      "          10       0.78      0.78      0.78       146\n",
      "          11       0.92      0.87      0.89       185\n",
      "          12       0.70      0.78      0.74       418\n",
      "          13       0.95      0.85      0.90       319\n",
      "          14       0.96      0.91      0.93       325\n",
      "          15       0.91      0.75      0.82       127\n",
      "          16       0.82      0.82      0.82       302\n",
      "          17       0.69      0.83      0.75       122\n",
      "          18       0.95      0.89      0.92       218\n",
      "          19       0.73      0.62      0.67       106\n",
      "          20       0.79      0.74      0.76       366\n",
      "          21       0.81      0.89      0.85       176\n",
      "          22       0.53      0.64      0.58       194\n",
      "          23       0.75      0.76      0.75       331\n",
      "          24       0.82      0.66      0.73       250\n",
      "          25       0.85      0.72      0.78       239\n",
      "          26       0.80      0.73      0.76       132\n",
      "          27       0.87      0.90      0.88       267\n",
      "          28       0.90      0.88      0.89       178\n",
      "          29       0.82      0.75      0.78       438\n",
      "          30       0.88      0.66      0.75       186\n",
      "          31       0.69      0.71      0.70       233\n",
      "          32       0.91      0.82      0.87       148\n",
      "          33       0.99      0.91      0.95       267\n",
      "          34       0.87      0.74      0.80       261\n",
      "          35       0.56      0.52      0.54       203\n",
      "          36       0.95      0.87      0.91       150\n",
      "          37       0.78      0.70      0.74       171\n",
      "          38       0.78      0.80      0.79       410\n",
      "          39       0.77      0.71      0.74       273\n",
      "          40       0.65      0.66      0.66       151\n",
      "          41       0.73      0.70      0.71       125\n",
      "          42       0.84      0.85      0.85       255\n",
      "          43       0.96      0.87      0.92       273\n",
      "          44       0.83      0.75      0.79       271\n",
      "          45       0.83      0.71      0.77       171\n",
      "          46       0.82      0.77      0.79       414\n",
      "          47       0.86      0.86      0.86       177\n",
      "          48       0.92      0.82      0.87       207\n",
      "          49       0.85      0.74      0.79       150\n",
      "\n",
      "   micro avg       0.82      0.78      0.80     11654\n",
      "   macro avg       0.82      0.78      0.80     11654\n",
      "weighted avg       0.83      0.78      0.80     11654\n",
      " samples avg       0.80      0.76      0.75     11654\n",
      "\n",
      "\n",
      "Spent time: 553.1758568286896 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110\n",
      "train loss: 0.024284882470965385\t\teval loss: 0.1623009890317917\n",
      "epoch 120\n",
      "train loss: 0.022168483585119247\t\teval loss: 0.18639473617076874\n",
      "epoch 130\n",
      "train loss: 0.018245453014969826\t\teval loss: 0.20770154893398285\n",
      "epoch 140\n",
      "train loss: 0.01770789921283722\t\teval loss: 0.19166992604732513\n",
      "epoch 150\n",
      "train loss: 0.016199849545955658\t\teval loss: 0.21389655768871307\n",
      "\n",
      "epoch 150:\n",
      "\n",
      "f1_score for 50 classes: 0.8000486338035223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.72      0.80       215\n",
      "           1       0.89      0.81      0.85       208\n",
      "           2       0.91      0.84      0.88       255\n",
      "           3       0.92      0.82      0.87       200\n",
      "           4       0.89      0.90      0.90       171\n",
      "           5       0.89      0.92      0.90       360\n",
      "           6       0.96      0.86      0.91       248\n",
      "           7       0.84      0.77      0.80       285\n",
      "           8       0.85      0.53      0.65       128\n",
      "           9       0.81      0.56      0.66       249\n",
      "          10       0.83      0.70      0.76       146\n",
      "          11       0.94      0.83      0.88       185\n",
      "          12       0.77      0.74      0.76       418\n",
      "          13       0.97      0.83      0.90       319\n",
      "          14       0.95      0.89      0.92       325\n",
      "          15       0.95      0.74      0.83       127\n",
      "          16       0.86      0.81      0.83       302\n",
      "          17       0.76      0.75      0.76       122\n",
      "          18       0.98      0.89      0.93       218\n",
      "          19       0.75      0.62      0.68       106\n",
      "          20       0.84      0.71      0.77       366\n",
      "          21       0.85      0.84      0.85       176\n",
      "          22       0.63      0.58      0.60       194\n",
      "          23       0.83      0.66      0.74       331\n",
      "          24       0.85      0.63      0.73       250\n",
      "          25       0.85      0.72      0.78       239\n",
      "          26       0.81      0.70      0.75       132\n",
      "          27       0.90      0.86      0.88       267\n",
      "          28       0.94      0.87      0.90       178\n",
      "          29       0.82      0.73      0.77       438\n",
      "          30       0.93      0.66      0.77       186\n",
      "          31       0.69      0.67      0.68       233\n",
      "          32       0.88      0.82      0.85       148\n",
      "          33       0.99      0.91      0.95       267\n",
      "          34       0.91      0.72      0.81       261\n",
      "          35       0.65      0.48      0.56       203\n",
      "          36       0.96      0.87      0.91       150\n",
      "          37       0.81      0.68      0.74       171\n",
      "          38       0.83      0.73      0.78       410\n",
      "          39       0.87      0.66      0.75       273\n",
      "          40       0.73      0.62      0.67       151\n",
      "          41       0.74      0.66      0.70       125\n",
      "          42       0.90      0.82      0.86       255\n",
      "          43       0.97      0.85      0.91       273\n",
      "          44       0.86      0.72      0.78       271\n",
      "          45       0.91      0.67      0.77       171\n",
      "          46       0.85      0.72      0.78       414\n",
      "          47       0.89      0.81      0.85       177\n",
      "          48       0.92      0.81      0.86       207\n",
      "          49       0.91      0.73      0.81       150\n",
      "\n",
      "   micro avg       0.86      0.76      0.81     11654\n",
      "   macro avg       0.86      0.75      0.80     11654\n",
      "weighted avg       0.86      0.76      0.80     11654\n",
      " samples avg       0.82      0.73      0.75     11654\n",
      "\n",
      "\n",
      "Spent time: 833.5676417350769 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 160\n",
      "train loss: 0.013926589861512184\t\teval loss: 0.21660366654396057\n",
      "epoch 170\n",
      "train loss: 0.012624039314687252\t\teval loss: 0.2681832015514374\n",
      "epoch 180\n",
      "train loss: 0.011796997860074043\t\teval loss: 0.2241203635931015\n",
      "epoch 190\n",
      "train loss: 0.011178610846400261\t\teval loss: 0.22798240184783936\n",
      "epoch 200\n",
      "train loss: 0.010977676138281822\t\teval loss: 0.21529622375965118\n",
      "\n",
      "epoch 200:\n",
      "\n",
      "f1_score for 50 classes: 0.8028228993075582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.73      0.81       215\n",
      "           1       0.85      0.85      0.85       208\n",
      "           2       0.95      0.85      0.90       255\n",
      "           3       0.92      0.83      0.87       200\n",
      "           4       0.89      0.92      0.90       171\n",
      "           5       0.89      0.92      0.90       360\n",
      "           6       0.95      0.87      0.91       248\n",
      "           7       0.81      0.81      0.81       285\n",
      "           8       0.80      0.54      0.64       128\n",
      "           9       0.67      0.57      0.61       249\n",
      "          10       0.81      0.75      0.78       146\n",
      "          11       0.90      0.84      0.87       185\n",
      "          12       0.78      0.69      0.74       418\n",
      "          13       0.96      0.84      0.90       319\n",
      "          14       0.95      0.90      0.93       325\n",
      "          15       0.95      0.73      0.83       127\n",
      "          16       0.86      0.81      0.83       302\n",
      "          17       0.78      0.68      0.73       122\n",
      "          18       0.98      0.89      0.93       218\n",
      "          19       0.82      0.53      0.64       106\n",
      "          20       0.79      0.74      0.76       366\n",
      "          21       0.86      0.80      0.83       176\n",
      "          22       0.56      0.59      0.57       194\n",
      "          23       0.80      0.73      0.77       331\n",
      "          24       0.86      0.67      0.75       250\n",
      "          25       0.86      0.73      0.79       239\n",
      "          26       0.87      0.67      0.76       132\n",
      "          27       0.91      0.86      0.88       267\n",
      "          28       0.93      0.88      0.91       178\n",
      "          29       0.83      0.72      0.77       438\n",
      "          30       0.93      0.70      0.80       186\n",
      "          31       0.77      0.65      0.71       233\n",
      "          32       0.94      0.81      0.87       148\n",
      "          33       0.98      0.90      0.94       267\n",
      "          34       0.88      0.75      0.81       261\n",
      "          35       0.70      0.45      0.55       203\n",
      "          36       0.97      0.87      0.92       150\n",
      "          37       0.86      0.70      0.77       171\n",
      "          38       0.81      0.77      0.79       410\n",
      "          39       0.85      0.66      0.74       273\n",
      "          40       0.76      0.66      0.70       151\n",
      "          41       0.80      0.67      0.73       125\n",
      "          42       0.89      0.81      0.85       255\n",
      "          43       0.96      0.89      0.92       273\n",
      "          44       0.81      0.72      0.76       271\n",
      "          45       0.89      0.68      0.77       171\n",
      "          46       0.83      0.77      0.80       414\n",
      "          47       0.84      0.85      0.85       177\n",
      "          48       0.91      0.84      0.87       207\n",
      "          49       0.80      0.79      0.80       150\n",
      "\n",
      "   micro avg       0.86      0.77      0.81     11654\n",
      "   macro avg       0.86      0.76      0.80     11654\n",
      "weighted avg       0.86      0.77      0.81     11654\n",
      " samples avg       0.82      0.74      0.75     11654\n",
      "\n",
      "\n",
      "Spent time: 1110.2663295269012 seconds\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABINElEQVR4nO3dd3hc1bXw4d+aUbUky7IkN8lF7rgXgcGmmJhiqgnVQAgtH4HAhYQkBEICBMK9BHKTXAKBQEIgAUJICMT0aqqxccW9yF2usmxVW319f+wz1kgeybKs0ais93n0zMxpszWWz5rd1hZVxRhjjKnPF+kCGGOMaZssQBhjjAnJAoQxxpiQLEAYY4wJyQKEMcaYkCxAGGOMCckChDHGmJAsQBjTDCKySUROi3Q5jAknCxDGGGNCsgBhTAsRkVgR+Z2IbPd+ficisd6+NBF5Q0QKRGSviHwmIj5v309EZJuIFIvIGhGZFtnfxBgnKtIFMKYDuRs4HhgHKPAf4GfAz4EfArlAunfs8YCKyDDgFuBYVd0uIgMAf+sW25jQrAZhTMu5ErhfVXerah7wC+Aqb18l0Bvor6qVqvqZukRo1UAsMEJEolV1k6quj0jpjanHAoQxLacPsDno9WZvG8AjQA7wnohsEJE7AVQ1B/g+cB+wW0ReEpE+GNMGWIAwpuVsB/oHve7nbUNVi1X1h6o6EDgfuD3Q16CqL6rqid65CvyqdYttTGgWIIxpvmgRiQv8AH8HfiYi6SKSBtwDPA8gIueKyGAREaAQ17RUIyLDROQbXmd2GXAAqInMr2NMXRYgjGm+t3A39MBPHLAAWAosAxYBv/SOHQJ8AJQAXwJ/UNXZuP6Hh4A9wE6gB3BX6/0KxjRMbMEgY4wxoVgNwhhjTEgWIIwxxoRkAcIYY0xIFiCMMcaE1GFSbaSlpemAAQMiXQxjjGlXFi5cuEdV00Pt6zABYsCAASxYsCDSxTDGmHZFRDY3tM+amIwxxoQU1gAhItO99MU5gdwzDRx3kYioiGQHbbvLO2+NiJwZznIaY4w5VNiamETEDzwOnI5LczxfRGap6sp6xyUBtwHzgraNAGYCI3HJzj4QkaGqWh2u8hpjjKkrnH0QxwE5qroBQEReAmYAK+sd9wAuOdmPg7bNAF5S1XJgo4jkeNf7MozlNcZ0MpWVleTm5lJWVhbpooRdXFwcmZmZREdHN/mccAaIDGBr0OtcYFLwASIyAeirqm+KyI/rnTu33rkZ9d9ARG4AbgDo169fCxXbGNNZ5ObmkpSUxIABA3B5FDsmVSU/P5/c3FyysrKafF7EOqm95RZ/g1tpq1lU9SlVzVbV7PT0kKO0jDGmQWVlZaSmpnbo4AAgIqSmph5xTSmcNYhtQN+g15netoAkYBTwsfeP0wuYJSLnN+FcY4xpER09OAQ05/cMZw1iPjBERLJEJAbX6TwrsFNVC1U1TVUHqOoAXJPS+aq6wDtuprcIfBYuVfJX4ShkSXkVv3l/LYu37AvH5Y0xpt0KW4BQ1SrcYuzvAquAl1V1hYjc79USGjt3BfAyrkP7HeDmcI1gqqyq4dEP17Fka0E4Lm+MMQ3Kz89n3LhxjBs3jl69epGRkXHwdUVFRaPnLliwgFtvvTWs5QvrTGpVfQu3qErwtnsaOHZqvdcPAg+GrXCe+Bg/AAcqbQStMaZ1paamsmTJEgDuu+8+EhMT+dGPfnRwf1VVFVFRoW/T2dnZZGdnh9zXUjr9TOrYKPcRlFXaKo/GmMi75ppruPHGG5k0aRJ33HEHX331FSeccALjx49n8uTJrFmzBoCPP/6Yc889F3DB5brrrmPq1KkMHDiQRx99tEXK0mFyMTWXiBAX7aPMahDGdGq/eH0FK7cXteg1R/Tpyr3njTzi83Jzc5kzZw5+v5+ioiI+++wzoqKi+OCDD/jpT3/KK6+8csg5q1evZvbs2RQXFzNs2DBuuummI5rzEEqnDxAA8dF+DlRYgDDGtA2XXHIJfr9r/i4sLOTqq69m3bp1iAiVlZUhzznnnHOIjY0lNjaWHj16sGvXLjIzM4+qHBYg8AKE1SCM6dSa800/XBISEg4+//nPf86pp57Kq6++yqZNm5g6dWrIc2JjYw8+9/v9VFVVHXU5On0fBEBcjN+amIwxbVJhYSEZGS6RxLPPPtuq720BAoiLsgBhjGmb7rjjDu666y7Gjx/fIrWCIyGq2qpvGC7Z2dna3AWDLnpiDnHRPl74zvEtXCpjTFu2atUqjjnmmEgXo9WE+n1FZKGqhhwvazUIrJPaGGNCsQABxEX7bR6EMcbUYwECN5va+iCMMaYuCxBAXJTPhrkaY0w9FiBwNQgLEMYYU5cFCKyT2hhjQrEAgeukLq+qoaamYwz5Nca0H36//2CK73HjxvHQQw816zpTp06luUP9G2KpNnABAqC8quZg+m9jjGkN8fHxB1N+tzVWgwDio93HYP0Qxpi24J133uGSSy45+Do4tfdNN91EdnY2I0eO5N577w1rOawGgS0aZIwB3r4Tdi5r2Wv2Gg1nNd5kdODAAcaNG3fw9V133cVFF13EDTfcQGlpKQkJCfzjH/9g5syZADz44IN0796d6upqpk2bxtKlSxkzZkzLltsT1hqEiEwXkTUikiMid4bYf6OILBORJSLyuYiM8LYPEJED3vYlIvJkOMsZaGKyuRDGmNYWaGIK/Fx22WVERUUxffp0Xn/9daqqqnjzzTeZMWMGAC+//DITJkxg/PjxrFixgpUrV4atbGGrQYiIH3gcOB3IBeaLyCxVDf5tXlTVJ73jzwd+A0z39q1X1XHhKl+weC9A2EgmYzqxw3zTb20zZ87kscceo3v37mRnZ5OUlMTGjRv59a9/zfz580lJSeGaa66hrKwsbGUIZw3iOCBHVTeoagXwEjAj+ABVDV6+KQGIyDAiq0EYY9qaU045hUWLFvH0008fbF4qKioiISGB5ORkdu3axdtvvx3WMoSzDyID2Br0OheYVP8gEbkZuB2IAb4RtCtLRBYDRcDPVPWzEOfeANwA0K9fv2YX1PogjDGRUr8PYvr06Tz00EP4/X7OPfdcnn32WZ577jkAxo4dy/jx4xk+fDh9+/ZlypQpYS1bxDupVfVx4HERuQL4GXA1sAPop6r5IjIReE1ERtarcaCqTwFPgUv33dwyxB+sQVjCPmNM66qubviL6WOPPcZjjz1WZ1tDiwZ9/PHHLVgqJ5xNTNuAvkGvM71tDXkJuABAVctVNd97vhBYDwwNTzFrm5isBmGMMbXCGSDmA0NEJEtEYoCZwKzgA0RkSNDLc4B13vZ0r5MbERkIDAE2hKugcd48iDLrpDbGmIPC1sSkqlUicgvwLuAHnlHVFSJyP7BAVWcBt4jIaUAlsA/XvARwMnC/iFQCNcCNqro3XGWNtxqEMZ2WqiIikS5G2DVn9dCw9kGo6lvAW/W23RP0/LYGznsFeCWcZQtmndTGdE5xcXHk5+eTmpraoYOEqpKfn09cXNwRnRfxTuq2IC7Khrka0xllZmaSm5tLXl5epIsSdnFxcWRmZh7RORYgAJ9PiLVFg4zpdKKjo8nKyop0MdosS9bniYv2Wye1McYEsQDhiY+2VeWMMSaYBQhPfIzfJsoZY0wQCxCeOKtBGGNMHRYgPHHRPhvFZIwxQSxAeOKj/Zbu2xhjgliA8FgntTHG1GUBwhMX47cmJmOMCWIBwhMfbaOYjDEmmAUIT1y0j9KKqmYltDLGmI7IAkRNDXz0S8Z23U/B/kqu+ct89pZWRLpUxhgTcRYg9q6HuU9y8dfX8X/TujBn/R7+9701kS6VMcZEnAWItCFw7ZtIVQUzFl3HzUMK+M+S7ZSWV0W6ZMYYE1EWIAB6j4Xr34W4rvxX7u0MrVjJm0t3RLpUxhgTURYgAroPhOvew5eQxv/EP8+L8zZHukTGGBNRYQ0QIjJdRNaISI6I3Bli/40iskxElojI5yIyImjfXd55a0TkzHCW86CknsjUOxlWk0P69g/Zkr+/Vd7WGGPaorAFCBHxA48DZwEjgMuDA4DnRVUdrarjgIeB33jnjgBmAiOB6cAfvOuF35iZVCZn8YOoV3h/5c5WeUtjjGmLwlmDOA7IUdUNqloBvATMCD5AVYuCXiYAgUkIM4CXVLVcVTcCOd71ws8fRfSUmxnh28zyZQtb5S2NMaYtCmeAyAC2Br3O9bbVISI3i8h6XA3i1iM89wYRWSAiC1p0TdnB0wBI2v4FBfttToQxpnOKeCe1qj6uqoOAnwA/O8Jzn1LVbFXNTk9Pb7lCpWRRkZjJZFnOR6t3t9x1jTGmHQlngNgG9A16nelta8hLwAXNPLdliRA95FSm+Ffwn8VbD3+8McZ0QOEMEPOBISKSJSIxuE7nWcEHiMiQoJfnAOu857OAmSISKyJZwBDgqzCW9RAycCpJ7Kdw/Vds3WujmYwxnU/YAoSqVgG3AO8Cq4CXVXWFiNwvIud7h90iIitEZAlwO3C1d+4K4GVgJfAOcLOqtm4u7qxTADjZt5R/LrBahDGm85GOkr00OztbFyxY0LIX/cvZ7N62gRnyez67cxpR/oh32RhjTIsSkYWqmh1qn93xGpN9HT2qdjCkdD5fbsiPdGmMMaZVWYBozDHnoV3SuCbmQ15d3Hp95MYY0xZYgGhMVCwy4dtMZSHdlv+VAxW2JKkxpvOwAHE4J/2Qgoyp3OP7M2vfejTSpTHGmFZjAeJwYhPpdu2/2EwfZN17kS6NMca0GgsQTeCLiqIgaQhdSzfZmtXGmE7DAkQTxfQYQobuYt3OgkgXxRhjWoUFiCbqkTWKaKlm2fKlkS6KMca0CgsQTZTazy1lsX398giXxBhjWocFiKZKHQzAgZ1rqamxfghjOpyKUihpwWUDOgALEE3VpTsV0V3pXZVLTl5JpEtjjGlpH/8PPHt2pEvRpliAaCoRqlMGkSU7WLm96PDHG2Pal8JtUGCJOYNZgDgCsT2HMtC3k5U7LEAY0+FUlELVAaiujHRJ2gwLEEfAlzaEPpJPTq6tMmdMh1PhNR2X2RfAAAsQRyIt0FG92ibMGdPRlBd7j4WRLUcbYgHiSPRwQ117lm1iV1F5hAtjjGlRFaXuscwCRIAFiCPRfSA1vmiG+baycof9ERnTobRmE1NVBTx7Liz7V/jf6yiENUCIyHQRWSMiOSJyZ4j9t4vIShFZKiIfikj/oH3VIrLE+5lV/9yI8EejqUMYKrk2ksmYjqbcCxDlrfB/uygXNn0Gr34Xcj4M//s1U9gChIj4gceBs4ARwOUiMqLeYYuBbFUdA/wLeDho3wFVHef9nE8b4e85ghFR21i+zQKEMR1GTQ1UBpqYWuH/dok30CUqHv59A7TRPs1w1iCOA3JUdYOqVgAvATOCD1DV2aq633s5F8gMY3laRo/h9NbdrM/dGemSGGNaSiA4QOv0QRR7948R58P+PXBgX/jfsxnCGSAygOBZJ7netoZcD7wd9DpORBaIyFwRuSDUCSJyg3fMgry8Vpoi73VUJxTlsLu4rHXe0xgTXuVB2RFao4kpUIPImOAeC7ZA0XZY/u/wv/cRaBOd1CLyLSAbeCRoc39VzQauAH4nIoPqn6eqT6lqtqpmp6ent05hexwDwFDfVpZutY5qYzqEiuAaRGsEiF0gPugz3r0u3ArznoR/XdumRlGFM0BsA/oGvc70ttUhIqcBdwPnq+rBsaOqus173AB8DIwPY1mbrtsANCqe4b5cvs4tiHRpjDEtoaK49nlrzIMo2QUJPaDbAPe6MBf25Ljn+zaH//2bKJwBYj4wRESyRCQGmAnUGY0kIuOBP+KCw+6g7SkiEus9TwOmACvDWNam8/mQXqM5PmYTS7YWRLo0xpiWUNHKfRAluyGxB3Tp7jqqC7ZC/jq3ryBEgKiMTHN22AKEqlYBtwDvAquAl1V1hYjcLyKBUUmPAInAP+sNZz0GWCAiXwOzgYdUtW0ECIABUxhWvZa1W3fajGpjOoJAH4QvuvWamBJ7ggh06wv7NsLejW7fvk11j137HjzUDzZ+Gv5y1RPWPghVfUtVh6rqIFV90Nt2j6rO8p6fpqo96w9nVdU5qjpaVcd6j38OZzmP2ICT8FPN0IqVbNhTevjjjTGRV14MHz0IFfsP3ReYJNe1d+hO6sJcmPMY7FrRMmUp2e0CBEByX9g8B2q8JIH1m5hy50N1Ofzz2lbPNtsmOqnbnb6TUF8UJ/hWMn/j3kiXxhjTFF+/BJ8+DJu/OHTfwQCRcWgNYsVr8NtR8N7d8LcLa4eoNldNDZR6TUzgahBlBe65L+rQJqb8ddAlFarK4MP7j+69j5AFiOaITYSMiZwYvZq5G/IjXRpjOrf3fg4bPjn8cYEhpCUhsjEHmpi69jm0D2LJi+5b/hX/dLWQf1zlbvLNdWAf1FRBUi/3Ojlo+lff4w+tQeTnQMZEGHQqbFsYdJ0C+PCBukN0W5gFiGaSAScyUnNYun6b9UMYEymVZTDnUVc7aEzRdtjypXteGiJABGoQSfWamKorXY1j8DQYegZM+znkfuX6DAq3wXs/g6ojTNxZsss9BmoQyf3cY3x36DPOzYkI3FNqaiB/vVvyuOdo2LuhNiB88jB89mtY/9GRvf8RsADRXFkn46eGAaWL2Zwfok3TGBN+Rd7I+b3rGz9u5SxA3dyDUOtOV5S40UTxKVBdUTtqaNsit2/gVPc6I9s97lkLy1+BOb+HNW8fej2AnA+8963nYIDw+iC6ebMB0oZAt/5u0aJALad4O1TudwGi12j3O+xe6WoZ85+uLUuYWIBorn4nUBPVham+r62ZyZhIKcx1j3s31G6rKj80Ad7qN6DHSHcDDtygg5WXuKbjuGTvtVeL2PgJIJB1snvtrQnDnrWwe5V7vuyfocv2ySPw9h2H5lkK3PyDO6kBUodAipevNNAPsccb+po2xAsQwM5lbv1s8UFct9pjwsACRHNFxSIDT2Za1Nd8uX5PpEtjTOcUqEGU5tV2Li/4Czx/Ye2II1V3U+03yTXrhGxiKoWYhNoAEbjWhk/cjblLd/c6PsVNcNuzFnZ711/3Xm0upfLi2lFSxTvcz94NsGslrH7LlaXE6+QONDEl9YaULBhwogtgAIufh/fvgbzV7nXqENdXEZcMG2a7NOETr3FNUmGsQUSF7cqdgAw5nYy177B13VJqasbj80mki2RM5xKoQYBrZuoz3t1AAbbOg54jXY2hrADSj3Hf3vNDNEdVlEBMEsR2da/LCqF0j+tvmHRj3WPThsLu1ZC3BvpPcX0UH9zn5lAsfh6GnQUXP1M72mnTZzD/Ty5IDTzVlSW6C8Qkuv3+KLhtiVcOL7gses49JqS745J6uTkTvcbAqtfdvonXwoI/w5K/u8AjLX//sRrE0Rh8OgDjyhewfHvbyZ9iTLvxz2vg9duaf35h0LyA/PVQXQWbvGGsW+e7x8C38PRhdWsQ2xfDS1fCmz903/xjEyHOCxDlhTD3CddJPf6quu+ZNgS2L3LDTsdd4ZquFj4LC/8CMV1g13JXo6j2Oq/nPeWCw5AzXZ9GwRYYdVHoG3pMFzjhFph2r+v3KM2D1EG1x/Yc5R77Hg89hrtgVVHsaiphYDWIo5HSn6ruQ/nmns/4ePUuxmR2i3SJjGk/VGH9bEhIa/41CnNdzSBvlZuJvH2Ru2FGJ7hv/+C+7QOkD3fNQ/v3Qt5a+NNpbripPxbSh7o+gUATU9F2+OppOOY8ty9Y2lBQb5hrjxHwnfddjSM+BT76pastBJq+YhJdU5Q/Fi78o+szONw3/TMfdI/DzoYnJrv3Cwj0Q0y8prYs4JqZuvY5kk+uSawGcZSipv6Y0b5NRH39fKSLYkz7Uprnmlv2bXLf1JujMNd9o0/q45qYAvMhsq918wf273U1iPgUV3tITAcU1rzpgsNJP3Tf9HevcjfzQBPT+/e4WsRJtx/6ngdv2OKCTkyCuzlHx3ujkMpcjQFcgAk8xqccWTNQj+Fw5csw9a7abSMvgOm/cjWQ4LKEqaPaAsTRGn0JW7tOYGbRMxTubaU1KYzpCAJNPzVVrtnlSKm6AJHc1zXD5K+H9R+6b9lDp7tjche4voL04e7mnOB1DK+f7b7Vj7mstgyxiS6I9BzlOo2n/6o2HXewtCHusXuWaxIKFhiyunWee5xwtZvncPxNR/77AQw+zf1uATEJcPyNEBXjXif1cn0neWuad/3DaFITk4gk4JYArRGRocBw4G1VbWbY70BEKD3xTvq+dSkL5r1J9lnXRLpExrQ979/rho72O8E9Djip7k0tP6f2Rpi3Bta+476Nj7yg4Wse2OfmCCRnuk7mQMfutHvdjV187kadtwpGeNcJjBzaMtet7ZI6xNUayotcDSIqFm4KkYojWHJfiIo7uHhYHd28SW9bveatjAnwg2WNX+9oiLiAFaaRTE3tg/gUOElEUoD3cKm8LwOuDEup2pnBY6ZQ/Zawd8Ni4JpIF8eYtmXT5/DF71yeoQXPuG19j4deo9y3+Opy10Qy9EzY/CU8e7Zr40/o0XiACHRQJ2eCVrvnk26EE3/gbpz9JsOXj7uJZ+nD3f4Eb2Gx6nJXU/D5oPdYN9IoMKrocHw+OPsRSBt26L7AnIbdq1z+pKjYpl3zaIy+2AXKMGhqgBBV3S8i1wN/UNWHRWRJWErUDkXFJbIzOpPoPatQVSQMw82MaZdUYfZ/u7H+35vrOm+X/gO+eBQO7HU35/x1rgYBMPtBdxMfc6mbpbx/b+0chPoCQ1yTM2HgKa49fsgZte38F/0J/nKWS4vRwwsQgRoEuCGwUBsgYpsYIAAmfDv09riurq/hwD7XL9IaTrg5bJduah+EiMgJuBrDm942f3iK1D5Vph1DVvUmVu0oPvzBxrRnqm7sfVOSxG36zM0TOOmHEN/N3ZTHXgGoaxZJH+bSSOTnwMbP3PEn/gAGeDOXA00nVRXwwqWwZV7ttQ8GiL5u9NHQM+t2AnftDVfPgsm3uhoLeM1I8e55L2/IaKCfoak1iMMJ1CICyfjasaYGiO8DdwGveov+DMQt5GM8KVnj6Se7+XzlpkgXxZgjs/FTeHgglAaljHnzRw2nlt69Cl67ERb/7fDX3jzHPY7/Vu22wPh9cE0/qYNdE9MH97mhphOvcYEDajuy81bDundh8V9rr7Nvs7vZNzZMtls/OOMBiI5zr0VqaxE9vBpE3+Nc81fXjMP/Pk0R6Ifo2rtlrhdBTQoQqvqJqp6vqr8SER+wR1VvDXPZ2pXEfmPxibJp1cLDH2xMW7L5S9ifDzu/rt224lW3klkogeagbYsOf+3CrZDYyw0BDXaMt6hkIECU7IRtC+CMB92xyX3dbOM8rwYRyHu0/uPa3Eb7NkHKgCOfQZzYwzV5JaS61936wfeX1Y58OlqBdBlJnSRAiMiLItLVG820HFgpIj9uwnnTRWSNiOSIyJ0h9t8uIitFZKmIfCgi/YP2XS0i67yfq4/kl4qIQHvmruUU7rfBXaYdCSS6C9yMS/fA/j21Q09XzoKlLwcd76Wq2N6EAFGwtXboZ7Dsa90Q037HuwAB7gY9+mL33Odzo3MO1iC8AFGUW5sqY99GN9T0SI29HI7/Xt1tXfu492wJgRpEZwkQwAhVLQIuAN4GsoCrGjtBRPzA48BZwAjgchGpPy5sMZCtqmOAfwEPe+d2B+4FJgHHAfd6I6jaruR+VEclMJQtfLLO5kOYdiQQIPZ4w04D39bLC92iNJ/9Gj5+qPb4wA06P8ftb0zh1roL4gQkZ8KFT7mO4YGnuHQW5/6ubm0gfXjtUNjdq9wsZHC5llS9GkQzAsSx18OUMDaABAJiJwoQ0SISjQsQs7z5D4dbJec4IEdVN6hqBfASMCP4AFWdraqB8VlzgcBf0pnA+6q6V1X3Ae8DLVT/CxOfD1+vkUyI2sTs1SGyRRrTVu3b6B7z6gUIcLWI/PV1Zzvv3QB+b6LW9sUNX7emxi2qkxyiBhEsPgVmPHZom336MFdjKCtyZRr0DTfpbMPHLule5X7XxNTWZJ0Ck/8Lsk6KdEmOWlMDxB+BTUAC8KnXFBRiZe86MoDgFbZzvW0NuR5XO2nyuSJyg4gsEJEFeXmR/9YuQ85gDGtZuXol1TW2ypxpB8qKXMoLpDZA5AUFiK3z3CQ0ra5dCnPvBjfDFxpvZirNc/MNDhcgGhKYu7BjiVsfoccIGDTVjXYK9IM0p4kp3GIT4YxfulnP7VxTO6kfVdUMVT1bnc3AqS1VCBH5FpANPHIk56nqU6qararZ6enpLVWc5hvt8qNMrfiUxVv2RbgwptPZkwO/HQXv3FWbNvpwArWHzGNdv8P+vS65XaqXTiJ4Ocv8dW7dhOIdboZw94F1O6prauD5i91Ka1A7DDVUH0RTBGYqf/Go93q4y3BaXggrvPWl22INogNpaid1soj8JvBtXUT+F1ebaMw2IPgvI9PbVv/apwF3A+eravmRnNvmdB9IVZ9sZvjn8KE1M5nWpApvfN99a5/7B3ju3ENXMgsl0P8w7Cz3mLfa1SAGTHGjiDZ+Wntsfk7t8d0HueU3N39R2w9RvANy3odZt7rMqoVeJ3eoPoim6J4Foy9x1wQXMLKmuudLXwaktkPYhEVTm5ieAYqBS72fIuAvhzlnPjBERLJEJAaYCdRZoFVExuOar85X1eA76rvAGSKS4nVOn+Fta/Oixl7KCN9mNqyYH+mimM5kyYtuktn0h+DsX8O2hZDr/Q3WVMO/rqt7sw8I3PADQzw3fuZmAfcY4W6+FSUuUHRJrRsgUgfBCd9zaa4//EXda1WUwGvfcyOYoPlNTABn/o/ro4iKc7WFhFS3aE55kQs8rZHKohNraoAYpKr3eh3OG1T1F8DAxk5Q1SrgFtyNfRXwsjfJ7n4R8QZB8wiQCPxTRJaIyCzv3L3AA7ggMx+439vW9nnpffvtnUPuvvDkRzEGcAnncr15Nwv/4rKYTrgaxs50N/XFXgr6rfNcs8/XLx16jb0bXc6j9OHunHlPuO3pw2u/nacOdk1Oe3JqRzB1H+hmIE+60eVXyl1Y21w15TbYMgeWvewyjQbWWGiOxHS4+C9w+v3g85I3DJzqHq15KeyaGiAOiMiJgRciMgU4cLiTVPUtVR2qqoNU9UFv2z2qGggEp6lqT1Ud5/2cH3TuM6o62Ps5XG2l7ejah4qUwUz2rbDRTCY8amrgowfhmTPh5atcR/O2RW7FMp8PYpNc9tLl/3Z9EavecOcFahTznoId3qS4vRvdzd7ngxEz3KS2Y/+fm58QCBBpQ2rTYexe5QJKbJLbd+pP3Szk1W+4a/mi4OQfu6Cwc5nrfzja3GSDToVJ3637GixAtIKmBogbgcdFZJOIbAIeA77b+CmdV/SgqRzvX8PsldsjXRQTCR89CLP+69DtlWWuE/hoLXoWPn0Yeo52ye/mPuFGGWWdXHvMuCvcymqL/warvTWM96yFncvh7R/DJw+7QLNnjQsQAN98Em6eC+f82jXdBJqGUoe4JqWSna4mEuivABco0o9xAWffRhdUYpNgnJfo+WialxrS7wQXpDKzW/7apo6mjmL6WlXHAmOAMao6HvhGWEvWjsnAk4mnjNKNX1FcZrOqO50V/4ZFf4U179Td/t7P4I8nuz6Bw5n7hEuTXd+BffDhA9D/RJeITvzw+W9d2uy+x9Ue13+Ka4p5567aNZABPnrAPW74BLbOdZ3agSab+oJrEIFFchLS4fRf1D2u9xgXIPZuqJ24ln2de2xuB3VjouPh9pWuOc2E1RHNLVfVIm9GNUCItfgMAANOQhGO1eXMXhP5+RmmFVWV13bWvn0HVAa1xK7/yM0szl3Q+DVK8+Hdn8JXTx2675OH3TKdZ/3KpcHuP9mtd9D3uLr5jnw+uOjP3mxegVPvdo9r33HNQBXF8N7P3fOhZ4QuR//JbtLXgJOgzwTXUX3+712ncbDeY90Q2V0ra2sjaUPgm39s/kpqh+OPPvqmK3NYR5N8xP51GtKlO/QazakxK3ln+Y5Il8a0pvwct9jN2Cvc5K7VXnb8kt21OYwCTT4NWfeeu8bejYfuW/UGDD+nNlV1YM3jrFMOPTYhDa56FS7+s2siCmRIPfY74It2yfEGnHToDT8gqZerpST1hOQM+PH60MGk1xj3WFNZd+La2Jm1NQ/TLh1NgLCpwo2QIWcwXlexdPU6yiqb0KRgOoZAcrlJN7hU1Nu8UUZb5rrHxF7uJt/YHIU1b7nHfZvqHnegwM0t6DOhdtvIC91NftSFoa+VPrS2eSnQZj/mMlc7gNoA0xQNfWPvNYqD3xebkxvJtFmNBggRKRaRohA/xUArLZfUTo2+GB81fKNmDh+vsdFMnUbeGrcWcvpw6DOutjlpy1w3lv+k211nbiCQ1NS4x+oqeP37MPdJ1xQVFefG+h/YBzuWujQXu1e6Y3uOqn2/xHS45o26C9s3ZMI1cNwNbnjqiPPdeww/5+h/59ik2vdvi6kvTLM1GiBUNUlVu4b4SVLVpi5X2jn1OAbtMYKLY+byj/lbD3+8aX2q8PU/mrYyWlPlrXbDL6PjIWOi67ytqoAtX7qZxyO8fJWr3nBpKx4dC2/cDp//xs1leOcnbqLZ2JnuuL0b4cXL3Kioncvdtl6jQr71YfU91q2lLAITr3NrILTUqmeBZiYbetqhtFACdBOKjL6YMbqGdWtX2qS5tihvNbx6g1sjuTkqSt3Nv84119QmmcuY6JLVbZ3rAkW/490NOfNYN29g3XtuhNGCP7u1mEd+0y2YM/g0t6oawKZPoXi7S2mxZY7rL2iJNNI+X931mY/Wsde7+Q/1FwYy7ZoFiHAadTEqfr4f9QovWy2i7Tm4UI6XxbQkzzX1NIUq/Ol0eP222m3VlW6mcaAzONDm/5+b3TyF4We718PPdRlK5z7hho2eerdr9jnnNzD5FvjWK7VBZuk/3WNNlVu4p+eotjl6Z8CJ8I2fRboUpoVZgAinlP7Iid/nYv+nbP5qFlXVNZEukQm2b5N73LPWDS393Wh4dDx8+ohbbrOqvOFzty2C3SvcKKXAOgnbFrmRPIGbe3JfN6GrYItbECdjots+/Fz3uHWe6yQ+5Q644WM3+i0gOt7VFHavcJ3d8SkuyPRsZvOSMc1gASLcTr6DkqSB/LjyST5ZtiHSpWlfampqb77hEBwgti1w8wmi4+CjX8KLl8CXjzV87jLvm315obvRf/W0y6Aal+y+TYP7pt//BDd/4PT7a89NGwxpXi1jxAUNv0dgRFCf8TD4dPe8uf0PxjSDBYhwi44j7uIn6CP58MF9kS5N+zL7QfjTtOadW1XhJm41JrAATtE2l+lUfPD/ZsMdG93NOXitg2A11S7lxMCpbj7B3CfgnTvdcNOb59edPXzu7+C7n9atHQCM/xakDXUznhsSGBGUme1GHUFtLcSYVmABohVE9T+eJb0vY1rJ6+xe9kGki9N+bF/kRu40JTVFsPUfwePHwRMn1CalC2XfJtd8A65GkD7crQbWpbv71r5jaejzvv47lO6Gide6+QSr33BDRi94wk0qC9ale+h0E1NuhVvmg7+RwYCBGkTf41yz1K1LoMcxDR9vTAuzANFKen7zQbZoOv43bmv6al+dXcFW1+5evLPp51RXwb+ud+cBbP4y9HE1NW6mcyDBXcmuuhPQeo9xk9IOBK0MWFnmmp/+czNkHueS1g090+078QeHBoej1f8ESOoD/Sa75iqbY2BamQWIVpLRI413sn5KankuJe/ef/gTOjvV2iUrA4+NqTzgahpb58KBvW5N4KQ+tSmu6yvZBVVlLnW0z/sWnzG+dn+v0e5x5zJY+Bz8+wZ47FjXgT32crj6dZfxdPy3YNq9cMItzf9dGzLgRPjhKrdIjjERYAGiFZ09Yyb/rDmV+IVP1S68YkIr3eM6jQGKmhAg/ngK/OcWWP0W+GNg0DTXdr+tgcR4gQ7q1MG1CeaC2/cDE78WvwCv3+qyn6b0h2/Pcmmxo+Pc/rhkNzs68NqYDiSsAUJEpovIGhHJEZE7Q+w/WUQWiUiViFxcb1+1t8rcwZXm2rvMlC5sGvtDyjSKkrfvi3Rx2rbAesZw+BrEgQK3rsHXL8KSF1ziuthEFyD2bXLBBlytZONn8O7dbtIZuJm/aUNdUOkxsvaaiT1c3qSlL0FMItw8z6W0GBgiKZ4xHVTYAoSI+IHHgbOAEcDlIjKi3mFbgGuAF0Nc4kColebau2vOOI6/6jkk5sxyN6vOYMtc+L+xR7ZYTkHQxMLCbY0fG5johrhU2IEJaZnHusfcBa756e+Xu6GoXz7m1lRA3FyFybe6dZyjYupet7dXi5jwbYjv1vSyG9NBhLMGcRyQ461hXQG8BMwIPkBVN6nqUqDTzCBLT4qlYtLNbNNU9Lnz4K07wjvWvy3YOs99k9++uOnnFHoBIqmPq0GUF7vlLoMt+5drqgskvpv2c4jrBsO8BHS9x7kFdXI+cEOM174N3/g5XPGy63dI6u2ahvpNgokhFp/JyHbHTbrxyH5fYzqIcCbcywCC80vkApOO4Pw4EVkAVAEPqeprLVi2iLrm1LGcPf/X3J/0GtO++qObaXvJsx23HbvIW3p11woY3MR5DQVbXdNOz5GuD+L9e2HBM3Dh0zDmEpcm45XrYfQlbrZyVDxM+QGceHttKoqYLi6j6vyn3esJV8PJP3LPZ77oahuNmXyLS66X0v9If2NjOoS2nJG1v6puE5GBwEciskxV6/TsisgNwA0A/fr1i0QZmyW5SzRXnDKG69+N5dOpE+k39x5460cwo5GZu+1ZcIBoqsKtrvknOdPNhyjNd9tfvcH1LwRGJ2342KWfSB/qEtDVd9kLkPuVmzg3IqgC29AqasFiEqDH8KaX2ZgOJpxNTNuA4BXLM71tTaKq27zHDcDHwPgQxzylqtmqmp2enn50pW1l104ZQFpiDD/efBw6+Va3uPzmOZEuVngUe6vq7Vre+HGBIACuBtGtr1vJbH++q0Wc8Us3/PS1m2Dx8xCd4NZU3vR5bf6j+rr2doFhzCWH9jEYYxoVzgAxHxgiIlkiEgPMBJo0GklEUkQk1nueBkwBDpM3oX3pEhPFbdOGMG/jXl5JutJ9W/775fDfGfDOTyNdvJYVqEHkralNj52/Ht65C56aCiteg89/C48MhLfvdJPYCre4z6Rr0CzkY86DC//kJqyV7ILTf+G211TWZlA1xrSYsDUxqWqViNwCvAv4gWdUdYWI3A8sUNVZInIs8CqQApwnIr9Q1ZHAMcAfRaQGF8QeUtUOFSAArpzUn7eW7eTetzdx0oX/S89Fv3VDMef+AUZe4FIstGWqh089XePNhO7W381c3vG1Gzq64C8u91FyBvzT6yBOGwrznnCT08oKvRqEFyBSB9f2Bcx4zOVCmniNS5K3Z03DNQhjTLOFdR6Eqr6lqkNVdZCqPuhtu0dVZ3nP56tqpqomqGqqFxxQ1TmqOlpVx3qPfw5nOSPF5xN+felYRIQfLeyGXvu2Wwuga4ZbQaxgy+Ev0lzPnQ+vfKfxlNaN2b8Xfj/R3egbU5rn0l4M8bKRvnyV62zOvhZ+sMIltzvxBzDl+/C9uXD6Ay7PUVS8G6aanOHOGxTUuT36Yrj87+CPdjOhwQKEMWHQljupO4WMbvH88Iyh/OL1lby1bCfnjOkN5/8f/P0KdwP+xs9dYreWtH8vbPzEe54Plz3vOmSPxNKXYe96ePsONwM5dVDoawSal7JOdikrine4voTJ/1V7zGn31T6fcqv7CdROampc8Jjw7dDlOP4ml047MBvaGNNiLNVGG3DV8f0Z0bsrD7yxktLyKrfk5K2LYcgZ8P7P3UL2LWnnMvc49go3CuhvF7rZyEdiyfPuW3uXVPjjyfDffeDLx92+zXNqU1kEAkS3fi6x3fhvNS1vUaDpyudzfQ2pg0IflzLALbjTFldZM6adswDRBkT5fTxwwSh2FpXx6Ifr3MbkDLjkOZfm+Z2fwKe/dt+qG9LYvvp2emmsz3gALn4Gti106xk01Y6lLshkX++axKbc6kYXzXnMbX/2HHh8knsdGMHUNQNmvgAzHrebuTHthAWINmJi/xQuzc7kz59vZO2uYrfRH+Vu4KMvgY8egDdvDx0Idq+G/+kLa95xo4Te/OGhs46D7VjqZignpMHIb7o2/bXvhF53oaYGFv0Vfp8Nfz7Tvf+iv7rcRaMvdhPZTr8fTv0ZFG+HFy5xE9yyTob37nZLcvqioUtay3xQxphWYwGiDbnzrGNIjIviJ68spbrGCwRRsW728JTbXOfuxw/VPUnVffuvKIZ5T8KqWTD/T27YaEN2Lq3NMwSuA/jAPtix5NBjv/it6zCv3O9Saa+aBUtehFEX1V0lbcgZrh+geIerUVz0J4juAhtmQ1Kv0JPYjDFtmv2vbUO6J8Twi/NHsnhLAX/8NGjSuAic9gsY9y345CF46UpXC1CFpf9wN+G0oe4xEEBWve7yF+1cDuUlrnbw8UOw9j23BnOv4ADhjQTK+ahugVa/5RbIGXUR/NdCVwt49UaoLIUTbq57rM8HU+9yHdbHf8+lwR51oduX1LtlPyhjTKuwUUxtzPlj+/Duip389v21nDg4jTGZ3dwOETjvd9B9AHz+f26Zy7hkN1+g5yg3EunR8ZC/DkZeCCv+7dZHWPma6x/IyIaFfwEE0Lo1iIQ06D0W1n8Ip/wY9uTAWz90Hdg9RsL5v4foeDj2Oy5ADZxau6BOsDGXup+A7OvcjOeufcLzYRljwspqEG2MiPDgBaPpkRTHTc8vomB/Re1OfzSc/GO47Ws497cwdLrr9L3+Pbcc5ZDTXfv/eb9z6xmvfM3d4Pesc8Fh/FVuwhm4gBBs0DTY+hW8cbsblbR9iZuTcP17tcNXj/2OW6t5ahNneveZ4EYtDT/3KD8VY0wkiB7J6Jc2LDs7WxcsaGD1sHZoydYCLnlyDqcO68Efr5qINGXkT/EuNzGt1yj3zX3xC65msXe9qw2c9MPa3EWjL6577q4V8NIVLh9SZrYLPIFJasaYDktEFqpqdsh9FiDarqc/3cCDb63iN5eO5cIJmYc/wRhjjlBjAcKamNqw607MIrt/CvfOWsHOwrJIF8cY08lYgGjD/D7h15eMpapaufPfS+kotT1jTPtgAaKNG5CWwJ1nDefjNXn8/authz/BGGNaiAWIduCq4/tz0pA07vnPct5bsTPSxTHGdBIWINoBn0/4w5UTGJWRzM0vLmLuhvzDn2SMMUfJAkQ7kRQXzXPXHUff7l24+YVFbCs4EOkiGWM6OAsQ7UhyfDRPXZVNeVUNN/x1gUsNbowxYRLWACEi00VkjYjkiMgh+aRF5GQRWSQiVSJycb19V4vIOu/n6nCWsz0Z3COR318+nlU7irjtpcW1Sf2MMaaFhS1AiIgfeBw4CxgBXC4iI+odtgW4Bnix3rndgXuBScBxwL0ikhKusrY3pw7vwX3nj+SDVbv55ZsdbqluY0wbEc5kfccBOaq6AUBEXgJmAAfvaKq6ydtXU+/cM4H3VXWvt/99YDrw9zCWt1359gkD2LRnP898sZG0xFi+e/JAovzWYmiMaTnhvKNkAMED93O9beE+t9O4+5xjmD6yF4+8u4bp//cZObuLI10kY0wH0q6/corIDSKyQEQW5OXlRbo4rc7vE5741gSeuHICBfsr+O7fFlrHtTGmxYQzQGwD+ga9zvS2tdi5qvqUqmaranZ6enqzC9qeiQhnje7No5ePZ+OeUu7411LruDbGtIhwBoj5wBARyRKRGGAmMKuJ574LnCEiKV7n9BneNtOAyYPSuPOs4by5bAe3vbSYssoQ60sbY8wRCFsntapWicgtuBu7H3hGVVeIyP3AAlWdJSLHAq8CKcB5IvILVR2pqntF5AFckAG4P9BhbRp2w8mDAPjvt1bz8Zo8LpqQwd3njCAmql23JBpjIsTWg+iA5m3I5+UFubyyKJepw9J58lsTiYv2R7pYxpg2yNaD6GQmDUzlfy8dy/9cOJpP1uZxy4s2oc4Yc+QsQHRglx/Xj/vOG8kHq3Zx76zllFdZv4QxpunCOVHOtAFXTx5A7r79PP3ZRmavzuPWaYO5aEKmTaozxhyW3SU6gZ+efQzPXz+JtKRYfvLKMs747ae8s3ynrVBnjGmU1SA6ARHhxCFpTBmcyvsrd/Hwu2u48fmFpCXGML5fCr84fyR9usVHupjGmDbGRjF1QlXVNby2ZDvzNuTz9vKddInx8/iVEzh2QPdIF80Y08psFJOpI8rv4+KJmTxyyVheuWky0X4flzz5Jdc/O589JeWRLp4xpo2wANHJDeuVxHs/OJk7pg/ji/V7uPiJOWzJ3x/pYhlj2gALEIaE2Ci+N3UwL3znePbtr2TG45/z6drOl/zQGFOXBQhz0MT+Kbx28xR6JMXx7We+4qSHP+Lhd1ZTWV1/uQ5jTGdgAcLUkZWWwKs3T+aec0cwrGcSf/h4PVf+aR7LtxVGumjGmFZmo5hMo15dnMtP/72cA5XVTMrqzs/PHcGojORIF8sY00IaG8VkAcIcVuH+Sv61KJc/zM5h7/4KzhrVi5tOGczoTAsUxrR3FiBMiygqq+SPn6znr3M2U1xexYmD07j8uH58Y3gP4mMsW6wx7ZEFCNOiisoqeXHeFv7yxUZ2FZWTmhDDwxePYdoxPSNdNGPMEbIAYcKiukaZtyGfB95cxaodRQxMS2BYryROHprOOWN60zUuOtJFNMYchgUIE1blVdU8+8Umlmwt4OutBWwvLKNn11geunAMpw7vEeniGWMa0ViACGuyPhGZDvwfbsnRP6nqQ/X2xwJ/BSYC+cBlqrpJRAYAq4A13qFzVfXGcJbVNF9slJ/vnuKWO1VVFm3Zx52vLOPaZ+eT3T+FS7P7MqZvMpVVSr/ULiTHW83CmPYgbDUIEfEDa4HTgVzc+tKXq+rKoGO+B4xR1RtFZCbwTVW9zAsQb6jqqKa+n9Ug2payympe+moLT3+2kW0FBw5uT4yN4prJA7jlG4NtGVRj2oBI1SCOA3JUdYNXiJeAGcDKoGNmAPd5z/8FPCYiEsYymVYSF+3nmilZXD15AOt2l7B6ZzExfuH1pTt4bHYOn+Xs4Z5zR5CWGEP/1IRIF9cYE0I4A0QGsDXodS4wqaFjVLVKRAqBVG9flogsBoqAn6nqZ/XfQERuAG4A6NevX8uW3rQIEWFozySG9kwCYPqo3pw3Zie3v7yEi56YA8CUwalcNCGTpLhoThiUSmKs+7P854KtrNlZzE/PPgafz743GNPa2uqCQTuAfqqaLyITgddEZKSqFgUfpKpPAU+Ba2KKQDlNM0wf1YvRmaewansR6/NKeOrTDdz+8tcAdInxc+6Y3vRPTeCRd10XVPfEGL43dXAki2xMpxTOALEN6Bv0OtPbFuqYXBGJApKBfHUdI+UAqrpQRNYDQwHrZOggMrrFk9EtntPoydWTB7Ct4AB5xeW8tngbs77ezv6KaqYMTiU5Ppr/fW8t3eJjuDQ7k+KyKrp1icZaIo0Jv3B2UkfhOqmn4QLBfOAKVV0RdMzNwOigTuoLVfVSEUkH9qpqtYgMBD7zjtvb0PtZJ3XHUVJexZycPZw0JJ2qmhque3Y+8zftI9ovVFYrk7K68+A3R9MrOY6EGL8FC2OOQsTmQYjI2cDvcMNcn1HVB0XkfmCBqs4SkTjgb8B4YC8wU1U3iMhFwP1AJVAD3Kuqrzf2XhYgOi5V5e3lO1mwaR+JcVE88/lGSsqrABiYlsBZo3txXFYq4/t1s8l5xhwhmyhnOpRtBQf4YOUu9ldU83lOHl+uz6dGQQSyUhNISYhhZJ+unDe2DxP7pVgHtzGNsABhOrSS8iq+3lrAws37WLm9iIIDFSzZWkBZZQ19kuM4YVAafbvHc8rQdMb17WZNUsYEsQBhOp3S8ireX7mL17/ezsodRewqKqMm6E+9W5doRmckc+u0IfRN6UKXWP/B5qmKqhr+s2QbkwenkdEtPkK/gTGtwwKE6fQKD1Ty4apdbM7fT40qe0sreHfFTvaUVAAQ4/dx5qheDEjtwvsrd7F6ZzH9U7vw75smk5oYG+HSGxM+FiCMCaG0vIo3l+6gqkZZu6uYVxdvo6iskj7J8Vw9uT+/eX8tmSldOG9MH+KifSgwZVAa/dO6EOP3WaoQ0yFYgDCmGWav2c2v3l7Nml3F1P9v4vcJ4/t2Izk+mtKKKq6Y1J9zR/e2DnHT7liAMOYoBIbUllVW8/m6PewpKWdvaQVfrM+noqqGsspqNu4pJSbKR2pCDCcPSaeqRvlsXR7fGN6Dy47tS0ZKPOmJsdZBbtqciKX7NqYjCOSGSoyN4oLxGYfsr65R3l6+g2XbCsndd4A3lm5HRJiU1Z1/L97GS/NdSrJ+3bswrm83ovzCzsIyyiqrGdknmaE9E8lKS2RAWhf6JMdTo8rXuQX4RBjcI5Ekm9thIsQChDFHye8Tzh3Th3PH9AHcKChFiY3ys7u4jMVbCsjdd4A5OXv4OreAqmolPSmWmCgfry7edrCGAq6zPCbKV2dbn+Q4Jg7ozrThPejRNZbeyfH0694Fv08or6pm274D9O3ehWi/r9V/d9OxWROTMRGkquwqKmfjnlI27illc34pJeVVTBmcRpRPWLe7hDU7i/kiZw/5pRUHz4v2CwmxUZSUVVFVo3SJ8TN5UBrnj+tD9y4xzFm/h9cWb+P4gal879RBxMdEsXlPKUVlVUwZ7DLmVtcoUU0MKqrKiu1FHNO7K37rZ+lQrA/CmHau2htptW9/Bbl7D7Axv5T95VUkxkXRv3sCy7cX8u6KnewqKgfcrPITBqayYPM+Kqpq6lwr2i/4RKiuUUb06UpGt3iS412q9T7d4qmsrmFYzyRKy6tZtbOI7P4p/P6jHJ6ds4kLx2fw60vGHtIZH7iPWB9L+2MBwphOoLpGWZpbQGW10qdbHJkpXdi6dz9frs+nWpXMlHhi/D5mr8mjRhURWLq1kD0l5ewuLqfwQGXI64qAKkzo141FWwoY2jOR0vJq+nXvwrBeScRF+3lv5U52FZYxY3wGsVE+SsurOHVYD+Jj/BQeqCQ9KZb8kgr2lJRz9ujelJZXsWDTPk4b0ZPuCTGt/EmZYBYgjDGNCgSX4rIqRGDVjiJio/wM7ZnEx2t3k54Yy/UnZvHkJxuYvWY3vZPj2JDnmsVKyqvI7p9CZko8by3fiV+EaL9QVFYV8r38Pld7AUiI8TNpYCpdYvxcdXx/8ksrePTDdfTpFs+ojGR6do2lqlrx+4T+qV2orK4hyufj2AHdiY+xeSgtwQKEMSZsamr0YJNTWWU1MX4fNaos3loAQHJ8NHnF5XTrEk1ctJ9XFuaSEBvFpKzuvDBvC2t3FbOrqOzgrPahPROprlE27Ck9ZP5JQEyUj15d40iKiyIhJoo8b+hxVloCw3omkZEST8H+SuKiffTsGkd+STlrdhWzPq+UtMQYJvZP4ZvjM5i7YS+5+w6QkRLPqD5diYv2s2SrqyWN7+sSPe6vqKKyWkmOj6akvIq1u4oZ0btrh5koaQHCGNOmHaio5tk5m1CU/3fSQKL9Piqra9hTUk6M30dFdQ2b8/cTG+Wj8EAlX+TsIa+4nOKyKorLq0hLjCGlSwwb8kpZu6uY/NIKusT4qaiqoarGNaf1796FIT2TyCsuZ2luwcHcXFE+oarm0PtgTJQPvwgHKqsRgfF9u7FuVwnF5VXERfvI6BZPUlw0w3slkRQXRUl5FSXl1ZSUVXKgsprYKD8JsX7io6PcY4yfpNgojundlbTEWNbuKqZn1zj6dItjb2kle0rKqayuYWjPJAalJxIT5aO6Rlm1o4iV24tAICstgXF9uxHt96GqFJVVkVdcTnWNMqxXUrM+ewsQxphOpayymrhoP1XVNezbX0lKl+g6I7a27t3Peyt3cdyA7ozs05UdRWUs3VpAaUU14/p2Y9m2AndTBlISYjhQUc3sNbvJSkvktGN6sGRrAbuLXK1l9c4iyiprSIiNIikuisTYKOKj/ZRXVbO/IvBTxf6KasrrDRhoSLRf6JEUR15J+SGDDGKifHSJ8bO/ovrgvvH9uvHq96Y067OyAGGMMW1AWWU1i7cUUHiggiE9k9hVVEZecTmpCbGkJsYgAmt3lbBqRxE7C8tIT4plRO+ujO/XDZ8IK7YXsmhLAWWV1cRH+0lPiiU9KZbMlC5M7J/SrDJZgDDGGBNSYwEirFMvRWS6iKwRkRwRuTPE/lgR+Ye3f56IDAjad5e3fY2InBnOchpjjDlU2AKEiPiBx4GzgBHA5SIyot5h1wP7VHUw8FvgV965I4CZwEhgOvAH73rGGGNaSThrEMcBOaq6QVUrgJeAGfWOmQE85z3/FzBN3FTMGcBLqlquqhuBHO96xhhjWkk4A0QGsDXoda63LeQxqloFFAKpTTwXEblBRBaIyIK8vLwWLLoxxph2nf5RVZ9S1WxVzU5PT490cYwxpkMJZ4DYBvQNep3pbQt5jIhEAclAfhPPNcYYE0bhDBDzgSEikiUiMbhO51n1jpkFXO09vxj4SN2421nATG+UUxYwBPgqjGU1xhhTT9gWDFLVKhG5BXgX8APPqOoKEbkfWKCqs4A/A38TkRxgLy6I4B33MrASqAJuVtXqcJXVGGPMoTrMRDkRyQM2H8Ul0oA9LVSclmTlOjJttVzQdstm5ToybbVc0Lyy9VfVkJ24HSZAHC0RWdDQbMJIsnIdmbZaLmi7ZbNyHZm2Wi5o+bK161FMxhhjwscChDHGmJAsQNR6KtIFaICV68i01XJB2y2blevItNVyQQuXzfogjDHGhGQ1CGOMMSFZgDDGGBNSpw8Qh1uzohXL0VdEZovIShFZISK3edvvE5FtIrLE+zk7QuXbJCLLvDIs8LZ1F5H3RWSd99i8Ja2aX6ZhQZ/LEhEpEpHvR+IzE5FnRGS3iCwP2hby8xHnUe9vbqmITGjlcj0iIqu9935VRLp52weIyIGgz+3JcJWrkbI1+G/XWmvENFCufwSVaZOILPG2t9pn1sg9Inx/Z6raaX9wM7zXAwOBGOBrYESEytIbmOA9TwLW4tbRuA/4URv4rDYBafW2PQzc6T2/E/hVhP8tdwL9I/GZAScDE4Dlh/t8gLOBtwEBjgfmtXK5zgCivOe/CirXgODjIvSZhfy38/4vfA3EAlne/1t/a5Wr3v7/Be5p7c+skXtE2P7OOnsNoilrVrQKVd2hqou858XAKkKkOG9jgtfzeA64IHJFYRqwXlWPZjZ9s6nqp7h0McEa+nxmAH9VZy7QTUR6t1a5VPU9den1AebikmG2ugY+s4a02hoxjZVLRAS4FPh7ON67MY3cI8L2d9bZA0ST1p1obeKWXh0PzPM23eJVEZ9p7WacIAq8JyILReQGb1tPVd3hPd8J9IxM0QCXxyv4P21b+Mwa+nza0t/ddbhvmQFZIrJYRD4RkZMiVKZQ/3Zt5TM7CdilquuCtrX6Z1bvHhG2v7POHiDaHBFJBF4Bvq+qRcATwCBgHLADV72NhBNVdQJuCdmbReTk4J3q6rQRGTMtLlvw+cA/vU1t5TM7KJKfT0NE5G5cMswXvE07gH6qOh64HXhRRLq2crHa3L9dPZdT94tIq39mIe4RB7X031lnDxBtat0JEYnG/cO/oKr/BlDVXaparao1wNNEaOlVVd3mPe4GXvXKsStQZfUed0eibLigtUhVd3llbBOfGQ1/PhH/uxORa4BzgSu9mwpe802+93whrp1/aGuWq5F/u7bwmUUBFwL/CGxr7c8s1D2CMP6ddfYA0ZQ1K1qF17b5Z2CVqv4maHtwm+E3geX1z22FsiWISFLgOa6Tczl11/O4GvhPa5fNU+dbXVv4zDwNfT6zgG97o0yOBwqDmgjCTkSmA3cA56vq/qDt6SLi954PxK3DsqG1yuW9b0P/dm1hjZjTgNWqmhvY0JqfWUP3CML5d9Yave9t+QfX078WF/nvjmA5TsRVDZcCS7yfs4G/Acu87bOA3hEo20DcCJKvgRWBzwm3fviHwDrgA6B7BMqWgFuFMDloW6t/ZrgAtQOoxLX1Xt/Q54MbVfK49ze3DMhu5XLl4NqmA39nT3rHXuT9+y4BFgHnReAza/DfDrjb+8zWAGe1Zrm87c8CN9Y7ttU+s0buEWH7O7NUG8YYY0Lq7E1MxhhjGmABwhhjTEgWIIwxxoRkAcIYY0xIFiCMMcaEZAHCmMMQkWqpmzW2xbL+etlAIzVPw5hGRUW6AMa0AwdUdVykC2FMa7MahDHN5K0L8LC4dTK+EpHB3vYBIvKRl3DuQxHp523vKW79ha+9n8nepfwi8rSX4/89EYn3jr/Vy/2/VEReitCvaToxCxDGHF58vSamy4L2FarqaOAx4Hfett8Dz6nqGFwivEe97Y8Cn6jqWNx6Ayu87UOAx1V1JFCAm50LLrf/eO86N4bnVzOmYTaT2pjDEJESVU0MsX0T8A1V3eAlUdupqqkisgeXIqLS275DVdNEJA/IVNXyoGsMAN5X1SHe658A0ar6SxF5BygBXgNeU9WSMP+qxtRhNQhjjo428PxIlAc9r6a2b/AcXC6dCcB8L5uoMa3GAoQxR+eyoMcvvedzcJmBAa4EPvOefwjcBCAifhFJbuiiIuID+qrqbOAnQDJwSC3GmHCybyTGHF68eIvUe95R1cBQ1xQRWYqrBVzubfsv4C8i8mMgD7jW234b8JSIXI+rKdyEyxoaih943gsiAjyqqgUt9PsY0yTWB2FMM3l9ENmquifSZTEmHKyJyRhjTEhWgzDGGBOS1SCMMcaEZAHCGGNMSBYgjDHGhGQBwhhjTEgWIIwxxoT0/wEWuAYfHKz8igAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Conv 128*3\n",
    "mod = final_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "\n",
    "hist = train_model(mod, train_, test_, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['train_loss'], label='Train')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['eval_loss'], label='Eval')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e24e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa6aa428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "epoch 10\n",
      "train loss: 0.20114921033382416\t\teval loss: 0.19632606208324432\n",
      "epoch 20\n",
      "train loss: 0.12151434272527695\t\teval loss: 0.1345425248146057\n",
      "epoch 30\n",
      "train loss: 0.08020966500043869\t\teval loss: 0.12221838533878326\n",
      "epoch 40\n",
      "train loss: 0.05537307634949684\t\teval loss: 0.1343243569135666\n",
      "epoch 50\n",
      "train loss: 0.039809733629226685\t\teval loss: 0.15481601655483246\n",
      "\n",
      "epoch 50:\n",
      "\n",
      "f1_score for 50 classes: 0.7933256299794784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.80      0.85       215\n",
      "           1       0.85      0.73      0.79       208\n",
      "           2       0.88      0.87      0.87       255\n",
      "           3       0.87      0.89      0.88       200\n",
      "           4       0.84      0.90      0.87       171\n",
      "           5       0.86      0.94      0.90       360\n",
      "           6       0.90      0.86      0.88       248\n",
      "           7       0.78      0.85      0.81       285\n",
      "           8       0.77      0.52      0.62       128\n",
      "           9       0.71      0.57      0.63       249\n",
      "          10       0.73      0.80      0.76       146\n",
      "          11       0.87      0.85      0.86       185\n",
      "          12       0.71      0.75      0.73       418\n",
      "          13       0.94      0.86      0.90       319\n",
      "          14       0.94      0.94      0.94       325\n",
      "          15       0.93      0.91      0.92       127\n",
      "          16       0.82      0.83      0.83       302\n",
      "          17       0.74      0.80      0.77       122\n",
      "          18       0.94      0.90      0.92       218\n",
      "          19       0.73      0.65      0.69       106\n",
      "          20       0.73      0.77      0.75       366\n",
      "          21       0.70      0.59      0.64       176\n",
      "          22       0.57      0.65      0.61       194\n",
      "          23       0.76      0.74      0.75       331\n",
      "          24       0.79      0.69      0.74       250\n",
      "          25       0.76      0.80      0.78       239\n",
      "          26       0.74      0.83      0.78       132\n",
      "          27       0.86      0.91      0.88       267\n",
      "          28       0.92      0.88      0.90       178\n",
      "          29       0.82      0.77      0.79       438\n",
      "          30       0.87      0.70      0.78       186\n",
      "          31       0.73      0.70      0.72       233\n",
      "          32       0.94      0.80      0.87       148\n",
      "          33       0.86      0.73      0.79       267\n",
      "          34       0.83      0.77      0.80       261\n",
      "          35       0.58      0.39      0.47       203\n",
      "          36       0.87      0.88      0.87       150\n",
      "          37       0.77      0.69      0.73       171\n",
      "          38       0.78      0.80      0.79       410\n",
      "          39       0.81      0.77      0.79       273\n",
      "          40       0.66      0.70      0.68       151\n",
      "          41       0.69      0.77      0.72       125\n",
      "          42       0.93      0.83      0.88       255\n",
      "          43       0.94      0.89      0.91       273\n",
      "          44       0.74      0.83      0.78       271\n",
      "          45       0.85      0.72      0.78       171\n",
      "          46       0.78      0.82      0.80       414\n",
      "          47       0.80      0.90      0.85       177\n",
      "          48       0.86      0.79      0.83       207\n",
      "          49       0.76      0.81      0.78       150\n",
      "\n",
      "   micro avg       0.81      0.79      0.80     11654\n",
      "   macro avg       0.81      0.78      0.79     11654\n",
      "weighted avg       0.81      0.79      0.80     11654\n",
      " samples avg       0.78      0.75      0.74     11654\n",
      "\n",
      "\n",
      "Spent time: 482.9733645915985 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60\n",
      "train loss: 0.02913506142795086\t\teval loss: 0.1693870723247528\n",
      "epoch 70\n",
      "train loss: 0.02375415712594986\t\teval loss: 0.1747894138097763\n",
      "epoch 80\n",
      "train loss: 0.01878267340362072\t\teval loss: 0.18076148629188538\n",
      "epoch 90\n",
      "train loss: 0.016096550971269608\t\teval loss: 0.20292353630065918\n",
      "epoch 100\n",
      "train loss: 0.013550454750657082\t\teval loss: 0.21781770884990692\n",
      "\n",
      "epoch 100:\n",
      "\n",
      "f1_score for 50 classes: 0.8049960224146996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86       215\n",
      "           1       0.86      0.80      0.83       208\n",
      "           2       0.87      0.88      0.88       255\n",
      "           3       0.90      0.86      0.88       200\n",
      "           4       0.86      0.90      0.88       171\n",
      "           5       0.87      0.92      0.89       360\n",
      "           6       0.94      0.83      0.88       248\n",
      "           7       0.80      0.85      0.82       285\n",
      "           8       0.68      0.58      0.62       128\n",
      "           9       0.75      0.59      0.66       249\n",
      "          10       0.76      0.77      0.77       146\n",
      "          11       0.87      0.84      0.86       185\n",
      "          12       0.76      0.75      0.76       418\n",
      "          13       0.93      0.86      0.89       319\n",
      "          14       0.95      0.92      0.94       325\n",
      "          15       0.94      0.88      0.91       127\n",
      "          16       0.88      0.82      0.85       302\n",
      "          17       0.75      0.77      0.76       122\n",
      "          18       0.96      0.89      0.92       218\n",
      "          19       0.80      0.72      0.76       106\n",
      "          20       0.82      0.69      0.75       366\n",
      "          21       0.73      0.58      0.65       176\n",
      "          22       0.58      0.66      0.62       194\n",
      "          23       0.80      0.76      0.78       331\n",
      "          24       0.81      0.70      0.75       250\n",
      "          25       0.80      0.73      0.76       239\n",
      "          26       0.80      0.80      0.80       132\n",
      "          27       0.85      0.90      0.87       267\n",
      "          28       0.92      0.87      0.90       178\n",
      "          29       0.83      0.76      0.79       438\n",
      "          30       0.90      0.73      0.81       186\n",
      "          31       0.74      0.76      0.75       233\n",
      "          32       0.96      0.80      0.87       148\n",
      "          33       0.94      0.77      0.85       267\n",
      "          34       0.83      0.79      0.81       261\n",
      "          35       0.54      0.47      0.50       203\n",
      "          36       0.94      0.87      0.91       150\n",
      "          37       0.79      0.70      0.74       171\n",
      "          38       0.82      0.78      0.80       410\n",
      "          39       0.85      0.74      0.79       273\n",
      "          40       0.77      0.72      0.74       151\n",
      "          41       0.77      0.71      0.74       125\n",
      "          42       0.93      0.83      0.88       255\n",
      "          43       0.94      0.90      0.92       273\n",
      "          44       0.82      0.77      0.79       271\n",
      "          45       0.88      0.65      0.75       171\n",
      "          46       0.82      0.78      0.80       414\n",
      "          47       0.81      0.92      0.86       177\n",
      "          48       0.88      0.82      0.85       207\n",
      "          49       0.82      0.77      0.80       150\n",
      "\n",
      "   micro avg       0.84      0.78      0.81     11654\n",
      "   macro avg       0.84      0.78      0.80     11654\n",
      "weighted avg       0.84      0.78      0.81     11654\n",
      " samples avg       0.80      0.75      0.75     11654\n",
      "\n",
      "\n",
      "Spent time: 966.6990716457367 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110\n",
      "train loss: 0.011040562763810158\t\teval loss: 0.22813069820404053\n",
      "epoch 120\n",
      "train loss: 0.01041000708937645\t\teval loss: 0.20531435310840607\n",
      "epoch 130\n",
      "train loss: 0.00909955333918333\t\teval loss: 0.2362905591726303\n",
      "epoch 140\n",
      "train loss: 0.007770299445837736\t\teval loss: 0.22582726180553436\n",
      "epoch 150\n",
      "train loss: 0.007602329831570387\t\teval loss: 0.23019294440746307\n",
      "\n",
      "epoch 150:\n",
      "\n",
      "f1_score for 50 classes: 0.8098347695440359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86       215\n",
      "           1       0.90      0.79      0.84       208\n",
      "           2       0.91      0.87      0.89       255\n",
      "           3       0.90      0.84      0.87       200\n",
      "           4       0.86      0.89      0.88       171\n",
      "           5       0.89      0.91      0.90       360\n",
      "           6       0.92      0.86      0.89       248\n",
      "           7       0.84      0.84      0.84       285\n",
      "           8       0.82      0.54      0.65       128\n",
      "           9       0.73      0.62      0.67       249\n",
      "          10       0.78      0.77      0.78       146\n",
      "          11       0.90      0.83      0.86       185\n",
      "          12       0.76      0.75      0.76       418\n",
      "          13       0.94      0.85      0.89       319\n",
      "          14       0.96      0.92      0.94       325\n",
      "          15       0.95      0.89      0.92       127\n",
      "          16       0.87      0.83      0.85       302\n",
      "          17       0.75      0.79      0.77       122\n",
      "          18       0.98      0.89      0.94       218\n",
      "          19       0.82      0.63      0.71       106\n",
      "          20       0.84      0.70      0.76       366\n",
      "          21       0.78      0.57      0.66       176\n",
      "          22       0.60      0.64      0.62       194\n",
      "          23       0.81      0.75      0.78       331\n",
      "          24       0.82      0.70      0.75       250\n",
      "          25       0.82      0.74      0.78       239\n",
      "          26       0.80      0.80      0.80       132\n",
      "          27       0.87      0.89      0.88       267\n",
      "          28       0.95      0.88      0.92       178\n",
      "          29       0.83      0.78      0.80       438\n",
      "          30       0.90      0.75      0.82       186\n",
      "          31       0.74      0.73      0.73       233\n",
      "          32       0.95      0.81      0.88       148\n",
      "          33       0.92      0.79      0.85       267\n",
      "          34       0.87      0.77      0.82       261\n",
      "          35       0.57      0.46      0.51       203\n",
      "          36       0.96      0.86      0.91       150\n",
      "          37       0.80      0.70      0.75       171\n",
      "          38       0.83      0.78      0.81       410\n",
      "          39       0.86      0.74      0.80       273\n",
      "          40       0.76      0.69      0.72       151\n",
      "          41       0.74      0.74      0.74       125\n",
      "          42       0.92      0.85      0.88       255\n",
      "          43       0.96      0.89      0.92       273\n",
      "          44       0.82      0.77      0.79       271\n",
      "          45       0.93      0.67      0.78       171\n",
      "          46       0.86      0.74      0.80       414\n",
      "          47       0.84      0.88      0.86       177\n",
      "          48       0.91      0.80      0.85       207\n",
      "          49       0.85      0.82      0.83       150\n",
      "\n",
      "   micro avg       0.85      0.78      0.82     11654\n",
      "   macro avg       0.85      0.78      0.81     11654\n",
      "weighted avg       0.85      0.78      0.81     11654\n",
      " samples avg       0.81      0.75      0.76     11654\n",
      "\n",
      "\n",
      "Spent time: 1458.7025334835052 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 160\n",
      "train loss: 0.006753028370440006\t\teval loss: 0.24954204261302948\n",
      "epoch 170\n",
      "train loss: 0.006507575511932373\t\teval loss: 0.2113114595413208\n",
      "epoch 180\n",
      "train loss: 0.005612178239971399\t\teval loss: 0.26581400632858276\n",
      "epoch 190\n",
      "train loss: 0.005177581682801247\t\teval loss: 0.2603191137313843\n",
      "epoch 200\n",
      "train loss: 0.005007042083889246\t\teval loss: 0.27637624740600586\n",
      "\n",
      "epoch 200:\n",
      "\n",
      "f1_score for 50 classes: 0.8112029914302867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86       215\n",
      "           1       0.88      0.80      0.84       208\n",
      "           2       0.88      0.87      0.88       255\n",
      "           3       0.91      0.89      0.90       200\n",
      "           4       0.85      0.89      0.87       171\n",
      "           5       0.89      0.91      0.90       360\n",
      "           6       0.91      0.85      0.88       248\n",
      "           7       0.87      0.81      0.84       285\n",
      "           8       0.82      0.55      0.66       128\n",
      "           9       0.74      0.65      0.69       249\n",
      "          10       0.77      0.75      0.76       146\n",
      "          11       0.95      0.78      0.86       185\n",
      "          12       0.77      0.74      0.75       418\n",
      "          13       0.92      0.86      0.89       319\n",
      "          14       0.96      0.91      0.93       325\n",
      "          15       0.97      0.88      0.93       127\n",
      "          16       0.88      0.82      0.85       302\n",
      "          17       0.75      0.80      0.77       122\n",
      "          18       0.98      0.90      0.94       218\n",
      "          19       0.81      0.65      0.72       106\n",
      "          20       0.80      0.73      0.77       366\n",
      "          21       0.73      0.58      0.65       176\n",
      "          22       0.68      0.59      0.63       194\n",
      "          23       0.81      0.72      0.76       331\n",
      "          24       0.82      0.72      0.77       250\n",
      "          25       0.79      0.76      0.77       239\n",
      "          26       0.81      0.80      0.80       132\n",
      "          27       0.86      0.90      0.88       267\n",
      "          28       0.94      0.88      0.91       178\n",
      "          29       0.83      0.77      0.80       438\n",
      "          30       0.89      0.76      0.82       186\n",
      "          31       0.78      0.70      0.74       233\n",
      "          32       0.97      0.81      0.88       148\n",
      "          33       0.93      0.82      0.87       267\n",
      "          34       0.87      0.78      0.82       261\n",
      "          35       0.57      0.44      0.50       203\n",
      "          36       0.94      0.91      0.93       150\n",
      "          37       0.85      0.65      0.74       171\n",
      "          38       0.84      0.74      0.78       410\n",
      "          39       0.86      0.73      0.79       273\n",
      "          40       0.75      0.73      0.74       151\n",
      "          41       0.70      0.74      0.72       125\n",
      "          42       0.91      0.85      0.88       255\n",
      "          43       0.95      0.89      0.91       273\n",
      "          44       0.83      0.79      0.81       271\n",
      "          45       0.90      0.69      0.78       171\n",
      "          46       0.85      0.79      0.82       414\n",
      "          47       0.84      0.86      0.85       177\n",
      "          48       0.89      0.84      0.86       207\n",
      "          49       0.89      0.80      0.84       150\n",
      "\n",
      "   micro avg       0.85      0.78      0.82     11654\n",
      "   macro avg       0.85      0.78      0.81     11654\n",
      "weighted avg       0.85      0.78      0.82     11654\n",
      " samples avg       0.81      0.75      0.76     11654\n",
      "\n",
      "\n",
      "Spent time: 1952.424955368042 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 210\n",
      "train loss: 0.004719207528978586\t\teval loss: 0.27748867869377136\n",
      "epoch 220\n",
      "train loss: 0.004476815462112427\t\teval loss: 0.33619531989097595\n",
      "epoch 230\n",
      "train loss: 0.0043535069562494755\t\teval loss: 0.2798103094100952\n",
      "epoch 240\n",
      "train loss: 0.003994444385170937\t\teval loss: 0.3282116651535034\n",
      "epoch 250\n",
      "train loss: 0.0038168542087078094\t\teval loss: 0.4212845265865326\n",
      "\n",
      "epoch 250:\n",
      "\n",
      "f1_score for 50 classes: 0.8063143979317551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85       215\n",
      "           1       0.88      0.82      0.85       208\n",
      "           2       0.84      0.87      0.85       255\n",
      "           3       0.90      0.85      0.87       200\n",
      "           4       0.84      0.91      0.87       171\n",
      "           5       0.88      0.94      0.91       360\n",
      "           6       0.92      0.85      0.88       248\n",
      "           7       0.81      0.85      0.83       285\n",
      "           8       0.73      0.57      0.64       128\n",
      "           9       0.74      0.64      0.68       249\n",
      "          10       0.74      0.77      0.75       146\n",
      "          11       0.91      0.82      0.86       185\n",
      "          12       0.70      0.80      0.75       418\n",
      "          13       0.93      0.86      0.89       319\n",
      "          14       0.95      0.93      0.94       325\n",
      "          15       0.96      0.86      0.91       127\n",
      "          16       0.81      0.84      0.82       302\n",
      "          17       0.69      0.88      0.77       122\n",
      "          18       0.97      0.90      0.93       218\n",
      "          19       0.74      0.76      0.75       106\n",
      "          20       0.77      0.76      0.76       366\n",
      "          21       0.69      0.60      0.64       176\n",
      "          22       0.57      0.71      0.63       194\n",
      "          23       0.74      0.79      0.76       331\n",
      "          24       0.77      0.78      0.77       250\n",
      "          25       0.82      0.76      0.79       239\n",
      "          26       0.80      0.83      0.81       132\n",
      "          27       0.86      0.90      0.88       267\n",
      "          28       0.92      0.89      0.91       178\n",
      "          29       0.82      0.77      0.80       438\n",
      "          30       0.88      0.75      0.81       186\n",
      "          31       0.71      0.77      0.74       233\n",
      "          32       0.91      0.81      0.86       148\n",
      "          33       0.87      0.83      0.85       267\n",
      "          34       0.85      0.78      0.82       261\n",
      "          35       0.49      0.49      0.49       203\n",
      "          36       0.96      0.88      0.92       150\n",
      "          37       0.79      0.71      0.74       171\n",
      "          38       0.77      0.80      0.79       410\n",
      "          39       0.78      0.74      0.76       273\n",
      "          40       0.71      0.76      0.74       151\n",
      "          41       0.68      0.72      0.70       125\n",
      "          42       0.92      0.87      0.89       255\n",
      "          43       0.94      0.89      0.92       273\n",
      "          44       0.77      0.78      0.78       271\n",
      "          45       0.87      0.71      0.78       171\n",
      "          46       0.85      0.77      0.81       414\n",
      "          47       0.82      0.88      0.84       177\n",
      "          48       0.93      0.82      0.87       207\n",
      "          49       0.85      0.81      0.83       150\n",
      "\n",
      "   micro avg       0.82      0.80      0.81     11654\n",
      "   macro avg       0.82      0.80      0.81     11654\n",
      "weighted avg       0.82      0.80      0.81     11654\n",
      " samples avg       0.79      0.77      0.76     11654\n",
      "\n",
      "\n",
      "Spent time: 2443.771024465561 seconds\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABI/0lEQVR4nO2dd3hc1bW33zUjadRlyZLcZFvuDVdMMc2mhhZI6IQkkEZCQsolBbhpJJfkS0KSSwgkN5BGQgIhgYSa0HszBmyDbdxtWS6yJFu9TdnfH/scnZE0arZGI2nW+zx6zpwy5+wj2fu3V9lrizEGRVEUJXnxJboBiqIoSmJRIVAURUlyVAgURVGSHBUCRVGUJEeFQFEUJclRIVAURUlyVAgURVGSHBUCRekGEdkhIqcluh2KEm9UCBRFUZIcFQJF6QciEhCRW0Vkj/Nzq4gEnHOFIvKoiNSIyAEReUlEfM6560Vkt4jUi8hGETk1sW+iKB4piW6AogwzvgkcCywCDPAQ8C3g28BXgXKgyLn2WMCIyCzgWuAoY8weESkF/IPbbEXpHrUIFKV/XAF83xiz3xhTCXwP+JhzLgiMAyYbY4LGmJeMLeYVBgLAXBFJNcbsMMZsTUjrFSUGKgSK0j/GAzuj9nc6xwBuAbYAT4rINhG5AcAYswX4CnATsF9E7hOR8SjKEEGFQFH6xx5gctT+JOcYxph6Y8xXjTFTgfOA69xYgDHmr8aYE5zvGuDHg9tsRekeFQJF6ZlUEUl3f4B7gW+JSJGIFALfAe4BEJFzRWS6iAhQi3UJRURkloic4gSVW4BmIJKY11GUrqgQKErPPI7tuN2fdGAVsBZ4F3gbuNm5dgbwNNAAvAb8yhjzHDY+8COgCtgHFAM3Dt4rKErPiC5MoyiKktyoRaAoipLkqBAoiqIkOSoEiqIoSY4KgaIoSpIz7EpMFBYWmtLS0kQ3Q1EUZVjx1ltvVRljimKdG3ZCUFpayqpVqxLdDEVRlGGFiOzs7py6hhRFUZIcFQJFUZQkR4VAURQlyVEhUBRFSXJUCBRFUZIcFQJFUZQkR4VAURQlyVEhUBRFGeq0NsCzN8Put+JyexUCRVGUoU5rPbx4C+xdE5fbqxAoiqIMdSJBu/WlxuX2KgSKoihDnUjIbn3xqQqkQqAoijLUCTtC4FeLQFEUJTlpdw2pRaAoipKchB0hUItAURQlSYmE7XY4BotF5EwR2SgiW0Tkhh6uu1BEjIgsjWd7FEVRhiXtriF/XG4fNyEQET9wB3AWMBe4XETmxrguB/gy8Ea82qIoijKsGcauoaOBLcaYbcaYNuA+4PwY1/0P8GOgJY5tURRFGb4M43kEE4BdUfvlzrF2RGQJMNEY81hPNxKRq0VklYisqqysHPiWKoqiDGVGavqoiPiAnwNf7e1aY8ydxpilxpilRUUx115WFEUZubRPKBtmMQJgNzAxar/EOeaSAxwBPC8iO4BjgYc1YKwoitKJYewaehOYISJTRCQNuAx42D1pjKk1xhQaY0qNMaXA68B5xphVcWyToijK8GO4BouNMSHgWuAJYANwvzFmnYh8X0TOi9dzFUVRRhxxrjUUn7s6GGMeBx7vdOw73Vy7Ip5tURRFGbYMV4tAURRFGSC0+qiiKEqS0y4EahEoiqIkJ+2uIbUIFEVRkpNhnD6qKIqiDASRETqzWFEURekjYQ0WK4qiJDeRICDDssSEoiiKMhCEg3FzC4EKgaIoytAnEopboBhUCBRFUYY+kVDc4gOgQqAoijL0CQfjNocAkkgI/vjKdhZ+70laguFEN0VRFKV/RILqGhooapuDNLaGEt0MRVGU/hEOabB4IMhOt7/Exla1CBRFGWZEghojGAiyAzb/tr41mOCWKIqi9BMNFg8M2QG1CBRFGaboPIKBITvdqmmDWgSKogwEkcggPkstggGh3TXUosFiRVEOk5oy+MEY2LM6Pvd/8ttQ9oa3rxbBwKCuIUVRBoyDOyHcBvvWDvy9IxF49TbY8HDUMZ1ZPCCoa0hRlMNi2wtQtdl+bmu027o9A/+cUIvdttZ7x9Q1NDBkplrXUIO6hhRFORQe+gK8+FP7ua3Bbut2D/xzgs12Gy0EOrN4YPD5hOxACg3qGlIU5VBoqYXmg/ZzuxDsHfjnhGIIgc4sHjisEKhrSFFGJOGQHbW77htjvAVdDhdjbOffUmv34+kaCsZwDenM4oEjK+CnQUtMKMrIpHYXvHMPbH3O7q9/CH463XO19IfmGq/TB3sPE+kqBPXxEIImu22t845FQnFblAaSTAiy01PVNaQoIxW3k3Y70qpN1pXTVN3/ez14Nfzr896+6wpqFwJnv/kgtDUdWnu7I2awWF1DA0ZOIIWGFnUNKcqIxB1Bux2p22m3NvT/XnW7bYqoSxchaPTO1Q9wnKA9WBxlEeg8goFDXUOKMoJpcTpO1yLoPHrvD631HS0JV0yCjbZTjhaCgY4TRGcNGWM/6zyCgSM7kKoTyhRlpOKOoNuDrc5+tIulr7Q1QPMBryOOFpOWOrvvD9j9QxGCUBus+6d3/w7nHCEwEU9wNH104MhJT6FeXUOKMjLpYhE4+4diEbQ1WheTe69oC6Clxu4XTLX7hzKXYPMT8PerYO+arudcIQNPxHRC2cDhuoZMLBVWFGV4Uv4WvHxrVLDYGVEfaowgHPLiDK57KNqqaKm1QpBdDCnp3tyC/uB+J5Y1EYwKPncQAnUNDQjZgVQiBlqCg1g1UFGU+PLOn+GZ73mda6iTa6i/FkFbVKffdMA5Fm0ROEIQyIG07Nj3f/3/YPPT3T/DFaeGfV3PhWJYBOoaGjjcekO6OI2ijCAa9lt/+sEddr9zsLi/MYJoC8K1CDrECGrtPdOyIJAd2+J4/v/BK7d2/wz3fvUVXc9Fz3twxUzTRwcOtxS1BowVZQThjqqrt9it62Pva4wgHOzol4++3rUIWjsJQVujFYK07I7WAtj9lhrY/Xb3M5vdDj5W6mlnITDGuoY0fXRgcEtRa+E5RRlBNOy324Pb7TbYZDv2cKvd780iePJb8OcPefvRHXuz6xrqSQg63d+tPxRshP3rYz+z3TUUwyLo7BqKOANXDRYPAMZQ1FYGQFVja4IboyjKgGCM15lGnAFesLnjZKzegsVVm2Dfe1HXR8cIolxD6aNAfFYcQs1WBGK5hqKziMrfjP1M9xn1MWIEwWav02+tt24hUCEYEF74CQsfO5/paQd4fG0cKgYqijL4NB+0C8REE2r23ELQdcQe6x5t9V7n3BYrRtAIgVxIz/NG/GlZ9qez68nNBBJ/90LQ1oNFEGyGrCL7ubYc9rxjP6traABY9BEEuD33Hh5du0fnEyjKSKC7jjS6YFwsiyAShie+aQPM7amcezte70uNihE4weH0PG/En5YFaTldYwTu+Skndl3K8l+fh0ev80SnYb/n+nEJNduMpNRMeO12+MNZXnviRPIIwaiJcOq3md3wOkvCa3hMrQJFGf7EdK20QKsjBCkZsYPFteW2k93wqCcEbiVR9/q8EmsRVG91LIJsRwic67p1De2BjALInwJNVR3PbX7SWgmuEJhw16J4wRY7PyEl0PH4cE0fFZEzRWSjiGwRkRtinP+ciLwrIqtF5GURmRvP9nDkJzBpOVye8SZPrIvxD0hRlKFJTVnsiVuuRZA+ym7FZ4PFrkWQNyG2RdDumtnnXetaBO65UZNg+wvwyyWwd7Xt+DsIgesaqu9YKqJuD+ROsNc213jnGiqhsdJ2/G0NVqSgq5gFmyA1o+v7DscYgYj4gTuAs4C5wOUxOvq/GmPmG2MWAT8Bfh6v9gCQmo7MPptTeYOVWytoatPsIUWJC+Eg1OwauPv96Xx4+qaux10hGDvfbrOKO46yc8fHtghccaja4h1zLYLWBisoJmriafNBzzXk1gJys4ZMpGOmT91u+9yMUTbQ66aDVm6w28ZKaxGMnuY8t5MQhFqsEHRmmLqGjga2GGO2GWPagPuA86MvMMZERXTIAuJf+2Heh8kI13NUZC0vba7q/XpFUfrPmnvh9qUdffV9ZetzsOMVbz/UBge2WxdNZ+orIDULCqbY/exi7zjYkXms9FE3gFy10TsWbRGk5cCEJXbfHbkHcmDcIu/6tGx7DKx4tDXBa7+y1kvueM9Kaamx2wonlTTcBo1VMHq689zyjm0LNttnnnc7HPsF7/gwDRZPAKKHBOXOsQ6IyBdEZCvWIvhSHNtjmXYKJpDDh9JW8vT6GIEmRVEOn5oyO7I9FKvg8a/Ds//j7dftBoxdgawzDRW2889yBCBnnHN8HyCQM9Z27J3ri7kWgTsbGbzJXa0N1vd/8rfg69tg7BH2eFoWzPuwd31atj0GVlg2PAxP3Gg7ftc1BNY9BJ3mFBgongPZY2Hb8/ZQxTp45Mu2vanpsORjcOJXva8MR9dQXzHG3GGMmQZcD3wr1jUicrWIrBKRVZWVlYf3wJQAMvtcPuBbxbPrymkN6SxjRRlweiqq5nL3efDCLR2PtdbbGcLuJDHwBKBuD0Q61QlrqLCdvWsJ5Iyx2/oKm+4ZyLHzC0Kd5g657iLX/ROI8v23NdhOPiUNskZ7bqe0bM+dA55rCGwwucbOU6Jots0Yyhhl912raP8Gm1LqEsiFmR+ALc/Y9q37F7z1RytOqZn2mox87/phKgS7gYlR+yXOse64D/hQrBPGmDuNMUuNMUuLiooOv2XzPkxGpIEFbe/w4iZ1DynKgNMuBN38lw8HYcfLsOv1jsf3vQcY6zpxqXVcJ+G2rlk4jZWQVQijJtt9d9uwz47IA7l2v7N7qHMAuXiOZxG0NXgjfYAxjkUQcDr9Iy602/Q871hrgxWC7DHwhTdg0rEdXUPhkBWC8Yu8+wZyYNZZ9nk7XoYD2+xxE7FZQwC+qC56mLqG3gRmiMgUEUkDLgMejr5ARGZE7Z4DbI5jezymnowJ5HJBYCUPrT6EWuKKovRMe0pmN2naNWU2qNvZYnDr87fWeqP4aPdSZ/dQYxVkFsKMM+DTz3qj95oyyCyIGrF3EoLO+2PmWusiHPJcQy5jF9ite68L7oJr34L03Kj7N9i25UWNfaNdQ2Wv2mdGu5YC2TBluY0HbH7SK5EBIydYbIwJAdcCTwAbgPuNMetE5Psicp5z2bUisk5EVgPXAVfGqz0dSElD5nyQ02UVL72/m3BE1ydQlAGlN4vAHf3WdjofvVCLaxXUlnnHoq+PRGy5h6xCO3IuOdLrQJsPWj999Ig9ms774xbZkfiBbV6w2GXsfJh9Lkw+3u77/FDoBHqjhaCmzM5XcnHdOi21sP4h2+EvvNw7H8iBtEyYcKSdW3CgGyFwnzFc5xEYYx43xsw0xkwzxvzAOfYdY8zDzucvG2PmGWMWGWNONsasi2d7OjDvAtIjjSwNvcP2qsber1eU4cq/vgCPfbX36waS3mIEbgZQa21Ht83eNd7It9GJE9Ts8lYDK3vNLkJjjH2GiViLwMX1rQPkjuvYUUcTvR/Ig4nH2M/lK217oi2C1HS47C8wbkHX92gXmnrrwoppERyADY/AjNOtaLltdMVm/CLYu9YrcAeeawg8QRmOFsGQZ+pywoE8zvG/wbo9h5DipijDhfI3oez13q8bSHoTAtciiL4mEobK971Oud0i2AXjFtoR9eu/gqe/awvFufGCrCghiO5Ac8Z5nXFjpySTaIsgMx8KZ9prd610Kotm0yfc6w5sszGMUZO8cz6/jVFsf8m6neZ80Hme01439XT8Yq+wnDhdcrRF0C4EUYHmASZ5hcCfisw+l9N8b7N+twqBMoJpPtD/BdbDoUOfEBYJR83W7UEI3E7PDQY3H7RxA3fk3Vhp3T+1u+1IOy8q+7x8lScUmaO949EdaO4EKJ5rxaGzELY1eB1yRr7jWjoKdr7a1SLoCVcI3DkC0UIAVlx2r7Kfxy+22yynve4zoucmuNdEC1pmgdfmOJG8QgD4So4kV5rYu2tL7xcrynDEdaE0H+i44Ems6x74NGx91i6octtiuHU+HNzZ+zPK34IfT/FmyLoikDvBloOOrgTqcmCr1wG6cQS3Yy+abbcN++1kq3Cr7WDzSuxxf8B2ru7s4aweXEOp6dbC2P5ix+e31kP+ZEC8EXfJ0VC92T5vykm9vzfYFFN/ms0Igo6uIbCZQ+E2m/qZX2qPdbYICqZ62U1Tlnd9DzdrSeLXXSe1EFBsK16YinW6oL0yMmmt8+r092QV1JbDu3+Hjf+Bt+92ArSm+6yfaMpXWqGpcGr6u26hMfPs9tGv2BG8SzhoA6ulxwPitct13+RPtm6gxkrY/JQ9VnoizDkPjvo0TF5m7+e6hjrECKJdQ+PtdspJtm3RKaltDXa0nlngpXlOclxSSz8F00/r/b1d0rK8gPaoTkLgziUomOqlf7olpt0Ygc9nXV9ZRd7vLPo9Tv0OXPBbmHpy39vUT+IXhh4OFM8BYELbDvbUtjBhVIyULUUZzjRFBSDr93acEBVNhZOn0bDPlnRwiTWad1n9Vzsadl1IrvXgCsHYBTYt8r0HrLXwice9Z0VC9nx2secaiu7Ys4psx1250VbxLJoFxY6l8Mz/wMv/6z23g2uok0UAzij7f6xVcMQF9lhrg7VYTvue9zspPQku+bOd5NUf0nLsO+eXeqN8FzdGUTjTO1Ywxc43iM4CWnGj/fu412VFzZdKCcCCi/vXpn6S3EKQMYrWzLHMrC9nZ1WjCoEydHEtVpH+fS9aCHqyCCretdv6ClsaYtQkO2pv7UEIXrzFqfNTavfdmbWuEMz8gB3h7l1jF3KvWG/z9Tc/BYgd4eaOj7II3OBvEWQX2bz6Pe/AUZ/p+N4lS20sYcvTNuMnJc0750+zLpTUrI7B2Owxtmjd5OPt7OO2Bnt+yce87/p8MPc8+o1rDRz/5a7nXGsjWgiO+xIs+XjH60qP9z5f82q7t2KwSG7XEBApmsts2UXZgaZEN0VRumf9Q3DLtI6LrPeF6JTE7nL6oaNF0FDhdVzdCUGo1ZZCqN7sWQKdhSCjwI7Aj/uS9euv+r09vvkJW9Atu8j6/d1JYq7PP7PAisGuN6x/fdZZHZ/tZhXtW+sFXl1ErFspd7x3zJ8Cl91rXU2PXee8V0PfM4P6yqIruh5zXUPRQpCabstidMeYef0X/MMk6YUgMGE+02U3ZdU9jHwUJdFUvm87ysb9NpD7zj19+15fLQJ3zd56Vwhm2f3uXEPVW73yy66I1HRyDblB2KzR1ue+9Rk76i9fZWcCg31O9VYrLI1VdgTtT/UC2zPP9CZyuWQWeAHU6PiAS2qG5xZyKTkS5l9kSzlEInaWb18zg3rjmlfhmte6LiQDsV1DQ5CkFwJf8RzSJERLhWYOKUMYt0OuLbepl266Ym+4FkH2mO6FoK3JZvEE8mzHbiLWjy1+zyLY/Ra8/7j3nejyzcYp3FhTZhdfcZ/jdoJg3TkHttmANMYTgjHz7Pcr37cxAjcDaMGlUDQHPvx/HevtuJSeYLdZMYQgd7z9bmcmLLV1f/avs+84UBbBmHnW5RWL4jnWMioa2kKQ3DECaA8Umejp3Yoy1HBTMt1/p+5SjL3RdAAQ2yF1JwTlK23HOG2FdUGBdV2k53oCdNcpdnvtKrj/Sq/EgkvRHLvwyq+P82YERwdDS46y25f/15aMdlNH3ZF9xTqvbhDA4ivsT3eUngBv/F/HQLHLlY90zMNvb8NSu3XLPncO7MaDOR+EWefEFrMhxNBu3WDg5PYG6st6vk5REom7uIlbmKyvC740H7B+6ryJ3QvBK7dZn/yij3rHcsbZjrJzjOCBT9kR9fqHbEDZ7bjdYKcrAp0Zv9gGcRsqbKkFt2McPc122hXrrOsr1gg/FpOPt/fLHtP1XMaojumXLkWzbRDZFYKBjhF0xxAXAVAhgKwigv4MioN7qG0OJro1ihIbt0N2LYKe0jqjaTpgXRO5453qmp3+je95x/rul33Bm/AEtoMN5HnPGe+s1rV3DeAEMgtneb5v149fOBM+/jBc8qeOzwlke5kwrlsIbNmE4jlenn9fhSCzAD76IBx9dd+ud581fjFse8FrkwKoEIAILdkTmSj72aWZQ8pQxbUADsUiyHSEAOOt8euy5Wm7PfIT3qIuYIUgPdcrCBeJEpBTv223RbPsjz/NTvhKSYdl18LU5TC3w6q0lpKjbOG0aZ0mRo2ZB/vetRZBrOBvd0w7uWOb+0Lp8d67DJZFMAzQGAFg8qcw+eB7bD3QxBET8nr/gqLEm5Za+NUyOP8O2+F1iRH0wyLIGWcnT4F1D7mlGsDW8ckcbd0pxtgJWakZNjc/kOutp9vWZNfYLTkKln0RiufZ2bCRIMw626aCfm1TxwBxZ1bcaIPAna8ZH5UF1VeL4FBZdi288GP7eTBiBMMEtQiAQNFUJsl+dlZrOWolToRDsOvNvl9fudHm/e9aafddF42bBdQf11C7RUDXOEHdHk8kRKwl4K77Gx0sbmuEycfZLJ6UNJh1pk3RHDUJZjqunp5EAOzoffKyrscXfZR2d5NbcydepOfCVY/ZDKLuZlknISoEQKBoGukS5OD+Q6y2qCi98e798LvTYM19fbveXVS9psyO1Du7glpqbZXPtl7cmc1OjMDt3LsIwW5PCMCO8t0VuQK5nuURbIqfKyU13XbOmaPtRLN4U3oCfOaZjusBJzkqBGBrmQChqm29XKgoh4g7sn7xp327vl0IdtrRuJur7xIJWhfHr47teDzU6k0iCzbbDjyzwHZ6KRldZxfX7e44C/eiP1h3FFjXSUudFaK2ho51fAaa0uPhG9va638pg4sKAdjJM4C/Zkdi26GMXNxa8tWb7eSsqi1Q9kb310dbBN3FA3a8YoUi2k302FfhJ1Phngu9stDZxdbtkzveFjarKXNm1zbZWcDRdf59Pi/dMT3XCpC7Elj0gu7KiEKFANpN48ymPYTCkQQ3RhmRuPMAwPr9770Mfn9GVDqo4+pxcYWgbrc3wpdOK1RVOjXwo909u1YCxmYD7XMKybmVLHPH2/O3LYaVv/G+F+0aisb117ulqFUIRiwqBACp6TQHihhHFXtr+1nUSxn+hFrtCDmeNNdA9ljbmTfs92rpPPZVO6K/dQG8dod3/YHtdjGTSMiWXwAv2yfFqZLrFmlz3T2hVqjeYhdbB299ADclM3e8LfAWCcEbv/GqZnYnBG7w17UsVAhGLJo+6hDKKaGkqZJdB5qYWBBHX6gytAiH7Epcy6+Hoz4V+5qyN+DV2+wkqeh1Y8NB2PKMzaDpjeaDXnCycb93n63PwJPftBbD9hfh+C/ZCqP1e2DisbDrda9Dz59sXUH5kz1xAG9kX7nRunKmnWKtAbeQXFaUEICtBHpwO7zzl47HO9PZIohnjCDOBINBysvLaWkZ+QO99PR0SkpKSE3t+2L3KgQO/oJJTNj/Oq8daOK4RDdGGTxqdtpJVvt7KOK2+i/w/qO2jHF0+eB1/4QHP2MrT3ZXdMylpdYKgT/FFmZrrLSTuNb/C952ZuHuedsGZt1yzlNOskLgunjc9XBHdSME7jtMOxVe+YW3xoDrGnJX7DrxOlsS+r1/2P1uXUNOnv0IcA2Vl5eTk5NDaWkpMsglngcTYwzV1dWUl5czZcqUPn9PXUMO6YWljJdqyqrjt0C0MgSp2my3Dd3UyAFbFx86lnQGb53aviwM31xjJ21lFVv/f7DJlnRY6lghuROsq6emzJs97FbYbBeCUruNLgUBnmto/3o7y3fSsbYOT02ZdSO5Hfj4RXY1rQWXwDk/s8cyR8euywM2WAxQN/yFoKWlhdGjR49oEQAQEUaPHt1vy0ctAgdf/iTSJERtZTkwuKsDKQmk2hECd73czjQd8Ebfbp399u86pcs7l22IRfNB67s3Ea/oWXYxLP6o7fjnXQB/u8JaBa5FMHa+HcXXO0KTP7njFmzJBlcIKtbb+j8pAWsFNFTYrdv5TTwabtxl9wumwgnXeXGGWIwg1xAw4kXA5VDeU4XAJc+a3cHqnQluiNKFgzvhzuXwif9469YOFFW9CEF51GxgVwjW3m8Dud0JQUsdvPpLW8jt6ZusJdBSY11DPp83JyCr2PrvL/q9DfT6UmH329Y6yRlv8//nfNBm+IAVDrDWg/jtfcbOtxZJTZm1XGaf41w7xhGCTiUbojuJ077b8+/GdSm5K5BpbZ5Dprq6mlNPPRWAffv24ff7KSqyv9+VK1eSlpbW7XdXrVrFn/70J2677ba4tU+FwGXURAD8dTq7eMixf4PthPe9O/BC0N6ZdyMEZa95n93yDi/fan3+rnh0dis99W1464/WPfPOPda339ZgBSG6Tn52pwXKx8636Z1tjV7M4cgrPSEYu8DGDUqOcso/1Fp3z7v/gPudNXBP/Jrd5oxzlnKMekZ/SU23mUMHnImWacPbIkgko0ePZvXq1QDcdNNNZGdn87Wvfa39fCgUIiUldne8dOlSli5dGtf2aYzAJc8KQX5bhZajHmq0d7h9cMH0xv0fhzfu9PZdi6C1NvZ6wNtf9MonuxZBU5UtxhZu7dqustetCCB2EZZI0K7+BXYZxuiOOau447OmnWwtkMr3bUVO8LZgLYQrH7ETwNLzbDrqqEl2wtmed+DDv/EWjHGrch6OEIC1LEJOquswdw0NNa666io+97nPccwxx/CNb3yDlStXsmzZMhYvXsxxxx3Hxo12Fbjnn3+ec889F7Ai8slPfpIVK1YwderUAbMS1CJwCWTTlpbHhJBNIc3TKqRDh6Yqu+1JCJoO2Nz8vG4yYMC6T9Y/BDW74JirnVH9fiiYZjvrxsp2y9A+r9K6albcaAWj+aCdb9BY5V3jD3Rs19ZnAYGTvgYv3tLx+Rn5kBlV36az22bGGfDSz6zLpzhKAP5rnVcO2iWQC5lp7QMYTvwazD7bO5/tZDd1Xty9v2SPgapN9vMIcQ1975F1rN8zsGuUzx2fy3c/OK/3CztRXl7Oq6++it/vp66ujpdeeomUlBSefvpp/vu//5sHHnigy3fef/99nnvuOerr65k1axbXXHNNv1JFY9EnIRCRLKDZGBMRkZnAbODfxpgRNXQO505kQnMVZVqOemjhdrw9ZfY8/jXYuxa+uKr7azY/Zbd710Brg2cNTF7mCMH+jkKw5WnA2Oqab/7Wik1LTce6PyVHQcM+b79ut+08F3/UCsGoyd6i7m7WENhCcP5O/3knLLUj/ZbajpZAdNlol6M/Y0Vo9jlwyZ+92IDLgFkETnt9KbbqqDKgXHzxxfj9dk5JbW0tV155JZs3b0ZECAZjd6/nnHMOgUCAQCBAcXExFRUVlJTE+DfSD/pqEbwInCgi+cCTwJvApUAPi4oOP1IKJjNh/2qe1QVqhhaNfbAIKtbZDKDa3d1bBZufsn57E7YuGDdNc9op1pffOU6w+UnbqY9daN0yzQc9N1XhTLvQ+9gjYPVa7zt1e+wErfxSuPiPdmT+B2fCWfoor2ONtcSiP8XOAdjwiLfyV3cs+bj3ee55Xc+3WwQD4BqCYZ062plDGbnHi6ws7/f67W9/m5NPPpl//vOf7NixgxUrVsT8TiAQaP/s9/sJhUKH3Y6+xgjEGNMEXAD8yhhzMTB0fpsDRGrBZCb6qijTdQmGFk29WASRsBfQjA7ugs142fgfW2Bt+wsw/xIrBmWvW8sgfZQdiUPX9XbLXoOpK2ymT0a+IwROW878EXxhpe0oW+u8ctDRQjTvwzZl0++MpDPyvcXWs7vpoE/7Llz2l8MffRfNtplFvQlKb7jClTpyhGCoUltby4QJ9t/OH//4x0F9dp+FQESWYS2Ax5xj/h6uH57kTSSDVg5W7ev9WmXw6M0iqC2HcJv9vPPVjuf+fT3ceyn8/So7iWvpJ2DMEbDzFSsE4xZ6o95ooQmH7PPcyVvtQuBYBDlj7Upe7nddEYle6AVsKYn2e4yy7qDM0V0DxS75pTDzA93+KvpM4XS4fsfh1/dvtwg0UBxvvvGNb3DjjTeyePHiARnl94e+uoa+AtwI/NMYs05EpgLPxa1VicKZwh86oHMJhhSuEDRV2/o+nX3rbgpoel5XIcDYzeYnYP7FdtbtjDPg5Z/bEfOx19g0yUBux7kEjZV28pfbEWYUWOFwr3ELubnn6yusWLTVd63dUzDVBlzdIm7n/m/7GhhxxZ0ZfDi4FsEIcg0lmptuuinm8WXLlrFp06b2/ZtvvhmAFStWtLuJOn/3vffeG5A29ckiMMa8YIw5zxjzYxHxAVXGmC8NSAuGEk6gMLV+t5ajHioYY11DaTmA6Zix4+K6hRZcZkszV23xzrU12jILs8+FM35gjy39hN1GgtYiAG8mros7m9Zd2StjVEfXULuLx+ko63Z3X9Z57AI7QcwVsLnnw7gFff0NJBY31qCuoRFNn4RARP4qIrlO9tB7wHoR+Xp8m5YAnFS8cezXctRDhbYGG5R1J1jFcg9Vb7Ed1YlftZ3+iz/xzjVWwfRTrd/dzaTJK7ELrgOMW2S3Y+fD+485mUJ4pZfdInMZ+da1VFfuZPw4xnR+qR3pP3adnXEMXYXgxK/CZ1841N9AYhmBwWKlK32NEcw1xtQBHwL+DUwBPhavRiWMjHzCKVmUiE0hVRJAqLXjvjsCd9MpYwWMq7fahchzxti0ynf/7lkFTdXe6D2a026ypafdBcw/eKut0/OPT0KoLYZF4OT/V23umP+fngufftaKwcs/t8c6Zy2lpnuWw3Ajs8C60DRGMKLpqxCkikgqVggeduYPmLi1KlGIEMmbyAQVgsSw9Tn4fxO92jbgFUUr7sUicDv0475os4LevttO/mqq7jpxC6BwBpz8317tnYx8u99Sa7OF6vfZ+7jpl5kFdlu1qWtKZuF0O+oHQDx3ykjA57fCFktMlRFDX4XgN8AOIAt4UUQmAwM7NW+IkFIwmUmyX4UgEWx52pZt2PGSd8wNzroWgRsYDjtZFaE2O2FrtFNaIbsYZp4Ja+6zImDCXmC3N6acZFM9Nz9pLYKsYs8F5FoEzQdjC8sRF3nzBEbaxKsr/mFnVysjlr4Gi28zxkwwxpxtLDuBk+PctoQgY+Yx3beHPVU1iW5K8rD/fevecev+71rpnXNdQ7kTYOZZ8Obv4N7L4fYjbUmJmp02u6dgmvedxR+16Zxr/mr3Y3XcsQhkw+Tj7cSzhoqOi9CMXeAJSue1g8G6Ts74H299gZFE0azh69pS+kRfg8V5IvJzEVnl/PwMax309r0zRWSjiGwRkRtinL9ORNaLyFoRecaxNBLL+MWkECalcl2iW5I8/OoY+OUS2LPa7rulnxsqbTnnQJ4NWp5xsw0cb3zcLu6y+q+eheBaBADTT7d1cd51VuDqj1tjxhlQtRF2v9VRCDIL4DJHWNxMo84s+TisuL7vz1KSCr/fz6JFi9p/fvSjHx3SfVasWMGqVT2UUjkE+jqP4PfYbKFLnP2PAX/AzjSOiYj4gTuA04Fy4E0RedgYE70m4DvAUmNMk4hcA/wEW7oicYxfBEBBXQ9LFyrdEwnDnSvg+C/D/It6vtaYjvXxI0GbxbN3jfXVP/UdO+L/6AM24Fo4HT70a/ud1+6A126HI6+y3x0dZRH4U6B4DpQ7/1n6ahGAnQ381HesWymnk69/0jHwtS2em0hR+kFGRkZ7KeqhRl9jBNOMMd81xmxzfr4HTO3lO0cDW5zr24D7gPOjLzDGPOeUrgB4HTi8ykkDQd5EmlNHMT24heqG1t6vVzpStdnWwd/Wy3zDfe/BjyfbVbWia/QvuxYwthOv3GBdNe6SjQALLrYCc9y1dv7A239ySjcUdLx/8Rza8xn6GiMAyB0HRzjjGzdjKJrsIi9uoCiHyX/+8x8uvvji9v3oktPXXHMNS5cuZd68eXz3u70sInSY9PVfdLOInGCMeRlARI4Hmnv5zgQgepWXcuCYHq7/FDY1tQsicjVwNcCkSZP62ORDRITWooUsKN/GmvIaTpkdoziY0j1719ht9baer1v1ezvq373KuntKjoYJR8KM0+z5ivds2Wg3z78zM8+y9fGrt9gKoJ1xs4ygfxYB2JXF1t4/OLN/lcHn3zd460APFGPnw1k9u3qam5tZtGhR+/6NN97IhRdeyNVXX01jYyNZWVn87W9/47LLLgPgBz/4AQUFBYTDYU499VTWrl3LggXxmYjYV4vgc8AdIrJDRHYAtwOfHahGiMhHgaXALbHOG2PuNMYsNcYsdZd3iyeZpUcyQ8p5b7vWHCLUCs/9PztDty/scypxuouxxCLY7Pnv3QXgF19h/yNl5NsR/N411j0zqhvhT8uE6Y5oRMcHXFwhSMuxq3/1h3EL4dpVcMSF/fueovSA6xpyfy699FJSUlI488wzeeSRRwiFQjz22GOcf751nNx///0sWbKExYsXs27dOtavj5+7uk8WgTFmDbBQRHKd/ToR+Qqwtoev7QaiirtT4hzrgIicBnwTWG6MGRK+mLTSY+HVCPXb3gC6CQwmC2Wvwws/sq6WeR/q/XrXImiosIupBHK6XrPhUbsiGMB+5x93tPtm9HQ7pwA6LtTembnnw4aHO2YMubhCcKgLsxTGEBdlZNDLyH2wueyyy7j99tspKChg6dKl5OTksH37dn7605/y5ptvkp+fz1VXXUVLS/yqHfRrqUpjTJ0zwxjgul4ufxOYISJTRCQNuAx4OPoCEVmMnaNwnjGmh1VHBpmJRwOQW7mKSGTkzZvrF24J6JqdPV8HNvi7d61XluDlW+GRr3S9bsND1v+eW+JZBNGZPYXTvfWBR/UgBDM/AFNPhhmndz2XXWTFpT/xAUVJAMuXL+ftt9/mrrvuancL1dXVkZWVRV5eHhUVFfz73zG95gPG4US9pKeTxpiQiFwLPIEtWf17p3Lp94FVxpiHsa6gbODvYrNHyowxMVbZGGQy8qnJmc6Cmg1sr25kWtHIWKLvkGh0ZvYe7EEI/nMjvOPU8mmthYWXwso77bKLYGv3pzoB4WALbHnWXrN3rY0RQEchiHb1dOcaAmttfPxf3Z8/4sKBqcCpKANA5xjBmWeeyY9+9CP8fj/nnnsuf/zjH7n77rsBWLhwIYsXL2b27NlMnDiR448/Pq5tOxwh6HWobIx5HHi807HvRH0+7TCeH1dk0jKW1P2DR7dWJrcQ9GYR1O+DlXfZtNvsMfbn6M9aIXD/idSUQZGzQMr2FyHYCLPOsaWbXWIJQUrG4a2wdfZPer9GUQaJcDjc7bnbb7+d22+/vcOx7hanef755wewVZYehUBE6ond4QuQMeCtGULkzjwRWfdntr73OhybxNkjTb1YBCvvgkgILrjT1t13yRkP9U5Z5oM7PCFY/y872WvKifD+o/aY+GyZZ5fRM+x21KSO8wwURYkLPcYIjDE5xpjcGD85xpgRnUwtU04CIHfX8wSTeW0Ct8RDTZkt4hZNJGLz+Ged1VEEwE7MWuQsaX1wh91WbbY1gBZeZjN53LIFGfm2uJlLwRRAenYLKYoyYPQrWJxU5I6jpmAhy81K3t55MNGtSRyuRRBu7Vr5c99aW9Nn7oe6fu/MH8L5d1j3zsEddmWxf19vc/+XO9VGXCHoXAIiJWAXlJ9y4kC+iaIo3aBC0AMZ889joW8bb6zuKUt2hNNU7a1O9ehX4PVfe+fcRVymnRL7uyJ24ZbqzfDXS2DrM3Zxdnfhdje7KFYtoI89aMtUKMoAYUxyZAAeynuqEPRAYL6d2JG67u/Jm0baWNVef4lN/4Fnvg/NNXZ/yzPO4u89BHTzS61gbH0WzrrFLhzj0pMQKMoAkp6eTnV19YgXA2MM1dXVpKen935xFCPaz3/YFM5g35jlfGLf/by7+koWLumpQsYIxF3YZfwltiLo6Ol2Atia+2zt/vKVcFwvS1fnl9oy0XkT4ahOJZq7cw0pygBTUlJCeXk5lZWViW5K3ElPT6ekpH9l21QIeiH30l/T9ItjyHz2W7DkqUQ3Z3BprbULu+ROgE89ZSt8/ulD8OzNtnPPLLSll3siv9Ruj7yyY0AY7MIv0P9aQIrST1JTU5kyJYmz/3pBXUO9kFkwgZcLLmBaw5tQs6v3LwxXIhFbCTQadzJZVqF1DwVy4AM/hJlnwJwPwmeecTJ8emDqcph4LCy5suu5tEy44Ldw5CcG5BUURTk0VAj6QNOci/FhaHjznkQ3JX68/wj8ehlUbrKlIt5/HN75sz0XXeJ50jFw0e/hgt9AXh/Mz+I58Kknul/hasHFMGpi7HOKogwKKgR9YM6c+bwWnovvnT/3vQrncKPCWZFt7xr45+fgvsvhlVvtMa3XoygjGhWCPjB3fC6/4UIymvbAv7+R6ObEB3fJx11vwNq/2cXYXdSHrygjGhWCPpDq9xGcfAL3p18E79zjjZ6HG6E2aGuyK3vdtgQqN3rnqp31A979O2Bg0eVw6ndtldCsbtw6iqKMCFQI+shx0wr5RY0z03X7S4ltzKHy6H/B786ANX+zC8es+6c9bownBC01djvhSDjxOrhuA6SkJaS5iqIMDioEfeT0uWPYQyENGeNh58uJbk7/iDhVD7c9BxXvwuu/svtbn7Xbxkpoq/fW6C2c6S3QrkXfFGXEo0LQR2YUZzOlMIt3ZB7sfNWOoocydXvt6mJNB+BHk2yV0DpngbjWOsgeaxeIf/BqGxwGmG0Xzabk6MS0WVGUhKBC0EdEhDPmjuGxuil2tm3l+4luUvfU7ILfnQ73XGjb2dZgS0MATFoGCJz+fTtZbO3fbA0gsAu5+FK02JuiJBkqBP3g7PnjeCk0jwg+eOxr0FKb6CbZdM+/XwV3fxD2vGMtlb9fBbW7rADsesNe11oHvlS4/F749DNwxAV25L/8Blv/3x+wS3R+6R1YcGki30hRlEFGS0z0gwUleWQWT+EXfI3/2vW/8OS34LxfJq5BxsADn7HloVMC8NvTYMUNdvnHWefAxsdg2/Pe9WOPsL7/kiPt/qedkhnzL4bKDbYEhK4BoChJh1oE/UBEuOjIEn6xfxH1M86D9Q/bOvuJoLUeyl6Dqo1wxs3w+dft4jDP3gyBPDj5Rntd2es2CDzhSJh1dux7FU63JSMURUlKVAj6yYcXT8An8DTH2FTLHQlIJa0pg1umw18ugUCudfNkFsAFd1n3z5FXQtFsED+EWmDUZPjMs7B8hE6GUxTlsFDXUD8pzk1n+cwifrFd+FBqFrLyt7bCZuelGuPJun/ZDj4rFxZfAWnOwjHjF8FX3rV1fVw3z8HtkD958NqmKMqwQy2CQ+CiIyeyo85QMfEs64f/9fFQv+/Qb3hgG/zjk9Da0PF4TZmt/d/Z/bT+IRi3CL6+2c7+jSZ3nFfu2a0Mqn5/RVF6QIXgEDh1TjF5Gan8MOXzcNXjEGyC9x60Jzc8Cjte6d8N194P7z3QMbC74VG4dQH887Ow5l7veG25DQbPPc/u9zThK98VArUIFEXpHhWCQyA91c+HFo3nP+v3U1t8tB2dr70P3rob/nYFPHxt/yac7XBmKm9/0Tv29p/sql75U6xQgLU67vuIzfWf9+He7+u6q9Q1pChKD6gQHCIXL51IWyjCv1bvhgWX2Hz+R75kl108sM2ZyNVkLw6H7MIvsQi2wK6V9rMbeG5rgu0vwOxzbE7/jpfhiW86heI2wWV/7VtMYupyGHMEjJ1/+C+sKMqIRYXgEDliQh7zxudy78oyzMKPwOKPwXm32+wcsJO6fjLVrvr1q2Phsf/yvhxtLexeBeFWmHiMXQ94xyuw7kEbDJ75ASsyGHjtdphxOnzuZXu8L4ydD9e84tUNUhRFiYFmDR0GVy4r5RsPrOWV3WFOOP9270TJUXaxd7C1fKo3259FV9hA7t3nw1WPwPjFjjtIYPn1cM8F8Ecn1z8tGyYfbyt/XvIn6+cfv2iwX1FRlCRAheAwOH/xeH7yxEbuemkbJ8yIWrxl+fXw/qPQUGmzirKKbH7/I1+BrNG20ud7D9rYwrt/tx3+tFPg3FttKuiB7TbTxy3/PPf8BLydoijJggrBYRBI8XPlssn87KlNbNxXz6yxOfbEjNPtz5ZnrBAsusIWcvvLxbA/YkVh039sDODANjjp6zb7Z6ku4q4oyuCjMYLD5KPHTiY91cdvX9rW9eS0U+BD/wcn/BdMPw3O/V8bC1hxPVRtgqdvgtQsmHPeoLdbURTFRYXgMMnPSuPiIyfy0Oo97K9v6XhSxC75mDHK7h95FXzqSVvkDWytoGVfgED2YDZZURSlAyoEA8BVx5fSFo7w8Oo9fftCfqmtC/Spp+CUb8a1bYqiKL2hQjAATCvKZt74XB57d2/fv7TgElv/X1EUJcGoEAwQ5ywYxztlNZQfbEp0UxRFUfqFCsEA8cEF4wH4+6ryBLdEURSlf6gQDBATCzI5c95Y7nxxG3trmxPdHEVRlD6jQjCAfPOcOYSN4QePbUh0UxRFUfqMCsEAMrEgky+smM6ja/fy1PqKRDdHURSlT8RVCETkTBHZKCJbROSGGOdPEpG3RSQkIhfFsy2DxTUrpjF7bA7f+te7NLSGEt0cRVGUXombEIiIH7gDOAuYC1wuInM7XVYGXAX8NV7tGGzSUnz88IL5VNS1cvuzWxLdHEVRlF6Jp0VwNLDFGLPNGNMG3Ad0qJ5mjNlhjFkLdFOsf3iyZFI+Fy4p4Xcvb+P9fXWJbo6iKEqPxFMIJgC7ovbLnWP9RkSuFpFVIrKqsrJyQBoXb248ezZ5GWl8+d7VtATDiW6OoihKtwyLYLEx5k5jzFJjzNKioqJEN6dPFGYH+OnFC9hYUc8PH9csIkVRhi7xFILdwMSo/RLnWNKwYlYxnzphCn96bSdPaxaRoihDlHgKwZvADBGZIiJpwGXAw3F83pDkG2fOYu64XL7+jzVU1LX0/gVFUZRBJm5CYIwJAdcCTwAbgPuNMetE5Psich6AiBwlIuXAxcBvRGRdvNqTKAIpfm67fDHNwTBfvX8NkYjp/UuKoiiDiBgzvDqmpUuXmlWrViW6Gf3m3pVl3Pjgu9x41mw+u3xaopujKEqSISJvGWOWxjo3LILFI4HLjprImfPGcssTG3lu4/5EN0dRFKUdFYJBQkS45eIFzBqbwzX3vMV7u2sT3SRFURRAhWBQyUlP5e5PHk1ueirX3a/zCxRFGRqoEAwyhdkBfnzhAjZVNPDfD76rwWNFURKOCkECOHl2MdedPpMH39nNTY+MuEQpRVGGGSmJbkCy8sVTplPfEuSul7Yzc0wOHz12cqKbpChKkqIWQYIQEW44aw4rZhVx08PrWLn9QKKbpChKkqJCkED8PuEXly1mUkEm19zzFuv3aKVSRVEGHxWCBJOXkcpdVy4l1e/jov97VWsSKYoy6KgQDAGmFWXz0LXHM704m8/8eRW/fWkbw23Gt6IowxcVgiHCmNx0/nb1Ms6cN5abH9vA9x9dr2KgKMqgoEIwhMhI83PHR5Zw1XGl/OGVHfxSl7pUFGUQ0PTRIYbPJ3zn3LnUNgf5+VObqKhr4SunzaQoJ5DopimKMkJRi2AI4vMJt1y0gM8un8pf3ijjmB8+zR9e2Z7oZimKMkJRIRiipPh93HjWHJ78r5NYPrOImx/bwJs7dK6BoigDjwrBEGfmmBx+cfliJuZn8Mk/vsnr26oT3SRFUUYYKgTDgNz0VP7ymWMpyglw+V2v85X73uFvb5bR1BZKdNMURRkBqBAMEyaMyuCfnz+eT58whafWV3D9A+/ykbveoKapLdFNUxRlmKNCMIzIy0jlm+fM5d2bPsCvr1jC+j11nPqzF/j7ql0A1DYHE9xCRVGGIyoEwxCfTzhr/jge/PxxTCvK5uv/WMs5t73Ewu89yWNr9ya6eYqiDDNUCIYxR0zI496rj+Wq40rZXdPMxIIMvvvwOrUMFEXpFyoEwxy/T7jpvHm88+3T+fUVR3KwqY2P3PU6b+08qKufKYrSJ1QIRggiwhET8vjNR49kT00zF/76VU748bP89qVtNLRqdpGiKN0jw62w2dKlS82qVasS3YwhTW1TkGfer+D+Vbt4fdsBctJTuOGs2SwsGcXWygaKc9JZNm10opupKMogIiJvGWOWxjynQjCyWb2rhlueeJ9XtnSciPa7K5dy6pwxCWqVoiiDjQpBkhOJGB5es4dQxLCwJI8v3vsO++tbmVaUxSmzx3D1SVPx+yTRzVQUJY6oECgdWLenls/d8xZZaSm8v6+eiQUZnD1/HJ9fPp28zNREN09RlDigQqDExBjDv9/bxz/eKuf5jfvJTEth9tgcphZlccGSEo6dqnEERRkpqBAovbJ+Tx33vLGTLfsb2FRRT11zkAuWlFCUEyA3PZXlM4uYMy4HEXUhKcpwRIVA6ReNrSG+/dB7PL2+guZgmGDY/hvJTU9h5pgcZozJYVpRFhlpfvbVtnDSzCKOKi1IcKsVRekJFQLlsKhuaOXJ9RW8t7uWzRUNbNpfT02TN3s51S989qRpTCrI5LS5YwhFIozOCmgAWlGGED0JgS5VqfTK6OwAlx89qX3fGENtc5CmtjDpqX6u/evb3P6cs77yA3ZTmJ3GjOIcRmencdVxpWQFUijKCTA6K03dS4oyxFCLQDlsjDE0toXZUdXYHnR+q+wgFbUtNt7Q4s1szgmkUFqYRWlhFlNGZzJ5dBapKT7G56UzsSCTvbUtzJ+Qp9aEogww6hpSEkZtc5AXNlWS4hP21bawo7qR7VWN7KhuZPfBZmKVQyrJz2BGcTahiKEgK40PzBvLKbOLWVteS4pfmD02h4xUPy3BCBlp/sF/KUUZhqhrSEkYeRmpnLdwfMxzraEwuw82E44YNlU0sK+uhVEZqTy6dg+VDa2k+Hxs2FvHQ6v3kOITQo5qZKT6Kc4NsLO6iaNK88lNT2VsXjrjR2XQFoowY0w2WWkp+HzCtKIsCrMDlB1oYkZxtrqlFCUGahEoQ5pwxPDS5kqefX8/R5UWEEjx8fymSvbVtjCjOJtXtlYRiUDZgSYaWkOIQPQ/aRHISkuhoTXEjOJsjp9eSF1zkLW7a6lvCfKRoyeTn5VKcU6A1btq2VRRzxET8pg3Pped1Y00tIQozk1nzrgcAil+xuala5xDGZaoa0gZ8YQjhrZQBBHYVtlIWzhCMBzhlS1V7K1pYc64HB5du5d1e+rITk9hYUkeraEIL22uar+HT2BKYRbbqxpjuqxc0lN9TBiVQUl+JtnpKby/t45Uv4/8zDRKCzMpyc9kza4ajpiQR35mKil+H8U5AcbmpTO1MJsn1u1DBJZNG01mWgqvbKmiMDuNqYXZ5GelcaCxjVU7DnDMlNE601sZMFQIFMXBGNNhNF9R14II7K1poTAnwIRRGTS0hti4r45JBVkUZqdRfrCZLfsbaA1F2FfbzO6aZsoP2p+a5jZmjclFBA40trFpXz31rSEmjMpgd01zl+f7hA4ik5bioy0Uad/Pz0ylJRihORgmze9jenE2pYWZNLWF2VzRQCDFRyDVz8HGNtJSfEwsyCA9xc/CiaM42NRGZX0reRmpTCrIZO3uWhZPHMX8CXmEIob99S20BCOk+IS0FB8HG9vw+4S54/PITPOz+2AzPh/kpqeSm5GKT4TtVY1kBfyMy8tgbG46GWl+9tY288Bb5UzIz+CUWWPIzUhRC2kYkDAhEJEzgV8AfuC3xpgfdTofAP4EHAlUA5caY3b0dE8VAmUoEwxHqG0OUpgd4GBjG8FIhFDYsL++lZ3Vjby3u5YTZhSRn5nKq1ur2VfbwhnzxtASDLOtspFtVY34BE6dM4bXtlazqaKeHVWN+H3CgpJRBMMRWoJh8jLSaAmF2VPTTH1LiC37G8hIta6r/XUtNLaFKcoJUFnfOqDv11m4AAIp1hpqDoZJ9QvpqX4yUv1kpPkRYGtlI83BMDnpKRRmB0jxCYEUH/Mm5BEMRahrCZKXkUpDa4jRWQHSUnzsr2+lsTVEYXYa26samZifSU56CgebggTDEUoLs8gOpBBI8ZGW4iPNb7cA++pa2HWgmVS//Z258aWy6kYCqX4mj85kX20L40dlIEBzMEzEQFaanxS/rz1WVZQTwCeCCAjQFoqQk55KbkYKTW1hDja1sWFvPVOLsphelE1rKEJxbgATgZz0FJqCYfbWNGOASQWZ1DYH8fuEURnWSjTGEIoYguEIgRQ/EWOoqGvBGKhvCTFpdCZZaX7awhHS/L7DFtuECIGI+IFNwOlAOfAmcLkxZn3UNZ8HFhhjPicilwEfNsZc2tN9VQgUpSt1LUGynQB5WyjCgcY2xualU36wiW2VjaT4hHGjMkhP9REKG1pDEUZlptLcFmZTRT2toQjj8tLxiVDbHKSuJUhrMMKUoixagmH21bawt7aF2uYg+ZlpnDN/HOU1TazbXcf++hYONgXJSvMTihiag2FagmGa2+ys9KlFWeSkp1DbHORAYxvBsKGpLcTqshrSUnwUZgeobQ6SnZ5CdUMboXCEguw0stJSqKxvZfLoTMoONNEailCQlYZfhJ0Hmgj34L8ryEqjNRimsS3cfqyzNRZPopMbYhFI8dEWjrTHs1L9giC0hT2RTfP7yEjzU9scxCeQmZbCd86dyyVHTTykNiUqa+hoYIsxZpvTiPuA84H1UdecD9zkfP4HcLuIiBlu/ipFSTC56V4sIS3Fx9i8dABK8m3MoicmFvR8vjsmjc7kuGmFh/RdsOXRRTikka4xhrZwhLZQhNaQ3baFIhigKCdAdiCFYDjCrgNNiAg+gTG56dS3hKioa2FcXjr76lrwiZCZ5kcQmoIhWoIRxuamc6CxjZrmNjBWPCLGkJbio6YpSENriOyAn+xAKtOLs1lbXsOBxjYCqT6q6tsQgaqGNrIDfiaNzsIYw46qJgqy04hEDAeb2mhqC7dbMe59jTGUFmbhFyEz4GdteS0NrSHG5abTGorQ1BZmalHWIf++eyKeQjAB2BW1Xw4c0901xpiQiNQCo4EqFEUZ0fgOY9KgiBBI8RNI8ZPTzTWpfh9Ti7I7HEtP9VOUEwDsjPnucIW0L4zNG9vna/vDuQtip13Hg2GxZrGIXC0iq0RkVWVlZaKboyiKMqKIpxDsBqKdWSXOsZjXiEgKkIcNGnfAGHOnMWapMWZpUVFRnJqrKIqSnMRTCN4EZojIFBFJAy4DHu50zcPAlc7ni4BnNT6gKIoyuMQtRuD4/K8FnsCmj/7eGLNORL4PrDLGPAz8DviziGwBDmDFQlEURRlE4lpryBjzOPB4p2PfifrcAlwczzYoiqIoPTMsgsWKoihK/FAhUBRFSXJUCBRFUZKcYVd0TkQqgZ2H+PVCkm+yWjK+MyTne+s7JweH+s6TjTEx8++HnRAcDiKyqrtaGyOVZHxnSM731ndODuLxzuoaUhRFSXJUCBRFUZKcZBOCOxPdgASQjO8Myfne+s7JwYC/c1LFCBRFUZSuJJtFoCiKonRChUBRFCXJSRohEJEzRWSjiGwRkRsS3Z54ISI7RORdEVktIqucYwUi8pSIbHa2+Ylu5+EgIr8Xkf0i8l7UsZjvKJbbnL/7WhFZkriWHzrdvPNNIrLb+VuvFpGzo87d6LzzRhH5QGJafXiIyEQReU5E1ovIOhH5snN8xP6te3jn+P6tjTEj/gdb/XQrMBVIA9YAcxPdrji96w6gsNOxnwA3OJ9vAH6c6HYe5jueBCwB3uvtHYGzgX9j1x8/Fngj0e0fwHe+CfhajGvnOv/GA8AU59++P9HvcAjvPA5Y4nzOwa6BPnck/617eOe4/q2TxSJoXz/ZGNMGuOsnJwvnA3c7n+8GPpS4phw+xpgXsWXLo+nuHc8H/mQsrwOjRGTcoDR0AOnmnbvjfOA+Y0yrMWY7sAX7f2BYYYzZa4x52/lcD2zALm87Yv/WPbxzdwzI3zpZhCDW+sk9/XKHMwZ4UkTeEpGrnWNjjDF7nc/7gDGJaVpc6e4dR/rf/lrHDfL7KJffiHtnESkFFgNvkCR/607vDHH8WyeLECQTJxhjlgBnAV8QkZOiTxprT47onOFkeEeHXwPTgEXAXuBnCW1NnBCRbOAB4CvGmLrocyP1bx3jneP6t04WIejL+skjAmPMbme7H/gn1kyscE1kZ7s/cS2MG92944j92xtjKowxYWNMBLgLzyUwYt5ZRFKxHeJfjDEPOodH9N861jvH+2+dLELQl/WThz0ikiUiOe5n4AzgPTquDX0l8FBiWhhXunvHh4GPOxklxwK1UW6FYU0n//eHsX9rsO98mYgERGQKMANYOdjtO1xERLDL2W4wxvw86tSI/Vt3985x/1snOko+iNH4s7ER+K3ANxPdnji941RsBsEaYJ37nsBo4BlgM/A0UJDoth7me96LNY+DWJ/op7p7R2wGyR3O3/1dYGmi2z+A7/xn553WOh3CuKjrv+m880bgrES3/xDf+QSs22ctsNr5OXsk/617eOe4/q21xISiKEqSkyyuIUVRFKUbVAgURVGSHBUCRVGUJEeFQFEUJclRIVAURUlyVAgUxUFEwlHVHVcPZJVaESmNrhyqKEOJlEQ3QFGGEM3GmEWJboSiDDZqEShKLzhrPPzEWedhpYhMd46XisizTiGwZ0RkknN8jIj8U0TWOD/HObfyi8hdTp35J0Ukw7n+S079+bUicl+CXlNJYlQIFMUjo5Nr6NKoc7XGmPnA7cCtzrFfAncbYxYAfwFuc47fBrxgjFmIXUNgnXN8BnCHMWYeUANc6By/AVjs3Odz8Xk1RekenVmsKA4i0mCMyY5xfAdwijFmm1MQbJ8xZrSIVGGn+ged43uNMYUiUgmUGGNao+5RCjxljJnh7F8PpBpjbhaR/wANwL+AfxljGuL8qorSAbUIFKVvmG4+94fWqM9hvBjdOdgaOUuAN0VEY3fKoKJCoCh949Ko7WvO51exlWwBrgBecj4/A1wDICJ+Ecnr7qYi4gMmGmOeA64H8oAuVomixBMdeSiKR4aIrI7a/48xxk0hzReRtdhR/eXOsS8CfxCRrwOVwCec418G7hSRT2FH/tdgK4fGwg/c44iFALcZY2oG6H0UpU9ojEBResGJESw1xlQlui2KEg/UNaQoipLkqEWgKIqS5KhFoCiKkuSoECiKoiQ5KgSKoihJjgqBoihKkqNCoCiKkuT8f801MySCpuLsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Conv 256*3\n",
    "mod = final_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 250\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "\n",
    "hist = train_model(mod, train_, test_, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['train_loss'], label='Train')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['eval_loss'], label='Eval')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921b98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9087481c",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4b535d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, tk_list_train = get_voc_new(train.document_text, False, True)\n",
    "voc_dic = {item: idx+1 for idx, item in enumerate(voc)}\n",
    "\n",
    "def word2idx(token_list):\n",
    "    \n",
    "    token_idx = []\n",
    "    for sent in token_list:\n",
    "        sent_list = []\n",
    "        for token in sent:\n",
    "            if token in voc_dic:\n",
    "                idx = voc_dic[token]\n",
    "            else:\n",
    "                idx = 0\n",
    "            sent_list.append(idx)\n",
    "        token_idx.append(sent_list)\n",
    "\n",
    "    return token_idx\n",
    "\n",
    "class dataset_(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, max_len=1024):\n",
    "        super(dataset_, self).__init__()\n",
    "        pad = [i[:max_len] + [0]*(max_len-len(i[:max_len])) for i in X]\n",
    "        self.X = torch.tensor(pad, dtype=torch.int32)\n",
    "        self.y = torch.Tensor(np.array(list(y)))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "tk_idx_train = word2idx(tk_list_train)\n",
    "train_ = dataset_(tk_idx_train, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4632479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emb(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, dim_emb):\n",
    "        super(Emb, self).__init__()\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Embedding(voc_size, dim_emb, padding_idx=0),\n",
    "            nn.Linear(dim_emb, 256),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "    \n",
    "class Conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out, conv_size, pool_size, pool_stride=None):\n",
    "        super(Conv, self).__init__()\n",
    "        self.cv = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, conv_size),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(pool_size, pool_stride))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cv(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "    \n",
    "    \n",
    "class EndConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb, dim_out, conv_size):\n",
    "        super(EndConv, self).__init__()\n",
    "        self.ecv = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_out, conv_size),\n",
    "            nn.BatchNorm1d(dim_out),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z = self.ecv(x.transpose(-2, -1))\n",
    "        return nn.MaxPool1d(Z.size(-1))(Z).squeeze(-1)\n",
    "\n",
    "            \n",
    "class Att(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256, dim_enc=128):\n",
    "        super(Att, self).__init__()\n",
    "        self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "        self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "        self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        temp = self.Wq(K) \n",
    "        score = temp.transpose(-2, -1) / math.sqrt(K.size(-1)) ## (50, 1200)\n",
    "        return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "    \n",
    "class LN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_emb=256):\n",
    "        super(LN, self).__init__()\n",
    "        self.feed = nn.LayerNorm(dim_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.feed(x)\n",
    "\n",
    "\n",
    "class final_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size=len(voc)+1, dim_emb=256, dropout=0.5, max_len=1024):\n",
    "        super(final_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Emb(voc_size, dim_emb),  ## (1024, dim_emb)\n",
    "            Conv(dim_emb, 256, 3, 3, 2),\n",
    "            # Conv(256, 256, 2, 3, 2),\n",
    "            # Conv(128, 128, 1, 3, 2),\n",
    "            # EndConv(128, 64, 1),\n",
    "            nn.Dropout(dropout),\n",
    "            Att(256, 50),\n",
    "#             # Att(256, 50),\n",
    "#             nn.Linear(256, 256),\n",
    "#             # LN(dim_emb),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "78e800c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, train_dataset, device, norm=0.5,\n",
    "                lr=0.0005, epochs=50, batch_size=256):\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if norm:\n",
    "        Loss = nn.BCELoss(weight=train_.y.sum(axis=0)**-norm).to(device)\n",
    "\n",
    "    else:\n",
    "        Loss = nn.BCELoss().to(device)\n",
    "        \n",
    "    op = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            \n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "        \n",
    "        if not (epoch+1)%10:\n",
    "            print(f\"epoch {epoch+1}\")\n",
    "            print(f\"train loss: {train_loss}\")\n",
    "            print(f'Spent time: {time.time()-start} seconds')              \n",
    "        \n",
    "    print('Training complete!')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2370fa33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "epoch 10\n",
      "train loss: 17.62736701965332\n",
      "Spent time: 43.57790517807007 seconds\n",
      "epoch 20\n",
      "train loss: 11.519508361816406\n",
      "Spent time: 87.06126952171326 seconds\n",
      "epoch 30\n",
      "train loss: 8.530522346496582\n",
      "Spent time: 130.70476913452148 seconds\n",
      "epoch 40\n",
      "train loss: 6.779897212982178\n",
      "Spent time: 174.37212991714478 seconds\n",
      "epoch 50\n",
      "train loss: 5.523341655731201\n",
      "Spent time: 218.0446321964264 seconds\n",
      "epoch 60\n",
      "train loss: 4.641663074493408\n",
      "Spent time: 262.6173746585846 seconds\n",
      "epoch 70\n",
      "train loss: 3.9686288833618164\n",
      "Spent time: 306.1678283214569 seconds\n",
      "epoch 80\n",
      "train loss: 3.381434440612793\n",
      "Spent time: 350.4514436721802 seconds\n",
      "epoch 90\n",
      "train loss: 3.1918463706970215\n",
      "Spent time: 393.9771194458008 seconds\n",
      "epoch 100\n",
      "train loss: 2.925536632537842\n",
      "Spent time: 437.53164982795715 seconds\n",
      "epoch 110\n",
      "train loss: 2.312613010406494\n",
      "Spent time: 481.10934710502625 seconds\n",
      "epoch 120\n",
      "train loss: 1.997349739074707\n",
      "Spent time: 524.629825592041 seconds\n",
      "epoch 130\n",
      "train loss: 1.8232377767562866\n",
      "Spent time: 568.3856496810913 seconds\n",
      "epoch 140\n",
      "train loss: 1.6845148801803589\n",
      "Spent time: 612.1000483036041 seconds\n",
      "epoch 150\n",
      "train loss: 1.5065065622329712\n",
      "Spent time: 655.7762823104858 seconds\n",
      "epoch 160\n",
      "train loss: 1.4688105583190918\n",
      "Spent time: 699.4818556308746 seconds\n",
      "epoch 170\n",
      "train loss: 1.427561640739441\n",
      "Spent time: 743.1547691822052 seconds\n",
      "epoch 180\n",
      "train loss: 1.3107324838638306\n",
      "Spent time: 786.7905013561249 seconds\n",
      "epoch 190\n",
      "train loss: 1.2274852991104126\n",
      "Spent time: 830.478155374527 seconds\n",
      "epoch 200\n",
      "train loss: 1.0784002542495728\n",
      "Spent time: 874.0976088047028 seconds\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "mod = final_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "\n",
    "train_test(mod, train_, device, norm, lr=lr, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9d6e79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasolve-us/test.csv')\n",
    "voc_test, tk_list_test = get_voc_new(test.document_text, False, True)\n",
    "tk_idx_test = word2idx(tk_list_test)\n",
    "pad_test = [i[:1024] + [0]*(1024-len(i[:1024])) for i in tk_idx_test]\n",
    "test_ = torch.tensor(pad_test, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b9b593fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = test_.to(device)\n",
    "mod = mod.to(device)\n",
    "res = []\n",
    "with torch.no_grad():\n",
    "    for i in DataLoader(test_, batch_size=256, shuffle=False):\n",
    "        res.append(mod(i).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d3f8cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = torch.cat(res, dim=0).squeeze(-1)\n",
    "\n",
    "fin = np.round(fin.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c17e1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fin, columns=['predictions']).to_csv('cnn_att_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708c3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
