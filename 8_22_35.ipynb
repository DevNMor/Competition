{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4d59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be2c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('Preprocessing_Train.pkl')\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.document_text, train.label, test_size=0.25)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_neg_stop = [i for i in stop_words if \"n't\" not in i and \"no\" not in i]\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pos(x):\n",
    "    if x.startswith('J'):\n",
    "        return 'a'\n",
    "    elif x.startswith('V'):\n",
    "        return 'v'\n",
    "    elif x.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "    \n",
    "def tok(doc, non_neg=False, lemmatized=False):\n",
    "    \n",
    "    if non_neg:\n",
    "        stop = non_neg_stop\n",
    "    else:\n",
    "        stop = stop_words\n",
    "       \n",
    "    tokens = re.split(r'\\s', doc.lower())\n",
    "    \n",
    "    if lemmatized:\n",
    "        pos_list = nltk.pos_tag(tokens)\n",
    "        tokens = list(map(lambda x: lemma.lemmatize(x[0], pos(x[1])), pos_list))    \n",
    "\n",
    "    tokens = [i for i in tokens if i not in stop]\n",
    "    tokens = [i for i in tokens if len(re.findall(r'\\w', i)) >= 2]\n",
    "    tokens = [re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)[0] for i in tokens if re.findall(r\"\\w[a-zA-Z0-9.-]*\\w\", i)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_voc_new(X, non_neg=True, lemmatized=True):\n",
    "    \n",
    "    voc = []\n",
    "    token_list = []\n",
    "    for i in X:\n",
    "        tokens = tok(i, non_neg, lemmatized)\n",
    "        token_list.append(tokens)\n",
    "        voc += tokens\n",
    "        \n",
    "    voc = list(set(voc))\n",
    "    \n",
    "    return voc, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c683a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, tk_list_train = get_voc_new(X_train, False, True)\n",
    "# voc, tk_list_train = get_voc_new(train.document_text, False, True)\n",
    "voc_test, tk_list_test = get_voc_new(X_test, False, True)\n",
    "voc_dic = {item: idx+1 for idx, item in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34463b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(token_list):\n",
    "    \n",
    "    token_idx = []\n",
    "    for sent in token_list:\n",
    "        sent_list = []\n",
    "        for token in sent:\n",
    "            if token in voc_dic:\n",
    "                idx = voc_dic[token]\n",
    "            else:\n",
    "                idx = 0\n",
    "            sent_list.append(idx)\n",
    "        token_idx.append(sent_list)\n",
    "\n",
    "    return token_idx\n",
    "\n",
    "\n",
    "tk_idx_train = word2idx(tk_list_train)\n",
    "tk_idx_test = word2idx(tk_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0eacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(vocabulary=voc)\n",
    "vec_train = tfidf.fit_transform(X_train)\n",
    "vec_test = tfidf.transform(X_test)\n",
    "\n",
    "class _dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        super(_dataset, self).__init__()\n",
    "        self.X = torch.Tensor(X)\n",
    "        self.y = torch.Tensor(y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "_train = _dataset(vec_train.toarray(), np.array(list(y_train)))\n",
    "_test = _dataset(vec_test.toarray(), np.array(list(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d2ef881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndConv(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EndConv, self).__init__()\n",
    "        self.ecv = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, 128),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z = self.ecv(x.unsqueeze(-1).transpose(-2, -1))\n",
    "        return nn.MaxPool1d(Z.size(-1))(Z).squeeze(-1)\n",
    "\n",
    "\n",
    "# class Conv(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(Conv, self).__init__()\n",
    "#         self.cv = nn.Sequential(\n",
    "#             nn.Conv1d(1, 64, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(1024, 512))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.cv(x.unsqueeze(-1).transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "\n",
    "# class Att(nn.Module):\n",
    "    \n",
    "#     def __init__(self, dim_emb=256, dim_enc=50):\n",
    "#         super(Att, self).__init__()\n",
    "#         self.Wq = nn.Linear(dim_emb, dim_enc, bias=False)\n",
    "#         self.Wk = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "#         self.Wv = nn.Linear(dim_emb, dim_emb, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "#         temp = self.Wq(K) \n",
    "#         score = temp.transpose(-2, -1) / math.sqrt(K.size(-1))\n",
    "#         return torch.matmul(nn.Softmax(dim=-1)(score), V)\n",
    "    \n",
    "    \n",
    "# class LN(nn.Module):\n",
    "    \n",
    "#     def __init__(self, dim_emb=256):\n",
    "#         super(LN, self).__init__()\n",
    "#         self.feed = nn.LayerNorm(dim_emb)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.feed(x)\n",
    "\n",
    "\n",
    "class final_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size=len(voc)+1, dim_emb=256, dropout=0.5, max_len=1024):\n",
    "        super(final_model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            EndConv(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 50),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "190d0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, eval_dataset, device, norm=0.5,\n",
    "                lr=0.0005, epochs=50, batch_size=256):\n",
    "    \n",
    "    history = {'train_loss': [], 'eval_loss': [], 'detail_train': [], 'detail_eval': []}\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "    save_loss = nn.BCELoss(reduction='none').to(device)\n",
    "    \n",
    "    if norm:\n",
    "        Loss = nn.BCELoss(weight=train_.y.sum(axis=0)**-norm).to(device)\n",
    "\n",
    "    else:\n",
    "        Loss = nn.BCELoss().to(device)\n",
    "        \n",
    "    op = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print('Training start!')\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        eval_loss = 0\n",
    "        detail_train = torch.zeros(50).to(device)\n",
    "        detail_eval = torch.zeros(50).to(device)\n",
    "        pred = []\n",
    "        real = []\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(X).squeeze(-1)\n",
    "            loss = Loss(out, y)\n",
    "            save_train = save_loss(out, y).sum(0)\n",
    "            \n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "            detail_train += save_train\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in eval_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(X).squeeze(-1)\n",
    "                loss = Loss(out, y)\n",
    "                save_eval = save_loss(out, y).sum(0)\n",
    "                detail_eval += save_eval\n",
    "                eval_loss += loss\n",
    "                pred.append(out.cpu())\n",
    "                real.append(y.cpu())\n",
    "                \n",
    "        train_loss = (train_loss/len(train_loader)).item()\n",
    "        eval_loss = (eval_loss/len(eval_loader)).item() \n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['eval_loss'].append(eval_loss)  \n",
    "        history['detail_train'].append(detail_train.cpu().detach())\n",
    "        history['detail_eval'].append(detail_eval.cpu().detach())  \n",
    "        \n",
    "        if not (epoch+1)%10:\n",
    "            print(f\"epoch {epoch+1}\\ntrain loss: {train_loss}\\t\\teval loss: {eval_loss}\")\n",
    "        \n",
    "        if not (epoch+1)%50:\n",
    "            res = torch.cat(pred)\n",
    "            tru = torch.cat(real)\n",
    "            print(f'\\nepoch {epoch+1}:\\n')\n",
    "            print(f\"f1_score for 50 classes: {f1_score(tru, np.round(res), average='macro')}\")\n",
    "            print(classification_report(tru, np.round(res)))\n",
    "#             print(classification_report(tru, np.round(res), target_names=['8','22','35']))\n",
    "            print(f'\\nSpent time: {time.time()-start} seconds')\n",
    "            \n",
    "        \n",
    "    print('Training complete!')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a20cc7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c4b3adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 8.46 GiB (GPU 0; 6.00 GiB total capacity; 68.47 MiB already allocated; 3.94 GiB free; 70.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2464/1764346604.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m hist = train_model(mod, _train, _test, device, norm,\n\u001b[0m\u001b[0;32m      9\u001b[0m                 lr=lr, epochs=epochs, batch_size=batch_size)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2464/2918864440.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, eval_dataset, device, norm, lr, epochs, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0msave_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2464/2409468985.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2464/2409468985.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    296\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 298\u001b[1;33m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    299\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 8.46 GiB (GPU 0; 6.00 GiB total capacity; 68.47 MiB already allocated; 3.94 GiB free; 70.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "mod = final_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "norm = None\n",
    "\n",
    "hist = train_model(mod, _train, _test, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['train_loss'], label='Train')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['eval_loss'], label='Eval')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcb7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4bdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1591ce49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "epoch 10\n",
      "train loss: 0.19789904356002808\t\teval loss: 0.19607852399349213\n",
      "epoch 20\n",
      "train loss: 0.11642402410507202\t\teval loss: 0.12662045657634735\n",
      "epoch 30\n",
      "train loss: 0.0782708153128624\t\teval loss: 0.11894180625677109\n",
      "epoch 40\n",
      "train loss: 0.05474073812365532\t\teval loss: 0.11974044144153595\n",
      "epoch 50\n",
      "train loss: 0.04002870246767998\t\teval loss: 0.15997473895549774\n",
      "\n",
      "epoch 50:\n",
      "\n",
      "f1_score for 50 classes: 0.7899181513241395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81       224\n",
      "           1       0.95      0.76      0.85       251\n",
      "           2       0.96      0.84      0.90       260\n",
      "           3       0.92      0.82      0.87       169\n",
      "           4       0.95      0.91      0.93       191\n",
      "           5       0.88      0.93      0.91       338\n",
      "           6       0.95      0.88      0.91       246\n",
      "           7       0.91      0.72      0.81       318\n",
      "           8       0.85      0.63      0.73       126\n",
      "           9       0.83      0.41      0.55       233\n",
      "          10       0.77      0.58      0.66       146\n",
      "          11       0.92      0.82      0.87       206\n",
      "          12       0.81      0.74      0.77       441\n",
      "          13       0.95      0.82      0.88       281\n",
      "          14       0.98      0.84      0.90       336\n",
      "          15       0.96      0.79      0.86       126\n",
      "          16       0.90      0.83      0.87       310\n",
      "          17       0.79      0.70      0.75       115\n",
      "          18       0.98      0.93      0.95       238\n",
      "          19       0.81      0.44      0.57        95\n",
      "          20       0.90      0.49      0.63       350\n",
      "          21       0.84      0.83      0.84       186\n",
      "          22       0.73      0.40      0.52       246\n",
      "          23       0.82      0.73      0.77       352\n",
      "          24       0.85      0.77      0.81       241\n",
      "          25       0.88      0.76      0.82       236\n",
      "          26       0.83      0.76      0.80       133\n",
      "          27       0.95      0.83      0.89       306\n",
      "          28       0.97      0.85      0.90       196\n",
      "          29       0.85      0.72      0.78       410\n",
      "          30       0.90      0.70      0.78       187\n",
      "          31       0.78      0.61      0.69       250\n",
      "          32       0.98      0.81      0.89       121\n",
      "          33       0.98      0.92      0.95       249\n",
      "          34       0.88      0.81      0.85       277\n",
      "          35       0.74      0.31      0.43       216\n",
      "          36       0.97      0.78      0.87       162\n",
      "          37       0.92      0.56      0.70       160\n",
      "          38       0.85      0.75      0.79       381\n",
      "          39       0.84      0.63      0.72       241\n",
      "          40       0.89      0.45      0.59       146\n",
      "          41       0.85      0.52      0.64       133\n",
      "          42       0.90      0.75      0.82       241\n",
      "          43       0.95      0.88      0.92       278\n",
      "          44       0.88      0.75      0.81       258\n",
      "          45       0.90      0.62      0.74       180\n",
      "          46       0.91      0.69      0.78       427\n",
      "          47       0.92      0.79      0.85       219\n",
      "          48       0.93      0.73      0.82       236\n",
      "          49       0.93      0.68      0.79       187\n",
      "\n",
      "   micro avg       0.90      0.73      0.80     11855\n",
      "   macro avg       0.89      0.72      0.79     11855\n",
      "weighted avg       0.89      0.73      0.80     11855\n",
      " samples avg       0.84      0.70      0.74     11855\n",
      "\n",
      "\n",
      "Spent time: 555.555403470993 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60\n",
      "train loss: 0.030421320348978043\t\teval loss: 0.1490205079317093\n",
      "epoch 70\n",
      "train loss: 0.02225392870604992\t\teval loss: 0.17372974753379822\n",
      "epoch 80\n",
      "train loss: 0.01945211924612522\t\teval loss: 0.18585078418254852\n",
      "epoch 90\n",
      "train loss: 0.01612626388669014\t\teval loss: 0.1948976367712021\n",
      "epoch 100\n",
      "train loss: 0.013661335222423077\t\teval loss: 0.19307349622249603\n",
      "\n",
      "epoch 100:\n",
      "\n",
      "f1_score for 50 classes: 0.8191482104607304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.73      0.80       224\n",
      "           1       0.92      0.82      0.87       251\n",
      "           2       0.94      0.85      0.89       260\n",
      "           3       0.93      0.80      0.86       169\n",
      "           4       0.94      0.91      0.92       191\n",
      "           5       0.88      0.95      0.91       338\n",
      "           6       0.94      0.90      0.92       246\n",
      "           7       0.91      0.76      0.83       318\n",
      "           8       0.83      0.67      0.75       126\n",
      "           9       0.74      0.65      0.69       233\n",
      "          10       0.82      0.66      0.73       146\n",
      "          11       0.90      0.86      0.88       206\n",
      "          12       0.79      0.78      0.78       441\n",
      "          13       0.95      0.85      0.89       281\n",
      "          14       0.95      0.85      0.90       336\n",
      "          15       0.95      0.84      0.89       126\n",
      "          16       0.91      0.84      0.87       310\n",
      "          17       0.76      0.73      0.74       115\n",
      "          18       0.97      0.93      0.95       238\n",
      "          19       0.71      0.63      0.67        95\n",
      "          20       0.83      0.68      0.75       350\n",
      "          21       0.86      0.87      0.86       186\n",
      "          22       0.66      0.61      0.64       246\n",
      "          23       0.83      0.75      0.79       352\n",
      "          24       0.85      0.78      0.81       241\n",
      "          25       0.88      0.74      0.81       236\n",
      "          26       0.84      0.86      0.85       133\n",
      "          27       0.94      0.87      0.90       306\n",
      "          28       0.95      0.86      0.90       196\n",
      "          29       0.83      0.78      0.80       410\n",
      "          30       0.80      0.72      0.76       187\n",
      "          31       0.78      0.64      0.71       250\n",
      "          32       0.91      0.88      0.89       121\n",
      "          33       0.98      0.93      0.95       249\n",
      "          34       0.87      0.84      0.86       277\n",
      "          35       0.67      0.46      0.55       216\n",
      "          36       0.96      0.82      0.89       162\n",
      "          37       0.91      0.69      0.78       160\n",
      "          38       0.81      0.83      0.82       381\n",
      "          39       0.79      0.67      0.72       241\n",
      "          40       0.74      0.63      0.68       146\n",
      "          41       0.79      0.69      0.74       133\n",
      "          42       0.90      0.83      0.86       241\n",
      "          43       0.97      0.89      0.93       278\n",
      "          44       0.87      0.78      0.82       258\n",
      "          45       0.88      0.71      0.79       180\n",
      "          46       0.88      0.73      0.80       427\n",
      "          47       0.93      0.79      0.85       219\n",
      "          48       0.88      0.83      0.85       236\n",
      "          49       0.92      0.77      0.84       187\n",
      "\n",
      "   micro avg       0.87      0.78      0.83     11855\n",
      "   macro avg       0.87      0.78      0.82     11855\n",
      "weighted avg       0.87      0.78      0.82     11855\n",
      " samples avg       0.83      0.76      0.77     11855\n",
      "\n",
      "\n",
      "Spent time: 1113.3892834186554 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110\n",
      "train loss: 0.011798602528870106\t\teval loss: 0.22183285653591156\n",
      "epoch 120\n",
      "train loss: 0.010646900162100792\t\teval loss: 0.2363666146993637\n",
      "epoch 130\n",
      "train loss: 0.008900748565793037\t\teval loss: 0.23878370225429535\n",
      "epoch 140\n",
      "train loss: 0.008311294950544834\t\teval loss: 0.2252567857503891\n",
      "epoch 150\n",
      "train loss: 0.0077290828339755535\t\teval loss: 0.227946475148201\n",
      "\n",
      "epoch 150:\n",
      "\n",
      "f1_score for 50 classes: 0.8195298765293844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.74      0.81       224\n",
      "           1       0.89      0.83      0.86       251\n",
      "           2       0.93      0.85      0.89       260\n",
      "           3       0.91      0.84      0.87       169\n",
      "           4       0.91      0.92      0.92       191\n",
      "           5       0.87      0.96      0.91       338\n",
      "           6       0.93      0.90      0.92       246\n",
      "           7       0.85      0.78      0.82       318\n",
      "           8       0.80      0.68      0.74       126\n",
      "           9       0.73      0.66      0.69       233\n",
      "          10       0.79      0.71      0.75       146\n",
      "          11       0.89      0.87      0.88       206\n",
      "          12       0.79      0.79      0.79       441\n",
      "          13       0.92      0.85      0.89       281\n",
      "          14       0.95      0.88      0.91       336\n",
      "          15       0.95      0.83      0.88       126\n",
      "          16       0.91      0.83      0.87       310\n",
      "          17       0.73      0.77      0.75       115\n",
      "          18       0.97      0.95      0.96       238\n",
      "          19       0.69      0.65      0.67        95\n",
      "          20       0.88      0.67      0.76       350\n",
      "          21       0.85      0.84      0.85       186\n",
      "          22       0.66      0.64      0.65       246\n",
      "          23       0.84      0.76      0.80       352\n",
      "          24       0.82      0.80      0.81       241\n",
      "          25       0.85      0.79      0.82       236\n",
      "          26       0.82      0.89      0.85       133\n",
      "          27       0.90      0.89      0.89       306\n",
      "          28       0.91      0.87      0.89       196\n",
      "          29       0.82      0.77      0.80       410\n",
      "          30       0.78      0.75      0.77       187\n",
      "          31       0.74      0.68      0.71       250\n",
      "          32       0.94      0.88      0.91       121\n",
      "          33       0.95      0.93      0.94       249\n",
      "          34       0.85      0.87      0.86       277\n",
      "          35       0.66      0.49      0.56       216\n",
      "          36       0.93      0.85      0.89       162\n",
      "          37       0.82      0.74      0.78       160\n",
      "          38       0.85      0.78      0.81       381\n",
      "          39       0.75      0.70      0.73       241\n",
      "          40       0.73      0.68      0.71       146\n",
      "          41       0.78      0.64      0.70       133\n",
      "          42       0.85      0.85      0.85       241\n",
      "          43       0.96      0.93      0.94       278\n",
      "          44       0.82      0.82      0.82       258\n",
      "          45       0.81      0.73      0.77       180\n",
      "          46       0.86      0.73      0.79       427\n",
      "          47       0.87      0.85      0.86       219\n",
      "          48       0.88      0.84      0.86       236\n",
      "          49       0.87      0.81      0.84       187\n",
      "\n",
      "   micro avg       0.85      0.80      0.83     11855\n",
      "   macro avg       0.85      0.80      0.82     11855\n",
      "weighted avg       0.85      0.80      0.82     11855\n",
      " samples avg       0.82      0.77      0.77     11855\n",
      "\n",
      "\n",
      "Spent time: 1671.6184339523315 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 160\n",
      "train loss: 0.0068707335740327835\t\teval loss: 0.24622474610805511\n",
      "epoch 170\n",
      "train loss: 0.00618921360000968\t\teval loss: 0.2814781665802002\n",
      "epoch 180\n",
      "train loss: 0.00625342782586813\t\teval loss: 0.20178118348121643\n",
      "epoch 190\n",
      "train loss: 0.0062814936973154545\t\teval loss: 0.23518644273281097\n",
      "epoch 200\n",
      "train loss: 0.005374311935156584\t\teval loss: 0.2540018558502197\n",
      "\n",
      "epoch 200:\n",
      "\n",
      "f1_score for 50 classes: 0.8205775956054824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.74      0.82       224\n",
      "           1       0.91      0.80      0.86       251\n",
      "           2       0.92      0.86      0.89       260\n",
      "           3       0.93      0.84      0.88       169\n",
      "           4       0.93      0.88      0.90       191\n",
      "           5       0.88      0.93      0.91       338\n",
      "           6       0.94      0.90      0.92       246\n",
      "           7       0.86      0.81      0.83       318\n",
      "           8       0.81      0.69      0.75       126\n",
      "           9       0.70      0.66      0.68       233\n",
      "          10       0.80      0.73      0.76       146\n",
      "          11       0.88      0.86      0.87       206\n",
      "          12       0.82      0.75      0.79       441\n",
      "          13       0.94      0.85      0.89       281\n",
      "          14       0.93      0.87      0.90       336\n",
      "          15       0.93      0.84      0.88       126\n",
      "          16       0.90      0.85      0.87       310\n",
      "          17       0.74      0.75      0.74       115\n",
      "          18       0.98      0.94      0.96       238\n",
      "          19       0.73      0.65      0.69        95\n",
      "          20       0.86      0.67      0.75       350\n",
      "          21       0.87      0.85      0.86       186\n",
      "          22       0.72      0.59      0.65       246\n",
      "          23       0.85      0.72      0.78       352\n",
      "          24       0.89      0.78      0.83       241\n",
      "          25       0.90      0.69      0.78       236\n",
      "          26       0.84      0.84      0.84       133\n",
      "          27       0.96      0.85      0.90       306\n",
      "          28       0.95      0.87      0.91       196\n",
      "          29       0.80      0.77      0.79       410\n",
      "          30       0.80      0.72      0.76       187\n",
      "          31       0.73      0.68      0.70       250\n",
      "          32       0.93      0.89      0.91       121\n",
      "          33       0.97      0.93      0.95       249\n",
      "          34       0.87      0.85      0.86       277\n",
      "          35       0.61      0.50      0.55       216\n",
      "          36       0.95      0.85      0.90       162\n",
      "          37       0.85      0.70      0.77       160\n",
      "          38       0.84      0.79      0.82       381\n",
      "          39       0.77      0.68      0.72       241\n",
      "          40       0.75      0.66      0.70       146\n",
      "          41       0.76      0.71      0.74       133\n",
      "          42       0.87      0.85      0.86       241\n",
      "          43       0.96      0.91      0.94       278\n",
      "          44       0.86      0.80      0.83       258\n",
      "          45       0.84      0.74      0.79       180\n",
      "          46       0.86      0.74      0.80       427\n",
      "          47       0.90      0.81      0.86       219\n",
      "          48       0.88      0.83      0.86       236\n",
      "          49       0.86      0.82      0.84       187\n",
      "\n",
      "   micro avg       0.86      0.79      0.83     11855\n",
      "   macro avg       0.86      0.79      0.82     11855\n",
      "weighted avg       0.86      0.79      0.82     11855\n",
      " samples avg       0.83      0.77      0.77     11855\n",
      "\n",
      "\n",
      "Spent time: 2246.9060633182526 seconds\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Embedding_shallower but wider 256*3\n",
    "mod = final_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "\n",
    "hist = train_model(mod, train_, test_, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['train_loss'], label='Train')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['eval_loss'], label='Eval')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd53b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42783ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "epoch 10\n",
      "train loss: 0.20334823429584503\t\teval loss: 0.2193826287984848\n",
      "epoch 20\n",
      "train loss: 0.11933037638664246\t\teval loss: 0.1283724308013916\n",
      "epoch 30\n",
      "train loss: 0.07949564605951309\t\teval loss: 0.12263929843902588\n",
      "epoch 40\n",
      "train loss: 0.0556238554418087\t\teval loss: 0.12575559318065643\n",
      "epoch 50\n",
      "train loss: 0.0400378592312336\t\teval loss: 0.13247595727443695\n",
      "\n",
      "epoch 50:\n",
      "\n",
      "f1_score for 50 classes: 0.804003221532673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.74      0.82       224\n",
      "           1       0.87      0.82      0.84       251\n",
      "           2       0.91      0.86      0.89       260\n",
      "           3       0.95      0.83      0.89       169\n",
      "           4       0.88      0.93      0.90       191\n",
      "           5       0.88      0.93      0.90       338\n",
      "           6       0.92      0.88      0.90       246\n",
      "           7       0.84      0.86      0.85       318\n",
      "           8       0.79      0.60      0.68       126\n",
      "           9       0.80      0.54      0.65       233\n",
      "          10       0.79      0.79      0.79       146\n",
      "          11       0.88      0.87      0.87       206\n",
      "          12       0.82      0.76      0.79       441\n",
      "          13       0.90      0.86      0.88       281\n",
      "          14       0.93      0.85      0.89       336\n",
      "          15       0.95      0.85      0.90       126\n",
      "          16       0.84      0.82      0.83       310\n",
      "          17       0.72      0.80      0.76       115\n",
      "          18       0.94      0.92      0.93       238\n",
      "          19       0.60      0.48      0.53        95\n",
      "          20       0.82      0.69      0.75       350\n",
      "          21       0.81      0.82      0.81       186\n",
      "          22       0.64      0.59      0.61       246\n",
      "          23       0.79      0.76      0.78       352\n",
      "          24       0.79      0.78      0.79       241\n",
      "          25       0.86      0.77      0.81       236\n",
      "          26       0.80      0.80      0.80       133\n",
      "          27       0.93      0.87      0.90       306\n",
      "          28       0.90      0.89      0.90       196\n",
      "          29       0.83      0.75      0.79       410\n",
      "          30       0.87      0.73      0.79       187\n",
      "          31       0.77      0.66      0.71       250\n",
      "          32       0.97      0.82      0.89       121\n",
      "          33       0.98      0.93      0.95       249\n",
      "          34       0.84      0.85      0.85       277\n",
      "          35       0.65      0.30      0.41       216\n",
      "          36       0.92      0.78      0.85       162\n",
      "          37       0.93      0.62      0.75       160\n",
      "          38       0.79      0.81      0.80       381\n",
      "          39       0.79      0.66      0.72       241\n",
      "          40       0.80      0.58      0.67       146\n",
      "          41       0.73      0.58      0.65       133\n",
      "          42       0.90      0.85      0.87       241\n",
      "          43       0.94      0.94      0.94       278\n",
      "          44       0.83      0.80      0.81       258\n",
      "          45       0.88      0.73      0.80       180\n",
      "          46       0.87      0.72      0.78       427\n",
      "          47       0.88      0.87      0.88       219\n",
      "          48       0.93      0.78      0.85       236\n",
      "          49       0.84      0.76      0.80       187\n",
      "\n",
      "   micro avg       0.85      0.78      0.81     11855\n",
      "   macro avg       0.85      0.77      0.80     11855\n",
      "weighted avg       0.85      0.78      0.81     11855\n",
      " samples avg       0.82      0.75      0.76     11855\n",
      "\n",
      "\n",
      "Spent time: 465.0121052265167 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60\n",
      "train loss: 0.02990969642996788\t\teval loss: 0.15203748643398285\n",
      "epoch 70\n",
      "train loss: 0.023364661261439323\t\teval loss: 0.149562805891037\n",
      "epoch 80\n",
      "train loss: 0.018716024234890938\t\teval loss: 0.1729631870985031\n",
      "epoch 90\n",
      "train loss: 0.014972594566643238\t\teval loss: 0.20658040046691895\n",
      "epoch 100\n",
      "train loss: 0.013581608422100544\t\teval loss: 0.16993990540504456\n",
      "\n",
      "epoch 100:\n",
      "\n",
      "f1_score for 50 classes: 0.8177438514173571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82       224\n",
      "           1       0.90      0.84      0.87       251\n",
      "           2       0.92      0.86      0.89       260\n",
      "           3       0.94      0.86      0.90       169\n",
      "           4       0.93      0.95      0.94       191\n",
      "           5       0.88      0.91      0.90       338\n",
      "           6       0.92      0.89      0.91       246\n",
      "           7       0.85      0.86      0.85       318\n",
      "           8       0.73      0.63      0.68       126\n",
      "           9       0.72      0.67      0.69       233\n",
      "          10       0.79      0.80      0.79       146\n",
      "          11       0.90      0.84      0.87       206\n",
      "          12       0.78      0.78      0.78       441\n",
      "          13       0.94      0.85      0.89       281\n",
      "          14       0.96      0.85      0.90       336\n",
      "          15       0.92      0.86      0.89       126\n",
      "          16       0.87      0.84      0.86       310\n",
      "          17       0.80      0.78      0.79       115\n",
      "          18       0.97      0.92      0.95       238\n",
      "          19       0.70      0.59      0.64        95\n",
      "          20       0.78      0.76      0.77       350\n",
      "          21       0.86      0.84      0.85       186\n",
      "          22       0.68      0.59      0.63       246\n",
      "          23       0.82      0.74      0.78       352\n",
      "          24       0.80      0.80      0.80       241\n",
      "          25       0.87      0.77      0.81       236\n",
      "          26       0.85      0.80      0.83       133\n",
      "          27       0.94      0.87      0.90       306\n",
      "          28       0.91      0.88      0.90       196\n",
      "          29       0.82      0.75      0.79       410\n",
      "          30       0.80      0.79      0.79       187\n",
      "          31       0.81      0.58      0.68       250\n",
      "          32       0.94      0.85      0.90       121\n",
      "          33       0.98      0.94      0.96       249\n",
      "          34       0.89      0.81      0.85       277\n",
      "          35       0.64      0.45      0.53       216\n",
      "          36       0.93      0.78      0.85       162\n",
      "          37       0.86      0.76      0.80       160\n",
      "          38       0.78      0.82      0.80       381\n",
      "          39       0.78      0.68      0.73       241\n",
      "          40       0.75      0.69      0.72       146\n",
      "          41       0.69      0.56      0.62       133\n",
      "          42       0.90      0.83      0.87       241\n",
      "          43       0.96      0.93      0.94       278\n",
      "          44       0.81      0.78      0.79       258\n",
      "          45       0.89      0.75      0.82       180\n",
      "          46       0.86      0.75      0.80       427\n",
      "          47       0.93      0.83      0.88       219\n",
      "          48       0.92      0.79      0.85       236\n",
      "          49       0.91      0.79      0.85       187\n",
      "\n",
      "   micro avg       0.86      0.79      0.82     11855\n",
      "   macro avg       0.85      0.79      0.82     11855\n",
      "weighted avg       0.86      0.79      0.82     11855\n",
      " samples avg       0.83      0.77      0.77     11855\n",
      "\n",
      "\n",
      "Spent time: 931.4802551269531 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110\n",
      "train loss: 0.01133616641163826\t\teval loss: 0.20658698678016663\n",
      "epoch 120\n",
      "train loss: 0.01006733812391758\t\teval loss: 0.18896853923797607\n",
      "epoch 130\n",
      "train loss: 0.009496072307229042\t\teval loss: 0.21549272537231445\n",
      "epoch 140\n",
      "train loss: 0.008381159044802189\t\teval loss: 0.23306719958782196\n",
      "epoch 150\n",
      "train loss: 0.007227981463074684\t\teval loss: 0.2514427602291107\n",
      "\n",
      "epoch 150:\n",
      "\n",
      "f1_score for 50 classes: 0.8201153301887306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84       224\n",
      "           1       0.92      0.85      0.88       251\n",
      "           2       0.90      0.87      0.88       260\n",
      "           3       0.95      0.85      0.90       169\n",
      "           4       0.91      0.93      0.92       191\n",
      "           5       0.88      0.91      0.90       338\n",
      "           6       0.93      0.90      0.91       246\n",
      "           7       0.89      0.81      0.84       318\n",
      "           8       0.79      0.64      0.71       126\n",
      "           9       0.77      0.64      0.70       233\n",
      "          10       0.78      0.79      0.79       146\n",
      "          11       0.88      0.85      0.86       206\n",
      "          12       0.82      0.76      0.79       441\n",
      "          13       0.94      0.85      0.90       281\n",
      "          14       0.93      0.86      0.90       336\n",
      "          15       0.92      0.85      0.88       126\n",
      "          16       0.88      0.81      0.85       310\n",
      "          17       0.67      0.84      0.75       115\n",
      "          18       0.96      0.92      0.94       238\n",
      "          19       0.66      0.60      0.63        95\n",
      "          20       0.80      0.74      0.77       350\n",
      "          21       0.83      0.86      0.84       186\n",
      "          22       0.70      0.63      0.66       246\n",
      "          23       0.80      0.75      0.77       352\n",
      "          24       0.82      0.81      0.82       241\n",
      "          25       0.88      0.78      0.83       236\n",
      "          26       0.82      0.77      0.80       133\n",
      "          27       0.97      0.84      0.90       306\n",
      "          28       0.94      0.87      0.90       196\n",
      "          29       0.83      0.76      0.79       410\n",
      "          30       0.80      0.75      0.77       187\n",
      "          31       0.75      0.69      0.72       250\n",
      "          32       0.96      0.84      0.90       121\n",
      "          33       0.95      0.94      0.95       249\n",
      "          34       0.85      0.83      0.84       277\n",
      "          35       0.60      0.47      0.53       216\n",
      "          36       0.92      0.79      0.85       162\n",
      "          37       0.89      0.74      0.81       160\n",
      "          38       0.77      0.84      0.80       381\n",
      "          39       0.78      0.71      0.74       241\n",
      "          40       0.85      0.64      0.73       146\n",
      "          41       0.74      0.67      0.70       133\n",
      "          42       0.92      0.85      0.88       241\n",
      "          43       0.95      0.94      0.94       278\n",
      "          44       0.85      0.74      0.79       258\n",
      "          45       0.89      0.78      0.83       180\n",
      "          46       0.85      0.72      0.78       427\n",
      "          47       0.90      0.87      0.89       219\n",
      "          48       0.88      0.82      0.85       236\n",
      "          49       0.89      0.80      0.84       187\n",
      "\n",
      "   micro avg       0.86      0.80      0.83     11855\n",
      "   macro avg       0.85      0.79      0.82     11855\n",
      "weighted avg       0.86      0.80      0.82     11855\n",
      " samples avg       0.83      0.77      0.78     11855\n",
      "\n",
      "\n",
      "Spent time: 1397.7280168533325 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 160\n",
      "train loss: 0.006607775576412678\t\teval loss: 0.2620411813259125\n",
      "epoch 170\n",
      "train loss: 0.006697102915495634\t\teval loss: 0.25648126006126404\n",
      "epoch 180\n",
      "train loss: 0.005812198854982853\t\teval loss: 0.25479570031166077\n",
      "epoch 190\n",
      "train loss: 0.005289084743708372\t\teval loss: 0.24706295132637024\n",
      "epoch 200\n",
      "train loss: 0.005550635978579521\t\teval loss: 0.2357543557882309\n",
      "\n",
      "epoch 200:\n",
      "\n",
      "f1_score for 50 classes: 0.8206061348712459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85       224\n",
      "           1       0.92      0.85      0.88       251\n",
      "           2       0.93      0.86      0.89       260\n",
      "           3       0.94      0.85      0.89       169\n",
      "           4       0.93      0.94      0.93       191\n",
      "           5       0.89      0.90      0.90       338\n",
      "           6       0.94      0.89      0.91       246\n",
      "           7       0.88      0.81      0.84       318\n",
      "           8       0.76      0.61      0.68       126\n",
      "           9       0.74      0.64      0.69       233\n",
      "          10       0.80      0.75      0.78       146\n",
      "          11       0.89      0.88      0.89       206\n",
      "          12       0.81      0.77      0.79       441\n",
      "          13       0.93      0.86      0.89       281\n",
      "          14       0.95      0.87      0.91       336\n",
      "          15       0.90      0.86      0.88       126\n",
      "          16       0.86      0.86      0.86       310\n",
      "          17       0.77      0.77      0.77       115\n",
      "          18       0.96      0.92      0.94       238\n",
      "          19       0.67      0.61      0.64        95\n",
      "          20       0.82      0.74      0.77       350\n",
      "          21       0.88      0.84      0.86       186\n",
      "          22       0.69      0.63      0.66       246\n",
      "          23       0.81      0.76      0.78       352\n",
      "          24       0.84      0.81      0.82       241\n",
      "          25       0.89      0.76      0.82       236\n",
      "          26       0.80      0.77      0.79       133\n",
      "          27       0.94      0.88      0.91       306\n",
      "          28       0.93      0.87      0.90       196\n",
      "          29       0.80      0.77      0.78       410\n",
      "          30       0.84      0.77      0.80       187\n",
      "          31       0.82      0.62      0.71       250\n",
      "          32       0.94      0.84      0.89       121\n",
      "          33       0.97      0.94      0.95       249\n",
      "          34       0.84      0.83      0.83       277\n",
      "          35       0.62      0.45      0.52       216\n",
      "          36       0.92      0.78      0.85       162\n",
      "          37       0.88      0.75      0.81       160\n",
      "          38       0.80      0.83      0.82       381\n",
      "          39       0.82      0.68      0.74       241\n",
      "          40       0.82      0.66      0.73       146\n",
      "          41       0.75      0.58      0.65       133\n",
      "          42       0.90      0.85      0.87       241\n",
      "          43       0.95      0.93      0.94       278\n",
      "          44       0.88      0.73      0.80       258\n",
      "          45       0.92      0.78      0.84       180\n",
      "          46       0.85      0.73      0.79       427\n",
      "          47       0.88      0.86      0.87       219\n",
      "          48       0.88      0.81      0.85       236\n",
      "          49       0.88      0.82      0.85       187\n",
      "\n",
      "   micro avg       0.86      0.79      0.83     11855\n",
      "   macro avg       0.86      0.79      0.82     11855\n",
      "weighted avg       0.86      0.79      0.83     11855\n",
      " samples avg       0.83      0.77      0.78     11855\n",
      "\n",
      "\n",
      "Spent time: 1863.7436981201172 seconds\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embedding Shallower 256*3\n",
    "# 35和8复杂，22简单？\n",
    "mod = final_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "\n",
    "hist = train_model(mod, train_, test_, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['train_loss'], label='Train')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['eval_loss'], label='Eval')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a55229b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(vocabulary=voc)\n",
    "vec_train = tfidf.fit_transform(X_train)\n",
    "vec_test = tfidf.transform(X_test)\n",
    "\n",
    "class _dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        super(_dataset, self).__init__()\n",
    "        self.X = torch.Tensor(X)\n",
    "        self.y = torch.Tensor(y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "_train = _dataset(vec_train.toarray(), np.array(list(y_train)))\n",
    "_test = _dataset(vec_test.toarray(), np.array(list(y_test)))\n",
    "class MLP_model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(MLP_model, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(vec_train.shape[1], 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 50),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c833cabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "epoch 10\n",
      "train loss: 0.21831580996513367\t\teval loss: 0.22929878532886505\n",
      "epoch 20\n",
      "train loss: 0.14005769789218903\t\teval loss: 0.17516420781612396\n",
      "epoch 30\n",
      "train loss: 0.08362284302711487\t\teval loss: 0.14622800052165985\n",
      "epoch 40\n",
      "train loss: 0.04694589972496033\t\teval loss: 0.13391344249248505\n",
      "epoch 50\n",
      "train loss: 0.026620447635650635\t\teval loss: 0.13217030465602875\n",
      "\n",
      "epoch 50:\n",
      "\n",
      "f1_score for 50 classes: 0.7221689941737508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.50      0.63       230\n",
      "           1       0.98      0.69      0.81       242\n",
      "           2       0.86      0.64      0.74       266\n",
      "           3       0.90      0.69      0.78       168\n",
      "           4       0.92      0.78      0.85       181\n",
      "           5       0.89      0.63      0.74       357\n",
      "           6       0.95      0.64      0.77       258\n",
      "           7       0.93      0.72      0.81       316\n",
      "           8       0.75      0.32      0.45       134\n",
      "           9       0.85      0.50      0.63       252\n",
      "          10       0.94      0.54      0.68       138\n",
      "          11       0.94      0.70      0.80       211\n",
      "          12       0.85      0.67      0.75       454\n",
      "          13       0.91      0.58      0.71       312\n",
      "          14       0.94      0.69      0.80       330\n",
      "          15       0.91      0.47      0.62       129\n",
      "          16       0.88      0.72      0.79       320\n",
      "          17       0.91      0.55      0.68       117\n",
      "          18       0.91      0.74      0.82       234\n",
      "          19       0.88      0.36      0.51       119\n",
      "          20       0.86      0.60      0.71       326\n",
      "          21       0.91      0.65      0.76       178\n",
      "          22       0.83      0.43      0.57       237\n",
      "          23       0.82      0.74      0.78       345\n",
      "          24       0.93      0.56      0.70       278\n",
      "          25       0.84      0.62      0.71       244\n",
      "          26       0.89      0.56      0.69       159\n",
      "          27       0.91      0.74      0.81       266\n",
      "          28       0.95      0.71      0.81       202\n",
      "          29       0.84      0.65      0.73       411\n",
      "          30       0.85      0.54      0.66       193\n",
      "          31       0.89      0.48      0.62       239\n",
      "          32       0.94      0.67      0.78       132\n",
      "          33       0.95      0.72      0.82       253\n",
      "          34       0.93      0.78      0.85       265\n",
      "          35       0.80      0.34      0.48       199\n",
      "          36       0.95      0.67      0.79       149\n",
      "          37       0.94      0.48      0.64       166\n",
      "          38       0.85      0.75      0.80       399\n",
      "          39       0.87      0.48      0.62       259\n",
      "          40       0.89      0.39      0.54       171\n",
      "          41       0.93      0.41      0.57       139\n",
      "          42       0.92      0.61      0.74       258\n",
      "          43       0.97      0.87      0.92       285\n",
      "          44       0.87      0.63      0.73       296\n",
      "          45       0.97      0.72      0.83       174\n",
      "          46       0.87      0.66      0.75       447\n",
      "          47       0.87      0.66      0.75       219\n",
      "          48       0.89      0.77      0.83       220\n",
      "          49       0.92      0.66      0.77       190\n",
      "\n",
      "   micro avg       0.89      0.63      0.74     12067\n",
      "   macro avg       0.90      0.61      0.72     12067\n",
      "weighted avg       0.89      0.63      0.73     12067\n",
      " samples avg       0.77      0.60      0.64     12067\n",
      "\n",
      "\n",
      "Spent time: 39.2288715839386 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60\n",
      "train loss: 0.01579935848712921\t\teval loss: 0.13630959391593933\n",
      "epoch 70\n",
      "train loss: 0.009905390441417694\t\teval loss: 0.14313636720180511\n",
      "epoch 80\n",
      "train loss: 0.00652848556637764\t\teval loss: 0.1499459445476532\n",
      "epoch 90\n",
      "train loss: 0.004484423901885748\t\teval loss: 0.15843138098716736\n",
      "epoch 100\n",
      "train loss: 0.0031903989147394896\t\teval loss: 0.16545139253139496\n",
      "\n",
      "epoch 100:\n",
      "\n",
      "f1_score for 50 classes: 0.7453847506263519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.65       230\n",
      "           1       0.96      0.71      0.82       242\n",
      "           2       0.86      0.67      0.76       266\n",
      "           3       0.90      0.73      0.81       168\n",
      "           4       0.91      0.80      0.85       181\n",
      "           5       0.89      0.67      0.76       357\n",
      "           6       0.92      0.67      0.78       258\n",
      "           7       0.93      0.73      0.82       316\n",
      "           8       0.76      0.41      0.53       134\n",
      "           9       0.86      0.57      0.69       252\n",
      "          10       0.94      0.56      0.70       138\n",
      "          11       0.93      0.74      0.83       211\n",
      "          12       0.84      0.69      0.76       454\n",
      "          13       0.89      0.62      0.73       312\n",
      "          14       0.93      0.71      0.80       330\n",
      "          15       0.89      0.50      0.64       129\n",
      "          16       0.87      0.72      0.79       320\n",
      "          17       0.90      0.62      0.73       117\n",
      "          18       0.90      0.76      0.82       234\n",
      "          19       0.86      0.43      0.57       119\n",
      "          20       0.85      0.64      0.73       326\n",
      "          21       0.89      0.67      0.77       178\n",
      "          22       0.76      0.47      0.58       237\n",
      "          23       0.83      0.76      0.79       345\n",
      "          24       0.89      0.62      0.73       278\n",
      "          25       0.83      0.65      0.73       244\n",
      "          26       0.88      0.61      0.72       159\n",
      "          27       0.90      0.76      0.82       266\n",
      "          28       0.95      0.73      0.82       202\n",
      "          29       0.82      0.67      0.74       411\n",
      "          30       0.85      0.55      0.67       193\n",
      "          31       0.86      0.53      0.66       239\n",
      "          32       0.92      0.70      0.79       132\n",
      "          33       0.94      0.73      0.82       253\n",
      "          34       0.93      0.80      0.86       265\n",
      "          35       0.77      0.43      0.55       199\n",
      "          36       0.93      0.76      0.84       149\n",
      "          37       0.91      0.52      0.66       166\n",
      "          38       0.83      0.77      0.80       399\n",
      "          39       0.82      0.56      0.66       259\n",
      "          40       0.89      0.49      0.63       171\n",
      "          41       0.91      0.48      0.63       139\n",
      "          42       0.88      0.65      0.75       258\n",
      "          43       0.97      0.88      0.92       285\n",
      "          44       0.85      0.67      0.75       296\n",
      "          45       0.95      0.76      0.85       174\n",
      "          46       0.86      0.69      0.76       447\n",
      "          47       0.86      0.67      0.76       219\n",
      "          48       0.90      0.81      0.85       220\n",
      "          49       0.88      0.69      0.77       190\n",
      "\n",
      "   micro avg       0.88      0.67      0.76     12067\n",
      "   macro avg       0.88      0.65      0.75     12067\n",
      "weighted avg       0.88      0.67      0.75     12067\n",
      " samples avg       0.77      0.63      0.66     12067\n",
      "\n",
      "\n",
      "Spent time: 78.1623649597168 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110\n",
      "train loss: 0.002338390564545989\t\teval loss: 0.17372126877307892\n",
      "epoch 120\n",
      "train loss: 0.0017595629906281829\t\teval loss: 0.18163758516311646\n",
      "epoch 130\n",
      "train loss: 0.0013565381523221731\t\teval loss: 0.1883552074432373\n",
      "epoch 140\n",
      "train loss: 0.001072741113603115\t\teval loss: 0.19645927846431732\n",
      "epoch 150\n",
      "train loss: 0.0008616959094069898\t\teval loss: 0.2024856060743332\n",
      "\n",
      "epoch 150:\n",
      "\n",
      "f1_score for 50 classes: 0.7449204621152767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.54      0.65       230\n",
      "           1       0.96      0.71      0.82       242\n",
      "           2       0.85      0.68      0.76       266\n",
      "           3       0.91      0.74      0.82       168\n",
      "           4       0.90      0.78      0.84       181\n",
      "           5       0.87      0.68      0.76       357\n",
      "           6       0.91      0.68      0.78       258\n",
      "           7       0.93      0.72      0.81       316\n",
      "           8       0.73      0.40      0.51       134\n",
      "           9       0.85      0.57      0.68       252\n",
      "          10       0.92      0.57      0.71       138\n",
      "          11       0.92      0.74      0.82       211\n",
      "          12       0.82      0.70      0.75       454\n",
      "          13       0.88      0.62      0.73       312\n",
      "          14       0.90      0.72      0.80       330\n",
      "          15       0.85      0.49      0.62       129\n",
      "          16       0.87      0.72      0.79       320\n",
      "          17       0.90      0.62      0.74       117\n",
      "          18       0.90      0.76      0.82       234\n",
      "          19       0.82      0.43      0.56       119\n",
      "          20       0.83      0.66      0.73       326\n",
      "          21       0.88      0.67      0.76       178\n",
      "          22       0.75      0.47      0.58       237\n",
      "          23       0.82      0.76      0.79       345\n",
      "          24       0.89      0.63      0.74       278\n",
      "          25       0.82      0.66      0.73       244\n",
      "          26       0.87      0.62      0.73       159\n",
      "          27       0.88      0.76      0.82       266\n",
      "          28       0.95      0.73      0.83       202\n",
      "          29       0.82      0.66      0.73       411\n",
      "          30       0.82      0.57      0.67       193\n",
      "          31       0.83      0.57      0.68       239\n",
      "          32       0.92      0.71      0.80       132\n",
      "          33       0.94      0.74      0.83       253\n",
      "          34       0.93      0.80      0.86       265\n",
      "          35       0.74      0.45      0.56       199\n",
      "          36       0.93      0.75      0.83       149\n",
      "          37       0.88      0.52      0.66       166\n",
      "          38       0.84      0.76      0.80       399\n",
      "          39       0.81      0.58      0.68       259\n",
      "          40       0.89      0.50      0.64       171\n",
      "          41       0.88      0.50      0.64       139\n",
      "          42       0.86      0.64      0.74       258\n",
      "          43       0.97      0.88      0.92       285\n",
      "          44       0.83      0.68      0.75       296\n",
      "          45       0.94      0.78      0.85       174\n",
      "          46       0.84      0.70      0.76       447\n",
      "          47       0.86      0.67      0.75       219\n",
      "          48       0.89      0.80      0.85       220\n",
      "          49       0.85      0.68      0.76       190\n",
      "\n",
      "   micro avg       0.87      0.67      0.76     12067\n",
      "   macro avg       0.87      0.66      0.74     12067\n",
      "weighted avg       0.87      0.67      0.75     12067\n",
      " samples avg       0.77      0.64      0.67     12067\n",
      "\n",
      "\n",
      "Spent time: 117.07011032104492 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 160\n",
      "train loss: 0.0007075810572132468\t\teval loss: 0.21078169345855713\n",
      "epoch 170\n",
      "train loss: 0.0005913428030908108\t\teval loss: 0.21635471284389496\n",
      "epoch 180\n",
      "train loss: 0.0005071648629382253\t\teval loss: 0.22354045510292053\n",
      "epoch 190\n",
      "train loss: 0.0004368145018815994\t\teval loss: 0.22929690778255463\n",
      "epoch 200\n",
      "train loss: 0.0003836801915895194\t\teval loss: 0.2361086905002594\n",
      "\n",
      "epoch 200:\n",
      "\n",
      "f1_score for 50 classes: 0.7440022568278818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.55      0.66       230\n",
      "           1       0.95      0.72      0.82       242\n",
      "           2       0.85      0.70      0.76       266\n",
      "           3       0.88      0.74      0.81       168\n",
      "           4       0.90      0.78      0.84       181\n",
      "           5       0.86      0.69      0.77       357\n",
      "           6       0.91      0.69      0.78       258\n",
      "           7       0.93      0.71      0.81       316\n",
      "           8       0.73      0.39      0.51       134\n",
      "           9       0.81      0.58      0.67       252\n",
      "          10       0.92      0.58      0.71       138\n",
      "          11       0.92      0.74      0.82       211\n",
      "          12       0.83      0.70      0.76       454\n",
      "          13       0.88      0.63      0.73       312\n",
      "          14       0.91      0.70      0.79       330\n",
      "          15       0.83      0.49      0.61       129\n",
      "          16       0.88      0.71      0.79       320\n",
      "          17       0.88      0.63      0.74       117\n",
      "          18       0.90      0.76      0.82       234\n",
      "          19       0.79      0.42      0.55       119\n",
      "          20       0.82      0.66      0.73       326\n",
      "          21       0.88      0.66      0.76       178\n",
      "          22       0.75      0.48      0.59       237\n",
      "          23       0.82      0.77      0.79       345\n",
      "          24       0.88      0.63      0.73       278\n",
      "          25       0.82      0.66      0.73       244\n",
      "          26       0.87      0.61      0.72       159\n",
      "          27       0.88      0.76      0.82       266\n",
      "          28       0.95      0.75      0.84       202\n",
      "          29       0.81      0.67      0.73       411\n",
      "          30       0.82      0.58      0.68       193\n",
      "          31       0.83      0.54      0.65       239\n",
      "          32       0.92      0.72      0.81       132\n",
      "          33       0.94      0.74      0.83       253\n",
      "          34       0.93      0.80      0.86       265\n",
      "          35       0.74      0.46      0.57       199\n",
      "          36       0.93      0.76      0.83       149\n",
      "          37       0.87      0.52      0.65       166\n",
      "          38       0.83      0.76      0.79       399\n",
      "          39       0.82      0.56      0.67       259\n",
      "          40       0.87      0.51      0.65       171\n",
      "          41       0.87      0.51      0.64       139\n",
      "          42       0.85      0.64      0.73       258\n",
      "          43       0.96      0.88      0.92       285\n",
      "          44       0.83      0.68      0.75       296\n",
      "          45       0.94      0.80      0.86       174\n",
      "          46       0.82      0.71      0.76       447\n",
      "          47       0.86      0.69      0.76       219\n",
      "          48       0.88      0.80      0.84       220\n",
      "          49       0.85      0.69      0.77       190\n",
      "\n",
      "   micro avg       0.86      0.67      0.76     12067\n",
      "   macro avg       0.86      0.66      0.74     12067\n",
      "weighted avg       0.86      0.67      0.75     12067\n",
      " samples avg       0.77      0.64      0.67     12067\n",
      "\n",
      "\n",
      "Spent time: 156.40939211845398 seconds\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mod_ez = MLP_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 200\n",
    "lr = 3e-4\n",
    "batch_size = 128\n",
    "norm = None\n",
    "\n",
    "hist = train_model(mod_ez, _train, _test, device, norm,\n",
    "                lr=lr, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['train_loss'], label='Train')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist['eval_loss'], label='Eval')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76fe891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = []\n",
    "for x in [8, 22, 35]:\n",
    "    mb = []\n",
    "    for i in range(50):\n",
    "        dom = 0\n",
    "        non_dom = 0\n",
    "        count_1 = 0\n",
    "        count_2 = 0\n",
    "        nc_1 = 0\n",
    "        nc_2 = 0\n",
    "        if i not in [8, 22, 35]:\n",
    "            for j in train.label_idx:\n",
    "                if i in j:\n",
    "                    dom += 1\n",
    "                    if x in j:\n",
    "                        count_1 += 1\n",
    "                    else:\n",
    "                        nc_1 += 1\n",
    "                else:\n",
    "                    non_dom += 1\n",
    "                    if x in j:\n",
    "                        count_2 += 1\n",
    "                    else:\n",
    "                        nc_2 += 1\n",
    "            mb.append((count_1/dom, count_2/non_dom, nc_1/dom, nc_2/non_dom))\n",
    "    save.append(mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2694a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_8 = save[0]\n",
    "p_22 = save[1]\n",
    "p_35 = save[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20eb30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "with torch.no_grad():\n",
    "    for X, y in DataLoader(test_, batch_size=256):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        out = mod(X).squeeze(-1)\n",
    "        pred.append(out.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "425ed6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = torch.cat(pred)[:, [i for i in range(50) if i not in [8, 22, 35]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c916b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rea = test_.y[:, [i for i in range(50) if i not in [8, 22, 35]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "195a791d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4130, 2.3769, 2.4704,  ..., 2.4017, 2.3848, 2.4934])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.matmul(pre, torch.Tensor(p_8)[:, 0].squeeze(-1)) + torch.matmul(1-pre, torch.Tensor(p_8)[:, 1].squeeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5d52831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([44.5870, 44.6231, 44.5296,  ..., 44.5983, 44.6152, 44.5066])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.matmul(pre, torch.Tensor(p_8)[:, 2].squeeze(-1)) + torch.matmul(1-pre, torch.Tensor(p_8)[:, 3].squeeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92706d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1435, 4.5474, 4.2090,  ..., 4.3982, 4.2331, 4.3111])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.matmul(pre, torch.Tensor(p_22)[:, 0].squeeze(-1)) + torch.matmul(1-pre, torch.Tensor(p_22)[:, 1].squeeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4b3ae7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7967, 3.8313, 3.8017,  ..., 3.8292, 3.9071, 3.9900])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.matmul(pre, torch.Tensor(p_35)[:, 0].squeeze(-1)) + torch.matmul(1-pre, torch.Tensor(p_35)[:, 1].squeeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa1057f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_.y[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9df91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
